<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
<!-- End Google Tag Manager -->
  <title>DPU IP Details and System Integration &mdash; Vitis™ AI 3.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Vitis AI Model Zoo" href="workflow-model-zoo.html" />
    <link rel="prev" title="Overview" href="workflow.html" /> 
</head>

<body class="wy-body-for-nav">

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
<!-- End Google Tag Manager --> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../index.html" class="icon icon-home"> Vitis™ AI
            <img src="../_static/xilinx-header-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                3.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Setup and Install</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reference/release_notes_3.0.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference/system_requirements.html">System Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/install.html">Host Install Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart/mpsoc.html">Zynq™ Ultrascale+™</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart/vck5000.html">Versal™ VCK5000 Development Card</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart/vck190.html">Versal™ AI Core VCK190</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Workflow and Components</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="workflow.html">Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">DPU IP Details and System Integration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#about-the-dpu-ip">About the DPU IP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vitis-ai-dpu-ip-and-reference-designs">Vitis AI DPU IP and Reference Designs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dpu-nomenclature">DPU Nomenclature</a></li>
<li class="toctree-l3"><a class="reference internal" href="#historic-dpu-nomenclature">Historic DPU Nomenclature</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dpu-options">DPU Options</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#versal-trade-ai-core-ai-edge-series-alveo-v70-dpucv2dx8g">Versal™ AI Core / AI Edge Series / Alveo V70: DPUCV2DX8G</a></li>
<li class="toctree-l4"><a class="reference internal" href="#zynq-trade-ultrascale-trade-mpsoc-dpuczdx8g">Zynq™ UltraScale+™ MPSoC: DPUCZDX8G</a></li>
<li class="toctree-l4"><a class="reference internal" href="#versal-trade-ai-core-series-dpucvdx8g">Versal™ AI Core Series: DPUCVDX8G</a></li>
<li class="toctree-l4"><a class="reference internal" href="#versal-trade-ai-core-series-dpucvdx8h">Versal™ AI Core Series: DPUCVDX8H</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#version-and-compatibility">Version and Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ip-and-reference-designs">IP and Reference Designs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#integrating-the-dpu">Integrating the DPU</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vitis-integration">Vitis Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vivado-integration">Vivado Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vitis-ai-linux-recipes">Vitis AI Linux Recipes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vitis-ai-online-installation">Vitis AI Online Installation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#runtime">Runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="#library">Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="#optimization-for-mpsoc-targets">Optimization for MPSoC Targets</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#linux-devicetree-bindings">Linux Devicetree Bindings</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rebuilding-the-linux-image-with-petalinux">Rebuilding the Linux Image With Petalinux</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-recipes-vitis-ai">Using recipes-vitis-ai</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-upgrade-petalinux-esdk">Using Upgrade Petalinux eSDK</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-memory-requirements-and-the-linux-cma">Model Memory Requirements and the Linux CMA</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#example-1">Example 1:</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-2">Example 2:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="workflow-model-zoo.html">Vitis™ AI Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow-model-development.html">Developing a Model for Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow-model-deployment.html">Deploying a Model with Vitis AI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reference/release_documentation.html">Vitis™ AI User Guides &amp; IP Product Guides</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/Vitis-AI-Tutorials">Vitis™ AI Developer Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="workflow-third-party.html">Third-party Inference Stack Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference/version_compatibility.html">IP and Tools Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/branching_tagging_strategy.html">Branching and Tagging Strategy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources and Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reference/additional_resources.html">Technical Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference/additional_resources.html#additional-resources">Additional Resources</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Related AMD Solutions</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/DPU-PYNQ">DPU-PYNQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/finn/">FINN &amp; Brevitas</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/inference-server/">Inference Server</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/amd/UIF">Unified Inference Frontend</a></li>
<li class="toctree-l1"><a class="reference external" href="https://ryzenai.docs.amd.com/en/latest/">Ryzen™ AI Developer Guide ~July 29</a></li>
<li class="toctree-l1"><a class="reference external" href="https://onnxruntime.ai/docs/execution-providers/community-maintained/Vitis-AI-ExecutionProvider.html">Vitis™ AI ONNX Runtime Execution Provider</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/VVAS/">Vitis™ Video Analytics SDK</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Vitis™ AI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>DPU IP Details and System Integration</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/workflow-system-integration.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="dpu-ip-details-and-system-integration">
<span id="workflow-dpu"></span><h1>DPU IP Details and System Integration<a class="headerlink" href="#dpu-ip-details-and-system-integration" title="Permalink to this heading">¶</a></h1>
<section id="about-the-dpu-ip">
<h2>About the DPU IP<a class="headerlink" href="#about-the-dpu-ip" title="Permalink to this heading">¶</a></h2>
<p>AMD uses the acronym D-P-U to identify soft accelerators that target deep-learning inference. These “<strong>D</strong> eep Learning <strong>P</strong> rocessing <strong>U</strong> nits” are a vital component of the Vitis AI solution. This (perhaps overloaded) term can refer to one of several potential accelerator architectures covering multiple network topologies.</p>
<p>A DPU comprises elements available in the AMD programmable logic fabric, such as DSP, BlockRAM, UltraRAM, LUTs, and Flip-Flops, or may be developed as a set of microcoded functions that are deployed on the AMD AI Engine, or “AI Engine” architecture. Furthermore, in the case of some applications, the DPU is likely to be comprised of programmable logic and AI Engine array resources.</p>
<p>An example of the DPUCZ, targeting Zynq™ Ultrascale+™ devices is displayed in the following image:</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../_images/DPUCZ.PNG"><img alt="../_images/DPUCZ.PNG" src="../_images/DPUCZ.PNG" style="width: 1300px;" /></a>
<figcaption>
<p><span class="caption-text">Features and Architecture of the Zynq Ultrascale+ DPUCZ</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Vitis AI provides the DPU IP and the required tools to deploy both standard and custom neural networks on AMD adaptable targets:</p>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="../_images/VAI-1000ft.PNG"><img alt="../_images/VAI-1000ft.PNG" src="../_images/VAI-1000ft.PNG" style="width: 1300px;" /></a>
<figcaption>
<p><span class="caption-text">Vitis AI 1000 Foot View</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Vitis AI DPUs are general-purpose AI inference accelerators. A single DPU instance in your design can enable you to deploy multiple CNNs simultaneously and process multiple streams simultaneously. The Processing depends on the DPU having sufficient parallelism to support the combination of the networks and the number of streams. Multiple DPU instances can be instantiated per device. The DPU can be scaled in size to accommodate the requirements of the user.</p>
<p>The Vitis AI DPU architecture is called a “Matrix of (Heterogeneous) Processing Engines.”  While on the surface, Vitis AI DPU architectures have some visual similarity to a systolic array; the similarity ends there. DPU is a micro-coded processor with its Instruction Set Architecture. Each DPU architecture has its own instruction set, and the Vitis AI Compiler compiles an executable <code class="docutils literal notranslate"><span class="pre">.Xmodel</span></code> to deploy for each network. The DPU executes the compiled instructions in the <code class="docutils literal notranslate"><span class="pre">.Xmodel</span></code>. The Vitis AI Runtime addresses the underlying tasks of scheduling the inference of multiple networks, multiple streams, and even multiple DPU instances. The mix of processing engines in the DPU is heterogeneous, with the DPU having different engines specialized for different tasks. For instance, CONV2D operators are accelerated in a purpose-built PE, while another process depthwise convolutions.</p>
<p>One advantage of this architecture is that there is no need to load a new bitstream or build a new hardware platform while changing the network.  This is an important differentiator from Data Flow accelerator architectures that are purpose-built for a single network.  That said, both the Matrix of Processing Engines and Data Flow architectures have a place in AMD designs.  If you need a highly optimized, specialized Data Flow accelerator for inference, refer to the <a class="reference external" href="https://xilinx.github.io/finn/">FINN &amp; Brevitas</a> solutions.  Data Flow architectures based on FINN can support inference at line rates for high-speed communications and extremely high sample rates for inference in the RF domain.  Neither of these two applications is a great fit for Vitis AI.  The reality is that both of these flows are complementary, and support for both can play an essential role in customer product differentiation and future-proofing.</p>
</section>
<section id="vitis-ai-dpu-ip-and-reference-designs">
<h2>Vitis AI DPU IP and Reference Designs<a class="headerlink" href="#vitis-ai-dpu-ip-and-reference-designs" title="Permalink to this heading">¶</a></h2>
<p>Today, AMD DPU IPs are not incorporated into the standard Vivado™ IP catalog and instead, the DPU IP is released embedded in a reference design.  Users can start with the reference design and modify it to suit their requirements.  The reference designs are fully functional and can be used as a template for IP integration and connectivity as well as Linux integration.</p>
<p>The DPU IP is also is released as a separate download that can be incorporated into a new or existing design by the developer.</p>
<section id="dpu-nomenclature">
<h3>DPU Nomenclature<a class="headerlink" href="#dpu-nomenclature" title="Permalink to this heading">¶</a></h3>
<p>There are a variety of different DPUs available for different tasks and AMD platforms. The following decoder helps extract the features, characteristics, and target hardware platforms from a given DPU name.</p>
<img alt="../_images/dpu_nomenclature_current.PNG" src="../_images/dpu_nomenclature_current.PNG" />
</section>
<section id="historic-dpu-nomenclature">
<h3>Historic DPU Nomenclature<a class="headerlink" href="#historic-dpu-nomenclature" title="Permalink to this heading">¶</a></h3>
<p>As of the Vitis™ 1.2 release, the historic DPUv1/v2/v3 nomenclature was deprecated. To better understand how these historic DPU names map into the current nomenclature, refer to the following table:</p>
<img alt="../_images/dpu_nomenclature_legacy_mapping.PNG" src="../_images/dpu_nomenclature_legacy_mapping.PNG" />
</section>
<section id="dpu-options">
<h3>DPU Options<a class="headerlink" href="#dpu-options" title="Permalink to this heading">¶</a></h3>
<section id="versal-trade-ai-core-ai-edge-series-alveo-v70-dpucv2dx8g">
<h4>Versal™ AI Core / AI Edge Series / Alveo V70: DPUCV2DX8G<a class="headerlink" href="#versal-trade-ai-core-ai-edge-series-alveo-v70-dpucv2dx8g" title="Permalink to this heading">¶</a></h4>
<p>The DPUCV2DX8G is a high-performance, general-purpose convolutional neural network(CNN)
processing engine optimized for AMD Versal™ Adaptive SoC devices containing AI-ML tiles. This
IP is user-configurable and exposes several parameters to configure the number of AI Engines
used and programmable logic (PL) resource utilization.</p>
</section>
<section id="zynq-trade-ultrascale-trade-mpsoc-dpuczdx8g">
<h4>Zynq™ UltraScale+™ MPSoC: DPUCZDX8G<a class="headerlink" href="#zynq-trade-ultrascale-trade-mpsoc-dpuczdx8g" title="Permalink to this heading">¶</a></h4>
<p>The DPUCZDX8G IP has been optimized for Zynq UltraScale+ MPSoC. You can integrate this IP
as a block in the programmable logic (PL) of the selected Zynq UltraScale+ MPSoCs with direct
connections to the processing system (PS). The DPU is user-configurable and exposes several
parameters which can be specified to optimize PL resources or customize enabled features.</p>
</section>
<section id="versal-trade-ai-core-series-dpucvdx8g">
<h4>Versal™ AI Core Series: DPUCVDX8G<a class="headerlink" href="#versal-trade-ai-core-series-dpucvdx8g" title="Permalink to this heading">¶</a></h4>
<p>The DPUCVDX8G is a high-performance general CNN processing engine optimized for the
Versal AI Core Series. The Versal devices can provide superior performance/watt over
conventional FPGAs, CPUs, and GPUs. The DPUCVDX8G is composed of AI Engines and PL
circuits. This IP is user-configurable and exposes several parameters which can be specified to
optimize AI Engines and PL resources or customize features.</p>
</section>
<section id="versal-trade-ai-core-series-dpucvdx8h">
<h4>Versal™ AI Core Series: DPUCVDX8H<a class="headerlink" href="#versal-trade-ai-core-series-dpucvdx8h" title="Permalink to this heading">¶</a></h4>
<p>The DPUCVDX8H is a high-performance and high-throughput general CNN processing engine
optimized for the Versal AI Core series. Besides traditional program logic, Versal devices integrate
high performance AI engine arrays, high bandwidth NoCs, DDR/LPDDR controllers, and other
high-speed interfaces that can provide superior performance/watt over conventional FPGAs,
CPUs, and GPUs. The DPUCVDX8H is implemented on Versal devices to leverage these benefits.
You can configure the parameters to meet your data center application requirements.</p>
</section>
</section>
<section id="version-and-compatibility">
<h3>Version and Compatibility<a class="headerlink" href="#version-and-compatibility" title="Permalink to this heading">¶</a></h3>
<p>As the user must incorporate the IP into the Vivado IP catalog themselves, it is very important to understand that the designs and IP in the table below were verified with specific versions of Vivado, Vitis, Petalinux and Vitis AI.  Please refer to <a class="reference internal" href="reference/version_compatibility.html#version-compatibility"><span class="std std-ref">Version Compatibility</span></a> for additional information.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is anticipated that users may wish to leverage the latest release of Vitis AI, Vitis or Vivado with DPU IP that has not been updated in this release.  For Adaptable SoC targets it is anticipated that the latest Vitis AI components such as Model Zoo models, Petalinux recipes, Quantizer, Compiler, VART and the Vitis AI Library can be directly leveraged by the user.  However, updated reference designs will no longer be provided for minor (x.5) Vitis AI releases for MPSoC and Versal AI Core targets.  Users are encouraged to use Vitis AI 3.0 for evaluation of those targets, and migrate to the Vitis AI 3.5 release if desired or necessary for production.</p>
</div>
<p>The table below associates currently available DPU IP with the supported target, and provides links to download the reference design and documentation.  For convenience, a separate IP repo is provided for users who do not wish to download the reference design.  The IP is thus included both in the reference design, but also is available as a separate download.</p>
</section>
<section id="ip-and-reference-designs">
<h3>IP and Reference Designs<a class="headerlink" href="#ip-and-reference-designs" title="Permalink to this heading">¶</a></h3>
<p>Users can download the DPU IP and reference design for their target platform from <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/3.0/dpu">the Vitis AI Github</a>.</p>
</section>
</section>
<section id="integrating-the-dpu">
<span id="id1"></span><h2>Integrating the DPU<a class="headerlink" href="#integrating-the-dpu" title="Permalink to this heading">¶</a></h2>
<p>The basic steps to build a platform that integrates a Vitis™ AI DPU are as follows:</p>
<p>1. A custom hardware platform is built using the Vitis software platform based on the Vitis
Target Platform. The generated hardware includes the DPU IP and other kernels. In the Vitis
AI release package, pre-built SD card images (for ZCU102/104, KV260, VCK190 and
VEK280) and Versal shells are included for quick start and application development. You can
also use the AMD Vitis™ or Vivado™ flows to integrate the DPU and build the custom hardware
to suit your need.</p>
<p>2. The Vitis AI toolchain in the host machine is used to build the model. It takes the pre-trained
floating models as the input and runs them through the AI Optimizer (optional), AI Quantizer and AI Compiler.</p>
<p>3. You can build executable software which runs on the built hardware. You can write your
applications with C++ or Python which calls the Vitis AI Runtime and Vitis AI Library to load
and run the compiled model files.</p>
<section id="vitis-integration">
<span id="id2"></span><h3>Vitis Integration<a class="headerlink" href="#vitis-integration" title="Permalink to this heading">¶</a></h3>
<p>The Vitis™ workflow specifically targets developers with a software-centric approach to AMD SoC system development. Vitis AI is differentiated from traditional FPGA flows, enabling you to build FPGA acceleration into your applications without developing RTL kernels.</p>
<p>The Vitis workflow enables the integration of the DPU IP as an acceleration kernel that is loaded at runtime in the form of an <code class="docutils literal notranslate"><span class="pre">xclbin</span></code> file. To provide developers with a reference platform that can be used as a starting point, the Vitis AI repository includes several <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/3.0/dpu">reference designs</a> for the different DPU architectures and target platforms.</p>
<p>In addition, a Vitis tutorial is available which provides the <a class="reference external" href="https://github.com/Xilinx/Vitis-Tutorials/tree/2022.1/Vitis_Platform_Creation/Design_Tutorials/02-Edge-AI-ZCU104">end-to-end workflow</a> for creating a Vitis Platform for ZCU104 targets.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/vitis_integration.PNG"><img alt="../_images/vitis_integration.PNG" src="../_images/vitis_integration.PNG" style="width: 1300px;" /></a>
</figure>
</section>
<section id="vivado-integration">
<span id="id3"></span><h3>Vivado Integration<a class="headerlink" href="#vivado-integration" title="Permalink to this heading">¶</a></h3>
<p>The Vivado™ workflow targets traditional FPGA developers. It is important to note that the DPU IP is not currently integrated into the Vivado IP catalog. Currently, in order to update support the latest operators and network topologies at the time of Vitis AI release, the IP is released asynchronously as a <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/3.0/dpu">reference design and IP repository</a>.</p>
<p>For more information, refer to the following resources:</p>
<ul class="simple">
<li><p>To integrate the DPU in a Vivado design, see this <a class="reference external" href="https://github.com/Xilinx/Vitis-AI-Tutorials/blob/2.0/Tutorials/Vitis-AI-Vivado-TRD/">tutorial</a>.</p></li>
<li><p>A quick-start example that assists you in deploying VART on Embedded targets is available <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/3.0/src/vai_runtime/quick_start_for_embedded.md">here</a>.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/vivado_integration.PNG"><img alt="../_images/vivado_integration.PNG" src="../_images/vivado_integration.PNG" style="width: 1300px;" /></a>
</figure>
</section>
<section id="vitis-ai-linux-recipes">
<span id="linux-dpu-recipes"></span><h3>Vitis AI Linux Recipes<a class="headerlink" href="#vitis-ai-linux-recipes" title="Permalink to this heading">¶</a></h3>
<p>Yocto and PetaLinux users will require bitbake recipes for the Vitis AI components that are compiled for the target. These recipes are provided in the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/v3.5/src/vai_petalinux_recipes">source code folder</a>.</p>
<p>Yocto and PetaLinux users will require bitbake recipes for the Vitis AI components that are compiled for the target. These recipes are provided in the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/3.0/src/vai_petalinux_recipes">source code folder</a>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>For Vitis AI releases &gt;= v2.0, Vivado users (Zynq™ Ultrascale+™ and Kria™ applications) must compile VART standalone without XRT. However, Vitis users must compile VART with XRT (required for Vitis kernel integration). All designs that leverage Vitis AI require VART, while all Alveo and Versal™ designs must include XRT. By default, the Vitis AI Docker images incorporate XRT. Perhaps most important is that the Linux bitbake recipe for VART <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/3.0/src/vai_petalinux_recipes/recipes-vitis-ai/vart/vart_3.0.bb#L17">assumes</a> by default that you are leveraging the Vitis flow. If you are leveraging the DPU in Vivado with Linux, you must either leverage <code class="docutils literal notranslate"><span class="pre">vart_3.0_vivado.bb</span></code> or, comment out the line <code class="docutils literal notranslate"><span class="pre">PACKAGECONFIG:append</span> <span class="pre">=</span> <span class="pre">&quot;</span> <span class="pre">vitis&quot;</span></code> in the <code class="docutils literal notranslate"><span class="pre">vart_3.0.bb</span></code> recipe in order to ensure that you are compiling VART without XRT. Failing to do so will result in runtime errors when executing VART APIs. Specifically, XRT, which is not compatible with Vivado will error out when it attempts to load an xclbin file, a kernel file that is absent in the Vivado flow.  Finally, be sure to only include one of the two bitbake recipes in the Petalinux build folder!</p>
</div>
<p>There are two ways to integrate the Vitis™ AI Library and Runtime in a custom design:</p>
<ul class="simple">
<li><p>Build the Linux image using Petalinux, incorporating the necessary recipes.</p></li>
<li><p>Install Vitis AI 3.0 to the target leveraging a pre-built package at run time.  For details of this procedure, please see <a class="reference internal" href="#vart-vail-online-install"><span class="std std-ref">Vitis AI Online Installation</span></a></p></li>
</ul>
</section>
<section id="vitis-ai-online-installation">
<span id="vart-vail-online-install"></span><h3>Vitis AI Online Installation<a class="headerlink" href="#vitis-ai-online-installation" title="Permalink to this heading">¶</a></h3>
<section id="runtime">
<h4>Runtime<a class="headerlink" href="#runtime" title="Permalink to this heading">¶</a></h4>
<p>If you have an updated version of the Vitis AI Runtime (perhaps you have made changes to the source code) and simply wish to install the update to your target without rebuilding Petalinux, follow these steps.</p>
<blockquote>
<div><ul>
<li><p>Copy the board_setup/[TARGET] folder to the board using scp, where [TARGET] = {mpsoc, vck190} and [IP_OF_TARGET] is the IP address of the target board.</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>scp -r board_setup/<span class="o">[</span>TARGET<span class="o">]</span> root@<span class="o">[</span>IP_OF_TARGET<span class="o">]</span>:~/
</pre></div>
</div>
</li>
<li><p>Log in to the board using ssh. You can also use the serial port to login.</p></li>
<li><p>Now, install the Vitis AI Runtime.</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> ~/<span class="o">[</span>TARGET<span class="o">]</span>
bash target_vart_setup.sh
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</section>
<section id="library">
<h4>Library<a class="headerlink" href="#library" title="Permalink to this heading">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Vitis AI 3.0 RPM packages are now available.</p>
</div>
<p>To install the Vitis AI Library on your target you can attempt the following procedure.  This procedure may be useful if you have a custom hardware target that you wish to enable.</p>
<ol class="arabic simple">
<li><p>Verify what version, if any, of the Vitis AI Library is currently installed on the target.</p></li>
</ol>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ cat /etc/yum.repos.d/oe-remote-repo-sswreleases-rel-v2022-generic-rpm.repo
</pre></div>
</div>
</div></blockquote>
<p>The result will be similar to the following:</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>oe-remote-repo-sswreleases-rel-v2022-generic-rpm-cortexa72_cortexa53<span class="o">]</span>
<span class="nv">name</span><span class="o">=</span>OE Remote Repo: sswreleases rel-v2022 generic rpm cortexa72_cortexa53
<span class="nv">baseurl</span><span class="o">=</span>http://petalinux.xilinx.com/sswreleases/rel-v2022/generic/rpm/cortexa72_cortexa53  <span class="o">(</span>contains vai2.5 and vai3.0 rpm packages<span class="o">)</span>
<span class="nv">gpgcheck</span><span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Execute the command to install the latest version of the Vitis AI Library.</p></li>
</ol>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ dnf install vitis-ai-library
</pre></div>
</div>
</div></blockquote>
<p>If a version of the Vitis AI Library is installed on the target, the result will be similar to the following.</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>Last metadata expiration check: <span class="m">2</span>:18:44 ago on Sun Jun <span class="m">25</span> <span class="m">20</span>:53:29 <span class="m">2023</span>.
Package vitis-ai-library-3.0-r0.0.cortexa72_cortexa53 is already installed.
Dependencies resolved.
Nothing to <span class="k">do</span>.
Complete!
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>If you need to remove the installed Vitis AI Library, execute this command.</p></li>
</ol>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>Target<span class="o">]</span> $ dnf remove vitis-ai-library
</pre></div>
</div>
</div></blockquote>
<p>The result will be similar to the following:</p>
<blockquote>
<div><div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span>Transaction <span class="nv">Summary</span>
<span class="o">============================================================================================================================</span>
Remove  <span class="m">1001</span> Packages

Freed space: <span class="m">857</span> M
Is this ok <span class="o">[</span>y/N<span class="o">]</span>: y

Erasing… please <span class="nb">wait</span> a <span class="k">while</span>

and <span class="k">then</span> “dnf install vitis-ai-library”:

<span class="o">============================================================================================================================</span>
Package          Architecture        Version    Repository                                                            <span class="nv">Size</span>
<span class="o">============================================================================================================================</span>
Installing:
vitis-ai-library cortexa72_cortexa53 <span class="m">3</span>.0-r0.0   oe-remote-repo-sswreleases-rel-v2022-generic-rpm-cortexa72_cortexa53 <span class="m">3</span>.2 M
Installing dependencies:
python3-protobuf cortexa72_cortexa53 <span class="m">3</span>.17.3-r0.0
oe-remote-repo-sswreleases-rel-v2022-generic-rpm-cortexa72_cortexa53 <span class="m">291</span> k
target-factory   cortexa72_cortexa53 <span class="m">3</span>.0-r0.0   oe-remote-repo-sswreleases-rel-v2022-generic-rpm-cortexa72_cortexa53  <span class="m">98</span> k
unilog           cortexa72_cortexa53 <span class="m">3</span>.0-r0.0   oe-remote-repo-sswreleases-rel-v2022-generic-rpm-cortexa72_cortexa53  <span class="m">25</span> k
vart             cortexa72_cortexa53 <span class="m">3</span>.0-r0.0   oe-remote-repo-sswreleases-rel-v2022-generic-rpm-cortexa72_cortexa53 <span class="m">697</span> k
xir              cortexa72_cortexa53 <span class="m">3</span>.0-r0.0   oe-remote-repo-sswreleases-rel-v2022-generic-rpm-cortexa72_cortexa53 <span class="m">754</span> k

Transaction <span class="nv">Summary</span>
<span class="o">============================================================================================================================</span>
Install  <span class="m">6</span> Packages

Total download size: <span class="m">5</span>.0 M
Installed size: <span class="m">28</span> M
Is this ok <span class="o">[</span>y/N<span class="o">]</span>: y

Please <span class="nb">wait</span> a <span class="k">while</span>

Installed:
  python3-protobuf-3.17.3-r0.0.cortexa72_cortexa53                target-factory-3.0-r0.0.cortexa72_cortexa53
  unilog-3.0-r0.0.cortexa72_cortexa53                             vart-3.0-r0.0.cortexa72_cortexa53
  vitis-ai-library-3.0-r0.0.cortexa72_cortexa53                   xir-3.0-r0.0.cortexa72_cortexa53

Complete!
</pre></div>
</div>
</div></blockquote>
</section>
<section id="optimization-for-mpsoc-targets">
<h4>Optimization for MPSoC Targets<a class="headerlink" href="#optimization-for-mpsoc-targets" title="Permalink to this heading">¶</a></h4>
<p>For custom MPSOC targets you can optionally run <code class="docutils literal notranslate"><span class="pre">zynqmp_dpu_optimize.sh</span></code> to optimize board settings.</p>
<blockquote>
<div><p>The script runs automatically after the board boots up with the official image. But you can also find the <code class="docutils literal notranslate"><span class="pre">dpu_sw_optimize.tar.gz</span></code> in <a class="reference external" href="https://www.xilinx.com/bin/public/openDownload?filename=DPUCZDX8G.tar.gz">DPUCZDX8G.tar.gz</a>.</p>
<div class="highlight-Bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> ~/dpu_sw_optimize/zynqmp/
./zynqmp_dpu_optimize.sh
</pre></div>
</div>
</div></blockquote>
</section>
</section>
<section id="linux-devicetree-bindings">
<h3>Linux Devicetree Bindings<a class="headerlink" href="#linux-devicetree-bindings" title="Permalink to this heading">¶</a></h3>
<p>When using the PetaLinux flow, the Linux Devicetree nodes for the DPU are automatically generated.  If modifications are made to the DPU IP parameters in the hardware design, changes to the .xsa must be propagated to PetaLinux in order to ensure that the corresponding changes to the Devicetree bindings are propagated to the software platform.</p>
<p>It is recognized that not all users will leverage PetaLinux.  Users choosing to deviate from the PetaLinux flow (eg, Yocto users) may require additional resources.  The following are suggested for additional reading:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18841988/Build+Device+Tree+Compiler+dtc">Build the Devicetree Compiler</a></p></li>
<li><p><a class="reference external" href="https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/341082130/Quick+guide+to+Debugging+Device+Tree+Generator+Issues">Debugging Devicetree Issues</a></p></li>
</ol>
<p>In addition, it is worth noting that documentation for the Vitis AI DPUCZ Devicetree bindings can be <a class="reference external" href="https://github.com/Xilinx/linux-xlnx/blob/master/Documentation/devicetree/bindings/misc/xlnx%2Cdpu.yaml">found here</a> .  These are relevant only to the DPUCZ.</p>
</section>
<section id="rebuilding-the-linux-image-with-petalinux">
<h3>Rebuilding the Linux Image With Petalinux<a class="headerlink" href="#rebuilding-the-linux-image-with-petalinux" title="Permalink to this heading">¶</a></h3>
<p>Most developers will need to build a Petalinux or Yocto Vitis AI 3.0 image for their platform. You can obtain the recipes for Vitis AI 3.0 in the following two ways:</p>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">recipes-vitis-ai</span></code> in this repo.</p></li>
<li><p>Upgrading the Petalinux eSDK.</p></li>
</ul>
<section id="using-recipes-vitis-ai">
<h4>Using recipes-vitis-ai<a class="headerlink" href="#using-recipes-vitis-ai" title="Permalink to this heading">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">recipes-vitis-ai</span></code> enables <strong>Vitis flow by default</strong>. Recipes for both Vivado and Vitis are provided. In the Vivado recipe, the following line is commented out:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#PACKAGECONFIG_append = &quot; vitis&quot;</span>
</pre></div>
</div>
</div>
<ol class="arabic">
<li><p>Copy the <code class="docutils literal notranslate"><span class="pre">recipes-vitis-ai</span></code> folder to <code class="docutils literal notranslate"><span class="pre">&lt;petalinux</span> <span class="pre">project&gt;/project-spec/meta-user/</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp Vitis-AI/src/petalinux_recipes/recipes-vitis-ai &lt;petalinux project&gt;/project-spec/meta-user/
</pre></div>
</div>
</li>
<li><p>Delete either <code class="docutils literal notranslate"><span class="pre">recipes-vitis-ai/vart/vart_3.0.bb</span></code> or <code class="docutils literal notranslate"><span class="pre">recipes-vitis-ai/vart/vart_3.0_vivado.bb</span></code> depending on workflow that you have selected for your design.  If you use <code class="docutils literal notranslate"><span class="pre">recipes-vitis-ai/vart/vart_3.0_vivado.bb</span></code> please rename it <code class="docutils literal notranslate"><span class="pre">recipes-vitis-ai/vart/vart_3.0.bb</span></code>.</p></li>
<li><p>Edit <code class="docutils literal notranslate"><span class="pre">&lt;petalinux</span> <span class="pre">project&gt;/project-spec/meta-user/conf/user-rootfsconfig</span></code>
file, appending the following lines:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CONFIG_vitis</span><span class="o">-</span><span class="n">ai</span><span class="o">-</span><span class="n">library</span>
<span class="n">CONFIG_vitis</span><span class="o">-</span><span class="n">ai</span><span class="o">-</span><span class="n">library</span><span class="o">-</span><span class="n">dev</span>
<span class="n">CONFIG_vitis</span><span class="o">-</span><span class="n">ai</span><span class="o">-</span><span class="n">library</span><span class="o">-</span><span class="n">dbg</span>
</pre></div>
</div>
</li>
<li><p>Source PetaLinux tool and run <code class="docutils literal notranslate"><span class="pre">petalinux-config</span> <span class="pre">-c</span> <span class="pre">rootfs</span></code> command. Select the following option.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Select</span> <span class="n">user</span> <span class="n">packages</span> <span class="o">---&gt;</span>
<span class="n">Select</span> <span class="p">[</span><span class="o">*</span><span class="p">]</span> <span class="n">vitis</span><span class="o">-</span><span class="n">ai</span><span class="o">-</span><span class="n">library</span>
</pre></div>
</div>
<p>Then, save it and exit.</p>
</li>
<li><p>Run <code class="docutils literal notranslate"><span class="pre">petalinux-build</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp Vitis-AI/src/petalinux_recipes/recipes-vai-kernel &lt;petalinux project&gt;/project-spec/meta-user/
</pre></div>
</div>
</li>
</ol>
</section>
<section id="using-upgrade-petalinux-esdk">
<h4>Using Upgrade Petalinux eSDK<a class="headerlink" href="#using-upgrade-petalinux-esdk" title="Permalink to this heading">¶</a></h4>
<p>Run the following commands to upgrade PetaLinux.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">source</span> &lt;petalinux-v2022.2&gt;/settings
petalinux-upgrade -u ‘http://petalinux.xilinx.com/sswreleases/rel-v2022/sdkupdate/2022.2_update1/’ -p ‘aarch64’
</pre></div>
</div>
<p>Following this upgrade, you will find <code class="docutils literal notranslate"><span class="pre">vitis-ai-library_3.0.bb</span></code> recipe in <code class="docutils literal notranslate"><span class="pre">&lt;petalinux</span> <span class="pre">project&gt;/components/yocto/layers/meta-vitis-ai</span></code>.</p>
<p>For details about this process, refer to <a class="reference external" href="https://docs.xilinx.com/r/en-US/ug1144-petalinux-tools-reference-guide/petalinux-upgrade-Option">Petalinux Upgrade</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">2022.2_update1</span></code> will be released approximately 1 month after Vitis 3.0 release. The name of <code class="docutils literal notranslate"><span class="pre">2022.2_update1</span></code> may be changed. Modify it accordingly.</p>
</div>
</section>
</section>
<section id="model-memory-requirements-and-the-linux-cma">
<h3>Model Memory Requirements and the Linux CMA<a class="headerlink" href="#model-memory-requirements-and-the-linux-cma" title="Permalink to this heading">¶</a></h3>
<p>Contiguous memory is required for the deployment of models on the DPU.  It is thus important that developers understand that they have to allocate memory accordingly.  If this is not evaluated, the user may find that model deployment fails.  One sign that this is a problem is that an error is issued when attempting to execute the model.  Below is an example:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Out of memory: Killed process <span class="m">12349</span> <span class="o">(</span>python3<span class="o">)</span> total-vm:1943752kB, anon-rss:1142200kB, file-rss:1284kB, shmem-rss:0kB, UID:0 pgtables:3808kB oom_score_adj:0
Out of memory: Killed process
</pre></div>
</div>
</div></blockquote>
<p>This section provides guidelines on estimating the CMA memory space required for model deployment. To perform this estimation, you will need to run <cite>xdputil</cite> on the neural network of interest.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ xdputil xmodel ./somemodel.xmodel  -l
</pre></div>
</div>
</div></blockquote>
<p>The <code class="docutils literal notranslate"><span class="pre">reg</span> <span class="pre">info</span></code> section of the output of this command provides details of various registers and their corresponding sizes.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>...
<span class="s2">&quot;reg info&quot;</span>:<span class="o">[</span>
                <span class="o">{</span>
                        <span class="s2">&quot;name&quot;</span>:<span class="s2">&quot;REG_0&quot;</span>,
                        <span class="s2">&quot;context type&quot;</span>:<span class="s2">&quot;CONST&quot;</span>,
                        <span class="s2">&quot;size&quot;</span>:5310976
                <span class="o">}</span>,
                <span class="o">{</span>
                        <span class="s2">&quot;name&quot;</span>:<span class="s2">&quot;REG_1&quot;</span>,
                        <span class="s2">&quot;context type&quot;</span>:<span class="s2">&quot;WORKSPACE&quot;</span>,
                        <span class="s2">&quot;size&quot;</span>:2182976
                <span class="o">}</span>,
                <span class="o">{</span>
                        <span class="s2">&quot;name&quot;</span>:<span class="s2">&quot;REG_2&quot;</span>,
                        <span class="s2">&quot;context type&quot;</span>:<span class="s2">&quot;DATA_LOCAL_INPUT&quot;</span>,
                        <span class="s2">&quot;size&quot;</span>:150528
                <span class="o">}</span>,
                <span class="o">{</span>
                        <span class="s2">&quot;name&quot;</span>:<span class="s2">&quot;REG_3&quot;</span>,
                        <span class="s2">&quot;context type&quot;</span>:<span class="s2">&quot;DATA_LOCAL_OUTPUT&quot;</span>,
                        <span class="s2">&quot;size&quot;</span>:1024
                <span class="o">}</span>
        <span class="o">]</span>,
        <span class="s2">&quot;instruction reg&quot;</span>:45600
</pre></div>
</div>
</div></blockquote>
<p>Here is what each register represents:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">REG_0</span></code> with context type <code class="docutils literal notranslate"><span class="pre">CONST</span></code> refers to the weights and biases used in the model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">REG_1</span></code> with context type <code class="docutils literal notranslate"><span class="pre">WORKSPACE</span></code> refers to the space required to store intermediate results.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">REG_2</span></code> with context type <code class="docutils literal notranslate"><span class="pre">DATA_LOCAL_INPUT</span></code> refers to the space required by the input.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">REG_3</span></code> with context type <code class="docutils literal notranslate"><span class="pre">DATA_LOCAL_OUTPUT</span></code> refers to the space required by the output.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">instruction</span> <span class="pre">reg</span></code> value represents the space required by the DPU (Deep Learning Processing Unit) instructions. All sizes are specified in bytes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some large networks may have additional registers (for example, <code class="docutils literal notranslate"><span class="pre">REG_4</span></code>, <code class="docutils literal notranslate"><span class="pre">REG_5</span></code>). The <code class="docutils literal notranslate"><span class="pre">context</span> <span class="pre">type</span></code> indicates the kind of space required.  If additional registers are listed, the developer must incorporate these into their assessment.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the previous example, <cite>xdputil</cite> reports that <code class="docutils literal notranslate"><span class="pre">REG_3</span></code> will use 1024 bytes. However, the <em>actual memory requirement may be implementation and kernel dependent</em>.  The exercise of understanding the use of the Linux Contiguous Memory Allocator for their specific kernel and implementation is left for the user.  Below is a simple example that illustrates some of the factors that the user may need to consider.</p>
</div>
<p>Let’s consider an implementation in which the minimum CMA page size is 4096 bytes.  This allocation for <code class="docutils literal notranslate"><span class="pre">REG_3</span></code> may be achieved using PAGE_ALIGN(1024). The <cite>PAGE_ALIGN</cite> function rounds up an address or memory size to the next multiple of 4096 (or the PAGE_SIZE), adds 4095, and clears the low 12 bits.</p>
<p>For the formulas provided below, the following additional terms must be defined:</p>
<ul class="simple">
<li><p><cite>T</cite> threads represent the number of model instances (usually 1 instance per thread).</p></li>
<li><p><cite>dpu_cores</cite> indicate the number of DPU cores. If the number of model instances is less than the number of DPU cores, not all the DPU cores are used. The value of <cite>dpu_cores</cite> is calculated as the minimum of the number of DPU cores and the number of model instances.</p></li>
</ul>
<p><cite>dpu_cores</cite> = min (number of dpu cores, number of single model instances)</p>
<p>In the formulas below, note that:</p>
<ul class="simple">
<li><p><cite>const_space</cite> = <cite>PAGE_ALIGN</cite> (<code class="docutils literal notranslate"><span class="pre">CONST</span></code> space)</p></li>
<li><p><cite>work_space</cite> = sum of all <cite>PAGE_ALIGN</cite> (each <code class="docutils literal notranslate"><span class="pre">WORKSPACE</span></code> space)</p></li>
<li><p><cite>in_space</cite> = sum of all <cite>PAGE_ALIGN</cite> (each <code class="docutils literal notranslate"><span class="pre">DATA_LOCAL_INPUT</span></code> space)</p></li>
<li><p><cite>out_space</cite> = sum of all <cite>PAGE_ALIGN</cite> (each <code class="docutils literal notranslate"><span class="pre">DATA_LOCAL_OUTPUT</span></code> space)</p></li>
<li><p><cite>instr_space</cite> = <cite>PAGE_ALIGN</cite> (<code class="docutils literal notranslate"><span class="pre">instruction</span> <span class="pre">reg</span></code> space)</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using the DPUCZDX8G IP, am additional fixed chunk of 5MB CMA memory is required if the user enables hardware Softmax.</p>
</div>
<section id="example-1">
<h4>Example 1:<a class="headerlink" href="#example-1" title="Permalink to this heading">¶</a></h4>
<p>When running <cite>B</cite> batches with <cite>T</cite> threads and <cite>D</cite> dpu_cores the required CMA can be computed as follows:</p>
<p>CMA (min required) = <cite>const_space</cite> + <cite>instr_space</cite> + <cite>B</cite> * (<cite>D</cite> * <cite>work_space</cite> + <cite>T</cite> * (<cite>in_space</cite> + <cite>out_space</cite>))</p>
</section>
<section id="example-2">
<h4>Example 2:<a class="headerlink" href="#example-2" title="Permalink to this heading">¶</a></h4>
<p>When running two neural networks (model1 and model2) with <cite>B</cite> batches, <cite>D</cite> dpu_cores, <cite>T1</cite> threads for model1, and <cite>T2</cite> threads for model2, the required CMA can be computed as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Use <cite>xdputil</cite> to run each model. D1 &amp; D2 need to be calculated separately.</p>
</div>
<ol class="arabic simple">
<li><p>CMA (min required for model1) = <cite>const_space1</cite> + <cite>instr_space1</cite> + <cite>B</cite> * (<cite>D1</cite> * <cite>work_space1</cite> + <cite>T1</cite> * (<cite>in_space1</cite> + <cite>out_space1</cite>))</p></li>
<li><p>CMA (min required for model2) = <cite>const_space2</cite> + <cite>instr_space2</cite> + <cite>B</cite> * (<cite>D2</cite> * <cite>work_space2</cite> + <cite>T2</cite> * (<cite>in_space2</cite> + <cite>out_space2</cite>))</p></li>
<li><p>CMA (min total) = required CMA for model1 + required CMA for model2</p></li>
</ol>
</section>
</section>
</section>
</section>


           </div>
          </div>
          
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="workflow.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="workflow-model-zoo.html" class="btn btn-neutral float-right" title="Vitis AI Model Zoo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2023, Advanced Micro Devices, Inc.
      <span class="lastupdated">Last updated on July 19, 2023.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>