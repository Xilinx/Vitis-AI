{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Tensorflow\n",
    "\n",
    "This tutorial demonstrates the steps required to prepare and deploy a trained Tensorflow model for FPGA acceleration using Xilinx MLSuite:  \n",
    "1. **Quantize the model** - The quantizer will generate scaling parameters for quantizing floats INT8. This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve more parallelization at lower power. \n",
    "2. **Subgraph Cutting and Compilation** - In this step, the original graph is cut, compiled and a custom FPGA accelerated python layer is inserted to be used for Inference. \n",
    "4. **Classification** - In this step, the modified Tensorflow model from the previous step are run on the FPGA to perform inference on an input image.\n",
    "    \n",
    "## Prerequisite Files \n",
    "1. **Model files** - This notebook requires that model files are located in  \n",
    "  `$VAI_ALVEO_ROOT/examples/tensorflow/models`\n",
    "2. **Image files** - This notebook requires ilsvrc2012 image files are downloaded in  \n",
    "  `HOME/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/`\n",
    "3. **Model related parameters** Edit the \"USER EDITABLE:\" portion of \"util.py\" according to your model parameters. (The default parameters provided in this file are relevant for inception_v1 and resnet_50 examples below)\n",
    "  \n",
    "## Setup (Before Running Notebook)\n",
    "This notebook should be run inside a tensorflow docker container.\n",
    "\n",
    "Download the models to \"/opt/models/tensorflow\":\n",
    "```\n",
    "$ python $VAI_ALVEO_ROOT/examples/tensorflow/getModels.py\n",
    "```\n",
    "\n",
    "Download 500 calibration images into \"~/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/\":\n",
    "```\n",
    "$ python -m ck pull repo:ck-env\n",
    "$ python -m ck install package:imagenet-2012-val-min\n",
    "$ python -m ck install package:imagenet-2012-aux\n",
    "$ head -n 500 ~/CK-TOOLS/dataset-imagenet-ilsvrc2012-aux/val.txt > ~/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/val.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from IPython.display import Image as display\n",
    "from ipywidgets import interact\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from util import top5_accuracy\n",
    "from vai.dpuv1.rt.xdnn_rt_tf import TFxdnnRT as xdnnRT\n",
    "from vai.dpuv1.rt.xdnn_util import make_list\n",
    "from vai.dpuv1.rt.xdnn_io import default_xdnn_arg_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables (obtained by running \"source overlaybins/setup.sh\")\n",
    "HOME             = os.getenv('HOME','/home/mluser/')\n",
    "VAI_ALVEO_ROOT   = os.getenv('VAI_ALVEO_ROOT',os.getcwd()+'/../')\n",
    "\n",
    "if os.path.isdir(os.path.join(VAI_ALVEO_ROOT, 'overlaybins','xdnnv3')):\n",
    "    XCLBIN = os.path.join(VAI_ALVEO_ROOT, 'overlaybins', 'xdnnv3')\n",
    "else:\n",
    "    XCLBIN = os.path.join('/opt/xilinx', 'overlaybins', 'xdnnv3')\n",
    "\n",
    "if 'VAI_ALVEO_ROOT' in os.environ and os.path.isdir(os.path.join(os.environ['VAI_ALVEO_ROOT'], 'vai/dpuv1')):\n",
    "      ARCH_JSON = os.path.join(os.environ['VAI_ALVEO_ROOT'], 'vai/dpuv1/tools/compile/bin/arch.json')\n",
    "else:\n",
    "      ARCH_JSON = os.path.join(os.environ['VAI_ROOT'], 'compiler/arch/dpuv1/ALVEO/ALVEO.json')\n",
    "    \n",
    "MODELDIR   = VAI_ALVEO_ROOT + \"/examples/tensorflow/models/container/tensorflow/\"\n",
    "IMAGEDIR   = HOME + \"/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/\"\n",
    "IMAGELIST  = HOME + \"/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/val.txt\"\n",
    "LABELSLIST = HOME + \"/CK-TOOLS/dataset-imagenet-ilsvrc2012-aux/synset_words.txt\"\n",
    "\n",
    "print(\"Running w/ HOME: %s\" % HOME)\n",
    "print(\"Running w/ VAI_ALVEO_ROOT: %s\" % VAI_ALVEO_ROOT)\n",
    "print(\"Running w/ XCLBIN: %s\" % XCLBIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Choose a model\n",
    "Choose a model using the drop down, or select custom, and enter your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantInfo = MODELDIR + 'quantization_fix_info.txt'\n",
    "\n",
    "@interact(MODEL=['resnet50','inception_v1','custom'])\n",
    "def selectModel(MODEL):\n",
    "    global protoBuffer, inputNode, outputNode, inputShape, means, pre_process\n",
    "\n",
    "    default_protoBuffer = {'resnet50': 'resnet50_baseline.pb', 'inception_v1': 'inception_v1_baseline.pb', 'pedestrian_attribute': 'pedestrian_attributes_recognition_quantizations.pb'}\n",
    "    default_inputNode   = {'resnet50': 'data', 'inception_v1': 'data', 'pedestrian_attribute': 'data'}\n",
    "    default_outputNode  = {'resnet50': 'prob', 'inception_v1': 'loss3_loss3', 'pedestrian_attribute': 'pred_upper,pred_lower,pred_gender,pred_hat,pred_bag,pred_handbag,pred_backpack'}\n",
    "    default_inputShape  = {'resnet50': '224,224', 'inception_v1': '224,224', 'pedestrian_attribute': '224,128'}\n",
    "    default_means       = {'resnet50': '104,117,124', 'inception_v1': '104,117,124', 'pedestrian_attribute': '104,117,124'}\n",
    "\n",
    "    if MODEL == \"custom\":\n",
    "        protoBuffer = None\n",
    "        inputNode   = None\n",
    "        outputNode  = None\n",
    "        inputShape  = None\n",
    "        means       = None\n",
    "        pre_process = None\n",
    "    else:\n",
    "        protoBuffer = MODELDIR + default_protoBuffer[MODEL]\n",
    "        inputNode   = default_inputNode[MODEL]\n",
    "        outputNode  = default_outputNode[MODEL]\n",
    "        inputShape  = default_inputShape[MODEL]\n",
    "        means       = default_means[MODEL]\n",
    "        pre_process = MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not protoBuffer:\n",
    "    @interact(PROTOBUFFER=\"Provide the path to your protobuffer\")\n",
    "    def selectTFmodel(PROTOBUFFER):\n",
    "        global protoBuffer\n",
    "        protoBuffer = PROTOBUFFER\n",
    "\n",
    "if not quantInfo:\n",
    "    @interact(QUANTINFO=\"Provide the path to your quantization file\")\n",
    "    def selectTFmodel(QUANTINFO):\n",
    "        global quantInfo\n",
    "        quantInfo = QUANTINFO\n",
    "\n",
    "if not inputNode:\n",
    "    @interact(INPUTNODE=\"Provide the input node(s) (comma separated string with no spaces)\")\n",
    "    def selectTFmodel(INPUTNODE):\n",
    "        global inputNode\n",
    "        inputNode = INPUTNODE\n",
    "\n",
    "if not outputNode:\n",
    "    @interact(OUTPUTNODE=\"Provide the output node(s) (comma separated string with no spaces)\")\n",
    "    def selectTFmodel(OUTPUTNODE):\n",
    "        global outputNode\n",
    "        outputNode = OUTPUTNODE\n",
    "\n",
    "if not inputShape:\n",
    "    @interact(INPUTSHAPE=\"Provide the input shapes (comma separated string with no spaces)\")\n",
    "    def selectTFmodel(INPUTSHAPE):\n",
    "        global inputShape\n",
    "        inputShape = INPUTSHAPE\n",
    "\n",
    "if not means:\n",
    "    @interact(MEANS=\"Provide the means (comma separated string with no spaces)\")\n",
    "    def selectTFmodel(MEANS):\n",
    "        global means\n",
    "        means = MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running with protoBuffer:   %s\" % protoBuffer)\n",
    "print(\"Running with quantInfo:     %s\" % quantInfo)\n",
    "print(\"Running with inputNode:     %s\" % inputNode)\n",
    "print(\"Running with outputNode:    %s\" % outputNode)\n",
    "print(\"Running with inputShape:    %s\" % inputShape)\n",
    "print(\"Running with means:         %s\" % means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Run the Quantizer\n",
    "\n",
    "Inspect the model to gather its input and output node(s), and input nodes' shape.  Next, quantize the model using graph and sample data parameters.  This quantization process produces two protobuf files containing the quantization information.  Finally, extract the quantization information using the compiler.  The end result is a txt file holding the scaling parameters for quantizing floats to INT8.  This is required because FPGAs will take advantage of Fixed Point Precision to achieve accelerated inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vai_q_tensorflow inspect --input_frozen_graph $protoBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vai_q_tensorflow quantize \\\n",
    "    --input_frozen_graph $protoBuffer \\\n",
    "    --input_nodes        $inputNode \\\n",
    "    --output_nodes       $outputNode \\\n",
    "    --input_shapes       ?,$inputShape,3 \\\n",
    "    --output_dir         $MODELDIR \\\n",
    "    --input_fn           util.input_fn_$pre_process \\\n",
    "    --method             1 \\\n",
    "    --calib_iter         100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vai_c_tensorflow \\\n",
    "    --frozen_pb          $MODELDIR/deploy_model.pb \\\n",
    "    --arch               $ARCH_JSON \\\n",
    "    --output_dir         $MODELDIR \\\n",
    "    --net_name           $quantInfo \\\n",
    "    --quant_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Partitioner and Compiler\n",
    "\n",
    "The partitioner takes in the model protoBuffer, and pre-computer quantization parameters and compiles the porttion of the network specified from starnode(s) to finalnode(s) for FPGA acceleration.\n",
    "\n",
    "In case startnode and/or finalnode is not specified (i.e., an empty list) the corresponding endnode is infered from the protoBuffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(startnode=inputNode, finalnode=outputNode):\n",
    "    return {\n",
    "        ### Some standard partitioner arguments [EDITABLE]\n",
    "        'startnode':            startnode,\n",
    "        'finalnode':            finalnode,\n",
    "        \n",
    "        ### Some standard compiler arguments [PLEASE DONT TOUCH]\n",
    "        'dsp':                  96,\n",
    "        'memory':               9,\n",
    "        'bytesperpixels':       1,\n",
    "        'ddr':                  256,\n",
    "        'data_format':          'NHWC',\n",
    "        'mixmemorystrategy':    True,\n",
    "        'noreplication':        True,\n",
    "        'xdnnv3':               True,\n",
    "        'usedeephi':            True,\n",
    "        'quantz':               ''  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load default arguments\n",
    "FLAGS, unparsed = default_xdnn_arg_parser().parse_known_args([])\n",
    "\n",
    "### Partition and compile\n",
    "rt = xdnnRT(FLAGS,\n",
    "            networkfile=protoBuffer,\n",
    "            quant_cfgfile=quantInfo,\n",
    "            xclbin=XCLBIN,\n",
    "            device='FPGA',\n",
    "            placeholdershape=\"{{'{}':[1,{},{},3]}}\".format(inputNode,*[int(x) for x in inputShape.split(',')]),\n",
    "            **get_args(inputNode, outputNode)\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Inference \n",
    "\n",
    "If the model in protoBuffer includes all the pre and post processings, the inference can be done simply by passing the input_data as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-processing function\n",
    "def preprocess(image):\n",
    "    input_height, input_width = 224, 224\n",
    "\n",
    "    ## Image preprocessing using numpy\n",
    "    img  = cv2.imread(image).astype(np.float32)\n",
    "    img -= np.array(make_list(means)).reshape(-1,3).astype(np.float32)\n",
    "    img  = cv2.resize(img, (input_width, input_height))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose image to run, display it for reference\n",
    "image  = IMAGEDIR + \"ILSVRC2012_val_00000003.JPEG\"\n",
    "\n",
    "display(filename=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accelerated execution\n",
    "\n",
    "## load the accelerated graph\n",
    "graph = rt.load_partitioned_graph()\n",
    "\n",
    "## run the tensorflow graph as usual (additional operations can be added to the graph)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    input_tensor  = graph.get_operation_by_name(inputNode).outputs[0]\n",
    "    output_tensor = graph.get_operation_by_name(outputNode).outputs[0]\n",
    "    \n",
    "    predictions = sess.run(output_tensor, feed_dict={input_tensor: [preprocess(image)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.loadtxt(LABELSLIST, str, delimiter='\\t')\n",
    "top_k = predictions[0].argsort()[:-6:-1]\n",
    "\n",
    "for l,p in zip(labels[top_k], predictions[0][top_k]):\n",
    "    print (l,\" : \",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_cnt = 100 \n",
    "batch_size = 1\n",
    "label_offset = 0\n",
    "\n",
    "top5_accuracy(graph, inputNode, outputNode, iter_cnt, batch_size, pre_process, label_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This notebook demonstrates how to target Xilinx FPGAs for inference using TensorFlow.  \n",
    "\n",
    "When the time comes to take your application to production please look at examples in /opt/ml-suite/examples/deployment_modes/  \n",
    "Highest performance is acheived by creating multiprocess pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
