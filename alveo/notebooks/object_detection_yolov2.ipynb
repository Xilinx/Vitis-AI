{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with YOLO-v2 (Darknet -> Caffe)\n",
    "\n",
    "This tutorial demonstrates the steps required to prepare and deploy a trained Darknet model for FPGA acceleration  \n",
    "We will prepare a trained YOLO v2 model, and then run a single detection.  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "You only look once (YOLO) is a state-of-the-art, real-time object detection algorithm.  \n",
    "The algorithm was published by Redmon et al. in 2016 via the following publications:\n",
    "[YOLOv1](https://arxiv.org/abs/1506.02640),\n",
    "[YOLOv2](https://arxiv.org/abs/1612.08242).  \n",
    "The same author has already released YOLO-v3, and some experimental tiny YOLO networks. We focus on YOLOv2.  \n",
    "This application requires more than just simple classification. The task here is to detect the presence of objects, and localize them within a frame.  \n",
    "Please refer to the papers for full algorithm details, and/or watch [this.](https://www.youtube.com/watch?v=9s_FpMpdYW8) \n",
    "In this tutorial, the network was trained on the 80 class [COCO dataset.](http://cocodataset.org/#home)\n",
    "\n",
    "## Background\n",
    "\n",
    "The authors of the YOLO papers used their own programming framework called \"Darknet\" for research, and development. \n",
    "The framework is written in C, and was [open sourced.](https://github.com/pjreddie/darknet)\n",
    "Additionally, they host documentation, and pretrained weights [here.](https://pjreddie.com/darknet/yolov2/)\n",
    "\n",
    "Currently, the Darknet framework is not supported by Xilinx Vitis-AI.\n",
    "Additionally, there are some aspects of the YOLOv2 network that are not supported by the Hardware Accelerator, such as the reorg layer. For these reasons the network was modified, retrained, and converted to caffe. In this tutorial we will run the network accelerated on an FPGA using INT8 quantized weights. All convolutions/pools are accelerated on the FPGA fabric, while the final sigmoid, softmax, and non-max suppression functions are executed on the CPU.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation (Offline Process, Performed Once):\n",
    "\n",
    "\n",
    "## Setup (Before Running Notebook)\n",
    "\n",
    "```sh\n",
    "source $VAI_ALVEO_ROOT/overlaybins/setup.sh\n",
    "```\n",
    "\n",
    "## Convert to Caffe\n",
    "\n",
    "- Xilinx provides a darknet2caffe.py python script  \n",
    "- The script will take as arguments a darknet `.cfg` file, and a darknet `.weights` file, then generate a `.prototxt` and a `.caffemodel`.  \n",
    "- This is necessary for integration into the downstream components of Vitis-AI.  \n",
    "                                                                          \n",
    "## Quantize The Model\n",
    "\n",
    "- The Quantizer will generate a `quantize_info.txt` file holding parameters for quantizing floats to INT8.\n",
    "- This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve faster inference\n",
    "- While floating point precision is useful in the model training scenario, it is not required for high speed, high accuracy inference\n",
    "          \n",
    "## Compile The Model\n",
    "\n",
    "- A Network Graph (`prototxt`) and a Weights Blob (`caffemodel`) are compiled along with the `quantize_info` that is generated in the prevoius step.\n",
    "- The network is optimized.\n",
    "- FPGA Instructions are generated.\n",
    "- These instructions are required to run the network in \"one-shot\", and minimize data movement.\n",
    "- This step also generates a `quantizer.json` which has channel-wise parameters to carry out fixed point operations accross channels and layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get VAI Environment and Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables (\"source overlaybins/setup.sh\")\n",
    "import os\n",
    "os.environ[\"DECENT_DEBUG\"] = '1'\n",
    "VAI_ALVEO_ROOT = os.getenv(\"VAI_ALVEO_ROOT\",\"../\")\n",
    "VAI_LIB = os.getenv(\"LIBXDNN_PATH\")\n",
    "print(\"Running w/ VAI_ALVEO_ROOT: %s\" % VAI_ALVEO_ROOT)\n",
    "print(\"Running w/ VAI_LIB: %s\" % VAI_LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, cv2, timeit\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from vai.dpuv1.rt import xdnn, xdnn_io\n",
    "from vai.dpuv1.rt.vitis.python.dpu.runner import Runner\n",
    "from vai.dpuv1.utils.postproc import yolo\n",
    "\n",
    "sys.path.append(os.path.join(VAI_ALVEO_ROOT, \"apps/yolo\"))\n",
    "from get_decent_q_prototxt import get_train_prototxt_deephi\n",
    "from yolo_utils import bias_selector, saveDetectionDarknetStyle, yolo_parser_args\n",
    "from yolo_utils import draw_boxes, generate_colors\n",
    "from get_mAP_darknet import calc_detector_mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an image from disk.\n",
    "\n",
    "Let's load an image (Image courtesy of openimages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(VAI_ALVEO_ROOT+\"/apps/yolo/test_image_set/5904386289_924b24d75d_z.jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.rcParams['figure.figsize'] = [24.0,16.0]\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load yolo_v2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "# Required files\n",
    "config[\"prototxt\"] = VAI_ALVEO_ROOT+\"/models/caffe/yolov2/fp32/yolo_v2_prelu_608.prototxt\" \n",
    "config[\"caffemodel\"] = VAI_ALVEO_ROOT+\"/models/caffe/yolov2/fp32/yolo_v2_prelu.caffemodel\" \n",
    "config[\"labels\"] = VAI_ALVEO_ROOT+\"/apps/yolo/coco.names\"\n",
    "config[\"images\"] = [VAI_ALVEO_ROOT+\"/apps/yolo/test_image_set/5904386289_924b24d75d_z.jpg\"]\n",
    "\n",
    "# YOLO Configs\n",
    "config[\"yolo_model\"] = \"yolo_v2_prelu\"\n",
    "config[\"yolo_version\"] = \"v2\"\n",
    "config['net_h'] = 608\n",
    "config['net_w'] = 608\n",
    "config['scorethresh'] = 0.7\n",
    "config['iouthresh'] =  0.4\n",
    "config['anchorCnt'] = 5\n",
    "config['classes'] = 80\n",
    "\n",
    "# VAI configs\n",
    "config['batch_sz']  = 1\n",
    "config['vitis_rundir'] = 'work'\n",
    "config['architecture'] = os.path.join(VAI_ALVEO_ROOT, 'apps/yolo/arch.json')\n",
    "config['xlnxlib'] = VAI_LIB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p vitis_rundir\n",
    "\n",
    "## Download Model From Xilinx.com\n",
    "prototxt = \"$VAI_ALVEO_ROOT/models/caffe/yolov2/fp32/yolo_v2_prelu_608.prototxt\"\n",
    "!if [ ! -f $prototxt ]; then cd $VAI_ALVEO_ROOT && \\\n",
    "wget https://www.xilinx.com/bin/public/openDownload?filename=models.caffe.yolov2_2019-08-01.zip -O temp.zip && \\\n",
    "unzip -o temp.zip && cd -; fi;\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the model\n",
    "\n",
    "Here, we will quantize the model. The inputs are model train_val prototxt, model weights, number of test iterations and calibration iterations. The output is quantized prototxt, weights, and quantize_info.txt and will be generated in the quantize_results/ directory.\n",
    "\n",
    "The Quantizer will generate a json file holding scaling parameters for quantizing floats to INT8 This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve accelerated inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to generate a train val prototxt from depoly prototxt\n",
    "get_train_prototxt_deephi(\".\", config[\"prototxt\"], \n",
    "                          config[\"vitis_rundir\"] +\"/\"+ \"train_val.prototxt\",\n",
    "                          VAI_ALVEO_ROOT+\"/apps/yolo/images.txt\", \n",
    "                          VAI_ALVEO_ROOT+\"/apps/yolo/test_image_set/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a config dictionary to pass parameters to the compiler\n",
    "# Quantizer Arguments\n",
    "#config[\"outmodel\"] = Defined in Step 4 # String for naming intermediate prototxt, caffemodel\n",
    "def Quantize(prototxt,caffemodel,calib_iter=1):\n",
    "    \n",
    "    quantizer = xfdnnQuantizer(\n",
    "        model=prototxt,\n",
    "        weights=caffemodel,\n",
    "        calib_iter=calib_iter,\n",
    "    )\n",
    "    \n",
    "    quantizer.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Quantizer\n",
    "!vai_q_caffe quantize \\\n",
    "    --model work/train_val.prototxt \\\n",
    "    --weights {config[\"caffemodel\"]} \\\n",
    "    --output_dir work/ \\\n",
    "    --calib_iter 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an VAI Compiler instance and pass it arguments.  \n",
    "The compiler takes in the quantizer outputs from the previous step (prototxt, weights, quantize_info) and outputs a compiler.json and quantizer.json.\n",
    "\n",
    "* A Network Graph (prototxt) and a Weights Blob (caffemodel) are compiled\n",
    "* The network is optimized\n",
    "* FPGA Instructions are generated\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler_flags = \"{ \\\n",
    "    'ddr':1024, \\\n",
    "    'quant_cfgfile':'work/quantize_info.txt', \\\n",
    "    'mixmemorystrategy':True, \\\n",
    "    'poolingaround':True, \\\n",
    "    'parallism':True, \\\n",
    "    'parallelread':['bottom','tops'], \\\n",
    "    'parallelismstrategy':['tops','bottom'], \\\n",
    "    'pipelineconvmaxpool':True, \\\n",
    "    'fancyreplication':True }\"\n",
    "\n",
    "!vai_c_caffe \\\n",
    "    --prototxt work/deploy.prototxt \\\n",
    "    --caffemodel work/deploy.caffemodel \\\n",
    "    --arch {config['architecture']} \\\n",
    "    --output_dir work \\\n",
    "    --net_name \"compiler\" \\\n",
    "    --options \"{compiler_flags}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['netcfg'] = 'work/compiler.json'\n",
    "config['quantizecfg'] = 'work/quantizer.json'\n",
    "config['weights'] = 'work/weights.h5'\n",
    "config['xclbin'] = '/opt/xilinx/overlaybins/xdnnv3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment (Online Process, Typically Performed Iteratively):  \n",
    "    \n",
    "\n",
    "Next, we'll utilize the Vitis-AI APIs to deploy our network to the FPGA. We will walk through the deployment APIs, step by step:\n",
    "\n",
    "1. Open a handle for FPGA communication\n",
    "2. Load weights, biases, and quantization parameters to the FPGA DDR\n",
    "3. Allocate storage for FPGA inputs (such as images to process)\n",
    "4. Allocate storage for FPGA outputs (the activation of the final layer run on the FPGA)\n",
    "5. Execute the network\n",
    "6. Run the postprocessing on CPU\n",
    "7. Print the result \n",
    "\n",
    "First, we will create the handle to communicate with the FPGA and choose which FPGA overlay to run the inference on. \n",
    "        \n",
    "### Open a handle for FPGA communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(config['vitis_rundir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate space in host memory for inputs, load images from disk, and prepare images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTensors = runner.get_input_tensors()\n",
    "outTensors = runner.get_output_tensors()\n",
    "batch_sz = config['batch_sz']\n",
    "if batch_sz == -1:\n",
    "    batch_sz = inTensors[0].dims[0]\n",
    "\n",
    "fpgaBlobs = []\n",
    "for io in [inTensors, outTensors]:\n",
    "    blobs = []\n",
    "    for t in io:\n",
    "        shape = (batch_sz,) + tuple([t.dims[i] for i in range(t.ndims)][1:])\n",
    "        blobs.append(np.empty((shape), dtype=np.float32, order='C'))\n",
    "    fpgaBlobs.append(blobs)\n",
    "fpgaInput = fpgaBlobs[0][0]\n",
    "\n",
    "# Load the image to the buffers\n",
    "fpgaInput[0,...], img_shape = xdnn_io.loadYoloImageBlobFromFile(img,  config['net_h'], config['net_w'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference in FPGA\n",
    "jid = runner.execute_async(fpgaBlobs[0], fpgaBlobs[1])\n",
    "runner.wait(jid)\n",
    "\n",
    "# Run the postprocessing on CPU\n",
    "boxes = yolo.yolov2_postproc(fpgaBlobs[1], config, [img_shape], biases=yolo.yolov2_bias_coco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the boxes on image.\n",
    "Now we must print the results, and we can draw the detections on the original image for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of class labels given a file containing the coco dataset classes\n",
    "with open(VAI_ALVEO_ROOT+\"/apps/yolo/coco.names\") as f:      \n",
    "    namez = f.readlines()      \n",
    "    names = [x.strip() for x in namez]\n",
    "\n",
    "# Given the detection results above, lets draw our findings on the original image, and display it\n",
    "bboxes = boxes[0]\n",
    "result_image = \"work/result.jpg\"\n",
    "colors = generate_colors(config[\"classes\"])\n",
    "draw_boxes(config[\"images\"][0], bboxes, names, colors, result_image, VAI_ALVEO_ROOT+\"/apps/yolo/font\", False)\n",
    "img = cv2.imread(result_image)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.rcParams['figure.figsize'] = [24.0,16.0]\n",
    "plt.imshow(img)\n",
    "plt.title(\"Output Image w/ Bounding Boxes Drawn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the detections our model made\n",
    "for j in range(len(bboxes)):\n",
    "    print(\"Obj %d: %s\\ class id = %d\" % (j, names[bboxes[j]['classid']], bboxes[j]['classid']))\n",
    "    print(\"\\t score = %f\" % (bboxes[j]['prob']))\n",
    "    print(\"\\t (xlo, ylo) = (%d, %d)\" % (bboxes[j]['ll']['x'], bboxes[j]['ll']['y']))\n",
    "    print(\"\\t (xhi, yhi) = (%d, %d)\" % (bboxes[j]['ur']['x'], bboxes[j]['ur']['y']))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
