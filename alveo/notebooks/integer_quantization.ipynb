{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevalence of Floating Point\n",
    "Today's algorithms for training deep convolutional neural networks rely heavily on floating point arithmetic.  \n",
    "  \n",
    "The goal of training is to minimize a multi-variate cost function, which in turn allows the network to make useful predecitions.  \n",
    "Minimizing this cost function requires making minute updates to network parameters (weights).   \n",
    "Below is a visualization of training w/ different update methods:\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn3/opt2.gif\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Floating point data types provide the fine precision that is key for training, however floating point arithmetic is known to be slower and more complex than fixed point arithmetic. When deploying trained networks for inference work loads, it is highly advantageous to employ fixed point arithmetic for the purpose of acceleration, model compression, and power reduction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP32 vs INT8\n",
    "Floating point representation is similar to the scientific notation representation we learn growning up.  \n",
    "Scientific notation is used to represent quantities with huge dynamic range:  \n",
    "$${\\text{mass of the sun}} = 1.99 × 10^{30} kg$$\n",
    "$${\\text{mass of a hydrogen atom}} = 1.67 × 10^{-27} kg$$  \n",
    "\n",
    "Single precision floating point (FP32) represents real numbers using 32 bits.  \n",
    "\n",
    "Those bits are subdivided into a sign bit s, 8b exponent e, and a 23b significand:  \n",
    "\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Float_example.svg/590px-Float_example.svg.png \"fp32\")\n",
    "\n",
    "$${\\displaystyle {\\text{value}}=(-1)^{\\text{s}}\\times 2^{(e-127)}\\times \\left(1+\\sum _{i=1}^{23}b_{23-i}2^{-i}\\right).}$$\n",
    "\n",
    "\n",
    "In comparison, Fixed Point representations are far simpler, and integer types even more so.  \n",
    "\n",
    "8-bit integers (INT8) represent real numbers using 8 bits.  \n",
    "\n",
    "The msb has a special significance as it has a negative weight:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/lean-tra/Swift-Korean/master/images/bitshiftsignedminusfour_2x.png \"int8\")\n",
    "\n",
    "$${\\displaystyle {\\text{value}}=-b_{7}2^{7}+\\sum _{i=0}^{6}b_{i}2^{i}.}$$\n",
    "\n",
    "The table below summarizes the important differences, but the theme is that floating point representations require 4x the storage space, require dramatically more hardware resources (Flip-Flops, Logic Gates), require more power, and as a growing body of research shows - are unnecessary for the task of inference.  \n",
    "  \n",
    "| Representation | Size (bits) | Max Value | Min Value | Smallest Positive Value |\n",
    "|----------------|-------------|-----------|-----------|-------------------------|\n",
    "|FP32|32|$$3.402823 × 10^{38}$$|$$-3.402823 × 10^{38}$$|$$1.175494 × 10^{-38}$$|\n",
    "|INT8|8|$$127$$|$$-128$$|$$1$$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate an FP32 vector with an INT8 vector using Uniform Symmetric Quantization\n",
    "Given a vector X of floating point values, whose range of elements is well bounded and evenly distributed, the vector can be well represented by integers using uniform symmetric quantization.  \n",
    "  \n",
    "Using INT8, we can easily represent the symmetric range {-127:127}\n",
    "\n",
    "If we can determine a symmetric floating point range {-threshold:+threshold} that bounds the elements of our vector, we can create an affine mapping between the floating point range, and the INT8 range using a scaling factor \"sf\". \n",
    "  \n",
    "Where:    \n",
    "\n",
    "$$\\mathrm{{sf}_x} = \\frac {127}{\\mathrm{threshold}}\\tag{1}\\label{1}$$  \n",
    "  \n",
    "$$\\mathrm{X_{INT8}} \\approx \\mathrm{{sf}_x}\\,\\!\\cdot\\!\\mathrm{{X}_{FP32}}\\tag{2}\\label{2}$$  \n",
    "  \n",
    "Note, that in practice the below equation is actually applied\n",
    "1. The FP32 vector needs to be clipped before scaling if the threshold is smaller than the range of the vector.  \n",
    "2. After scaling the result is rounded to values that can be represented as INT8 (Whole number integers) \n",
    "  \n",
    "$$\\mathrm{X_{INT8}} = round(\\,\\mathrm{{sf}_x}\\,\\!\\cdot\\!\\,clip(\\,\\mathrm{{X}_{FP32}},-threshold,threshold\\,)\\,)\\tag{3}\\label{3}$$  \n",
    "  \n",
    "Dequantizing the input is done by reversing the scale operation\n",
    "$$\\mathrm{X_{DEQUANTIZED}} = \\frac{ \\mathrm{{X}_{INT8}} } { \\mathrm{{sf}_x} } \\tag{4}\\label{4}$$   \n",
    "  \n",
    "In the next few cells, we will show you:\n",
    "1. Quantize an FP32 vector to INT8\n",
    "2. Compare the two vectors\n",
    "3. Dequantize the INT8 vector back to FP32\n",
    "4. Calculate the Error in Quantizing, and Dequantizing.\n",
    "5. Optionally, you can adjust the chosen threshold to see it's impact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to hold data, and meta-data for a vector X\n",
    "X = {}\n",
    "\n",
    "# Define random vector A[\"fp32\"] of 100 floating point numbers\n",
    "X[\"mu\"] = 0     # Mean\n",
    "X[\"sigma\"] = 1  # Standard deviation\n",
    "X[\"fp32\"] = X[\"mu\"] + X[\"sigma\"]*np.random.randn(100).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets choose the maximum and minimum floating point value to represent\n",
    "# i.e. we are selecting the range {-threshold:+threshold}\n",
    "# By default we will set the threshold to the maximum absolute value of the vector\n",
    "# However, if we believe the vector has outliers, we could select a smaller threshold\n",
    "mx = np.max(np.abs(X[\"fp32\"]))\n",
    "@widgets.interact(threshold=(0, mx,0.01))\n",
    "def f(threshold=mx):\n",
    "    print (\"Threshold set to: \",threshold)\n",
    "    X[\"threshold\"] = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using INT8, the maximum positive value is 127\n",
    "# So the positive floating point values can be \"int8\" to or \"mapped\" to\n",
    "# one of 128 possible quantization levels (including zero which is also valid)\n",
    "# To scale from the FP32 range to the INT8 range, we need a scaling factor\n",
    "# The scaling factor is simiply the ratio of INT8_MAX to the FP32 range we've chosen to represent\n",
    "# Apply equation (1)\n",
    "X[\"sf\"] = 127/X[\"threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply equation (2)\n",
    "# First clip the vector, so outlier values will be reduced to the threshold\n",
    "# Scale input to new range\n",
    "# Next use a rounding function to convert values to the whole numbers which INT8 can represent\n",
    "X[\"int8\"] = np.round(X[\"sf\"]*np.clip(X[\"fp32\"],-1*X[\"threshold\"],X[\"threshold\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dequantization is acheived by reversing the scale operation that was previously done\n",
    "# Error is incurred, because we cannot undo the round, or the clip\n",
    "X[\"dequantized\"] = X[\"int8\"]/X[\"sf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the L2 Distance between the original vector, and our recovered vector\n",
    "l2dist = np.sqrt(np.sum(np.square(X[\"fp32\"] - X[\"dequantized\"])))\n",
    "\n",
    "# Calculate the percent error of every element\n",
    "X[\"perror\"] = 100*np.abs((X[\"dequantized\"] - X[\"fp32\"])/(X[\"fp32\"]))\n",
    "\n",
    "print (\"The L2 Distance between the FP32 vector, and its Quantized->Dequantized Version is: \",l2dist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PLOT EVERYTHING ###\n",
    "from util import plot_all\n",
    "plot_all(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate FP32 arithmetic with INT8 arithmetic\n",
    "## Vector Dot Product Example\n",
    "Consider the dot product operation + bias of:\n",
    "$$Y_{FP32} = \\vec{W}_{FP32}\\,\\!\\cdot\\!\\,\\vec{X}_{FP32} + B_{FP32}\\tag{5}\\label{5}$$\n",
    "  \n",
    "If we could determine scaling factors sfW and sfX for both W and X where:  \n",
    "$$\\vec{W}_{INT8} \\approx \\mathrm{{sf}_w}\\,\\!\\cdot\\!\\vec{W}_{FP32}$$\n",
    "$$\\vec{X}_{INT8} \\approx \\mathrm{{sf}_x}\\,\\!\\cdot\\!\\vec{X}_{FP32}$$\n",
    "  \n",
    "Then by the multiplication property of equality we would have:  \n",
    "$${Y}_{INT} \\approx {sf}_{w}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!Y_{FP32} \\approx {sf}_{w}\\!\\cdot\\!\\vec{W}_{FP32}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!\\vec{X}_{FP32} + {sf}_{w}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!B_{FP32} \\approx \\vec{W}_{INT8}\\!\\cdot\\!\\vec{X}_{INT8} + {sf}_{w}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!B_{FP32}\\tag{6}\\label{6}$$  \n",
    "  \n",
    "The dot product needs to be accumulated into a signed integer accumulator wide enough to prevent overflow.  \n",
    "  \n",
    "This means if we scale W,X, and B, we can calculate the scaled version of y. Recovering the original value of y, requires reversing the scale, by dividing by the two scale factors:   \n",
    "  \n",
    "$${Y}_{DEQUANTIZED} = \\frac{{Y}_{INT}}{{sf}_{w}\\!\\cdot\\!{sf}_{x}}\\tag{7}\\label{7}$$\n",
    "  \n",
    "In the next two cells, we will show you:\n",
    "1. Define an FP32 vector of weights\n",
    "2. Define an FP32 bias\n",
    "3. Compute the dot product Y in floating point representation\n",
    "4. Quantize an FP32 vector of weights to INT8\n",
    "5. Quantize the Bias\n",
    "6. Compute the dot product Y in integer representation\n",
    "7. Dequantize the INT dot product vector back to FP32\n",
    "8. Calculate the Error between the FP32 calculation and the INT calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionaries to hold data, and meta-data\n",
    "W = {}\n",
    "B = {}\n",
    "Y = {}\n",
    "\n",
    "# Define random array W of 100 floating point weights\n",
    "W[\"mu\"] = 0\n",
    "W[\"sigma\"] = 0.3\n",
    "W[\"fp32\"] = W[\"mu\"] + W[\"sigma\"]*np.random.randn(100).astype(np.float32)\n",
    "\n",
    "# Vector X was defined earlier\n",
    "\n",
    "# Define random value B as a floating point bias\n",
    "B[\"fp32\"] = np.random.randn(1).astype(np.float32)\n",
    "\n",
    "# Calculate Y by taking the dot product of W and X, and adding the bias B\n",
    "# Apply equation (4)\n",
    "Y[\"fp32\"] = np.dot(W[\"fp32\"],X[\"fp32\"]) + B[\"fp32\"]\n",
    "Y[\"threshold\"] = np.max(np.abs(Y[\"fp32\"]))\n",
    "\n",
    "# Y[\"fp32\"] will be the reference that we are trying to approximate with integer arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize vector W to INT8\n",
    "W[\"threshold\"] = np.max(np.abs(W[\"fp32\"]))\n",
    "W[\"sf\"] = 127/W[\"threshold\"]\n",
    "W[\"int8\"] = np.round(W[\"sf\"]*np.clip(W[\"fp32\"],-1*W[\"threshold\"],W[\"threshold\"]))\n",
    "\n",
    "# Vector X is already quantized from earlier\n",
    "\n",
    "# Quantize the bias B\n",
    "B[\"int\"] = np.round(W[\"sf\"]*X[\"sf\"]*B[\"fp32\"])\n",
    "\n",
    "# Calculate Y[\"int\"]\n",
    "Y[\"int\"] = np.dot(W[\"int8\"],X[\"int8\"]) + B[\"int\"]\n",
    "\n",
    "# Dequantize Y (Approximation of Y)\n",
    "Y[\"dequantized\"] = Y[\"int\"]/(W[\"sf\"]*X[\"sf\"])\n",
    "\n",
    "# Calculate the percent error of every element\n",
    "Y[\"perror\"] = 100*np.abs((Y[\"dequantized\"] - Y[\"fp32\"])/(Y[\"fp32\"]))\n",
    "\n",
    "print(\" Y[fp32] = %f \\n Y[int] = %d ; W[sf] = %f ; X[sf] = %f \\n Y[dequantized] = %f \\n percent_error = %f\" \\\n",
    "            %(Y[\"fp32\"][0],Y[\"int\"][0],W[\"sf\"],X[\"sf\"],Y[\"dequantized\"][0],Y[\"perror\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate FP32 arithmetic with INT8 arithmetic\n",
    "## Matrix Product Example\n",
    "Consider the Matrix Product operation + bias of:\n",
    "$$\n",
    "   Y_{FP32} =  \\left[\\begin{matrix}\n",
    "    w_{00} & w_{01} & w_{0j} \\\\\n",
    "    w_{10} & w_{11} & w_{1j} \\\\\n",
    "    w_{20} & w_{21} & w_{2j} \\\\\n",
    "    w_{i0} & w_{i1} & w_{ij} \\\\\n",
    "    \\end{matrix}\\right]\\,\\!\\cdot\\!\\,\n",
    "    \\left[\\begin{matrix}\n",
    "    x_{00} \\\\\n",
    "    x_{10} \\\\\n",
    "    x_{j0} \\\\\n",
    "    \\end{matrix}\\right]\\, + \\,\n",
    "    \\left[\\begin{matrix}\n",
    "    b_{00} \\\\\n",
    "    b_{10} \\\\\n",
    "    b_{20} \\\\\n",
    "    b_{i0} \\\\\n",
    "    \\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "$$Y_{FP32} = \\mathrm{W_{FP32}}\\!\\cdot\\!\\mathrm{X_{FP32}} + B_{FP32}\\tag{8}\\label{8}$$\n",
    "  \n",
    "If we could determine scaling factors sfW and sfX for both W and X where:  \n",
    "$${W}_{INT8} \\approx \\mathrm{{sf}_w}\\,\\!\\cdot\\!{W}_{FP32}$$  \n",
    "$${X}_{INT8} \\approx \\mathrm{{sf}_x}\\,\\!\\cdot\\!{X}_{FP32}$$  \n",
    "  \n",
    "Then by the multiplication property of equality we would have:  \n",
    "$${Y}_{INT} \\approx {sf}_{w}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!Y_{FP32} \\approx {sf}_{w}\\!\\cdot\\!{W}_{FP32}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!{X}_{FP32} + {sf}_{w}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!B_{FP32} \\approx {W}_{INT8}\\!\\cdot\\!{X}_{INT8} + {sf}_{w}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!B_{FP32}\\tag{9}\\label{9}$$  \n",
    "\n",
    "  \n",
    "The matrix product needs to be accumulated into a signed integer accumulators wide enough to prevent overflow.  \n",
    "  \n",
    "This means if we scale W,X, and B, we can calculate the scaled version of y. Recovering the original value of y, requires reversing the scale, by dividing by the two scale factors:   \n",
    "\n",
    "$${Y}_{DEQUANTIZED} = \\frac{{Y}_{INT}}{{sf}_{w}\\!\\cdot\\!{sf}_{x}}\\tag{10}\\label{10}$$\n",
    "  \n",
    "In the next two cells, we will show you:\n",
    "1. Define an FP32 matrix of weights\n",
    "2. Define an FP32 matrix of bias (Ok, its a vector, but also a matrix with one row)\n",
    "3. Compute the matrix product w/ bias in floating point representation (For reference)\n",
    "4. Quantize an FP32 matrix of weights to INT8\n",
    "5. Quantize the Bias matrix\n",
    "6. Compute the matrix product Y in integer representation\n",
    "7. Dequantize the INT matrix product back to FP32\n",
    "8. Calculate the Error between the FP32 calculation and the INT calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionaries to hold data, and meta-data\n",
    "W = {}\n",
    "B = {}\n",
    "Y = {}\n",
    "\n",
    "# Define random matrix W of 10,000 floating point weights shaped (100x100)\n",
    "W[\"mu\"] = 0\n",
    "W[\"sigma\"] = 0.3\n",
    "W[\"fp32\"] = W[\"mu\"] + W[\"sigma\"]*np.random.randn(100*100).astype(np.float32).reshape(100,100)\n",
    "\n",
    "# Vector X was defined earlier\n",
    "\n",
    "# Define random matrix B as a floating point bias\n",
    "B[\"fp32\"] = np.random.randn(100).astype(np.float32).reshape(1,100)\n",
    "\n",
    "# Calculate Y by taking the dot product of W and X, and adding the bias B\n",
    "# Apply equation (4)\n",
    "Y[\"fp32\"] = np.dot(W[\"fp32\"],X[\"fp32\"]) + B[\"fp32\"]\n",
    "Y[\"threshold\"] = np.max(np.abs(Y[\"fp32\"]))\n",
    "Y[\"sf\"] = 127/Y[\"threshold\"]\n",
    "Y[\"int8\"] = np.round(Y[\"sf\"]*np.clip(Y[\"fp32\"],-1*Y[\"threshold\"],Y[\"threshold\"]))\n",
    "\n",
    "# Y[\"fp32\"] will be the reference that we are trying to approximate with integer arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize matrix W to INT8\n",
    "W[\"threshold\"] = np.max(np.abs(W[\"fp32\"]))\n",
    "W[\"sf\"] = 127/W[\"threshold\"]\n",
    "W[\"int8\"] = np.round(W[\"sf\"]*np.clip(W[\"fp32\"],-1*W[\"threshold\"],W[\"threshold\"]))\n",
    "\n",
    "# Vector X is already quantized from earlier\n",
    "\n",
    "# Quantize the bias B\n",
    "B[\"int\"] = np.round(W[\"sf\"]*X[\"sf\"]*B[\"fp32\"])\n",
    "\n",
    "# Calculate Y[\"int\"]\n",
    "Y[\"int\"] = np.dot(W[\"int8\"],X[\"int8\"]) + B[\"int\"]\n",
    "\n",
    "# Dequantize Y (Approximation of Y)\n",
    "Y[\"dequantized\"] = Y[\"int\"]/(W[\"sf\"]*X[\"sf\"])\n",
    "\n",
    "# Calculate the L2 Distance between the original vector, and our recovered vector\n",
    "l2dist = np.sqrt(np.sum(np.square(Y[\"fp32\"] - Y[\"dequantized\"])))\n",
    "\n",
    "# Calculate the percent error of every element\n",
    "Y[\"perror\"] = 100*np.abs((Y[\"dequantized\"] - Y[\"fp32\"])/(Y[\"fp32\"]))\n",
    "\n",
    "print (\"The L2 Distance between the FP32 calculation, and its INT->Dequantized Version is: \",l2dist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import plot_all2\n",
    "plot_all2(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate FP32 arithmetic with INT8 arithmetic\n",
    "## Cascaded Matrix Products Example\n",
    "Consider the Matrix Products operations + bias of:\n",
    "\n",
    "$$Z_{FP32} = \\mathrm{W0_{FP32}}\\!\\cdot\\!\\mathrm{X_{FP32}} + \\mathrm{B0_{FP32}}\\tag{11}\\label{11}$$\n",
    "  \n",
    "$$A_{FP32} = ReLU(Z_{FP32})$$\n",
    "  \n",
    "$$Y_{FP32} = \\mathrm{W1_{FP32}}\\!\\cdot\\!\\mathrm{A_{FP32}} +  \\mathrm{B1_{FP32}}\\tag{12}\\label{12}$$\n",
    "  \n",
    "If we could determine the following scaling factors where:  \n",
    "$${W0}_{INT8} \\approx \\mathrm{{sf}_{w0}}\\,\\!\\cdot\\!{W0}_{FP32}$$\n",
    "$${X}_{INT8} \\approx \\mathrm{{sf}_x}\\,\\!\\cdot\\!{X}_{FP32}$$\n",
    "$${W1}_{INT8} \\approx \\mathrm{{sf}_{w1}}\\,\\!\\cdot\\!{W1}_{FP32}$$\n",
    "$${A}_{INT8} \\approx \\mathrm{{sf}_a}\\,\\!\\cdot\\!{A}_{FP32}$$\n",
    "  \n",
    "Then by the multiplication property of equality we would have:  \n",
    "  \n",
    "$${Z}_{INT} \\approx {sf}_{w0}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!Z_{FP32} \\approx {sf}_{w0}\\!\\cdot\\!\\mathrm{{W0}_{FP32}}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!\\mathrm{{X}_{FP32}} + {sf}_{w0}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!\\mathrm{B0_{FP32}} \\approx \\mathrm{{W0}_{INT8}}\\!\\cdot\\!\\mathrm{{X}_{INT8}} + {sf}_{w0}\\!\\cdot\\!{sf}_{x}\\!\\cdot\\!\\mathrm{B0_{FP32}}\\tag{13}\\label{13}$$  \n",
    "\n",
    "$$A_{INT} = ReLU(Z_{INT})$$\n",
    "  \n",
    "Need to adjust the larger integer to int8 range for next layer  \n",
    "Effectively the below scale needs to be acheived  \n",
    "INT -> FP32 -> INT8  \n",
    "However, we can acheive the same scalings without converting back to FP32 in integer hardware\n",
    "$$A_{INT8} = \\frac{{sf}_{a}}{{sf}_{w0}\\!\\cdot\\!{sf}_{x}}\\!\\cdot\\!A_{INT}\\tag{14}\\label{14}$$ \n",
    "   \n",
    "$${Y}_{INT} \\approx {sf}_{w1}\\!\\cdot\\!{sf}_{a}\\!\\cdot\\!Y_{FP32} \\approx \\mathrm{W1_{INT8}}\\!\\cdot\\!A_{INT8} + {sf}_{w1}\\!\\cdot\\!{sf}_{a}\\!\\cdot\\!\\mathrm{B1_{FP32}}\\tag{15}\\label{15}$$ \n",
    "  \n",
    "Recovering the original value of y, requires reversing the scale, by dividing by the two scale factors:   \n",
    "  \n",
    "$${Y}_{DEQUANTIZED} = \\frac{{Y}_{INT}}{{sf}_{w1}\\!\\cdot\\!{sf}_{a}}\\tag{16}\\label{16}$$\n",
    "  \n",
    "In the next two cells, we will show you:\n",
    "1. Define two FP32 matricies of weights\n",
    "2. Define two FP32 matricies of bias (Ok, its a vector, but also a matrix with one row)\n",
    "3. Compute the matrix products w/ bias in floating point representation (For reference)\n",
    "4. Quantize two FP32 matricies of weights to INT8\n",
    "5. Quantize the Bias matricies\n",
    "6. Compute the product Y in integer representation\n",
    "7. Dequantize the INT matrix product back to FP32\n",
    "8. Calculate the Error between the FP32 calculation and the INT calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionaries to hold data, and meta-data\n",
    "W0 = {}\n",
    "W1 = {}\n",
    "B0 = {}\n",
    "B1 = {}\n",
    "Z = {}\n",
    "A = {}\n",
    "Y = {}\n",
    "\n",
    "# Define random matrix W0 of 10,000 floating point weights shaped (100x100)\n",
    "W0[\"mu\"] = 0\n",
    "W0[\"sigma\"] = 0.3\n",
    "W0[\"fp32\"] = W0[\"mu\"] + W0[\"sigma\"]*np.random.randn(100*100).astype(np.float32).reshape(100,100)\n",
    "\n",
    "# Define random matrix W1 of 10,000 floating point weights shaped (100x100)\n",
    "W1[\"mu\"] = 0\n",
    "W1[\"sigma\"] = 0.5\n",
    "W1[\"fp32\"] = W1[\"mu\"] + W1[\"sigma\"]*np.random.randn(100*100).astype(np.float32).reshape(100,100)\n",
    "\n",
    "# Vector X was defined earlier\n",
    "\n",
    "# Define random matrix B0 as a floating point bias\n",
    "B0[\"fp32\"] = np.random.randn(100).astype(np.float32).reshape(1,100)\n",
    "\n",
    "# Define random matrix B1 as a floating point bias\n",
    "B1[\"fp32\"] = np.random.randn(100).astype(np.float32).reshape(1,100)\n",
    "\n",
    "# Calculate Y by taking the dot product of W and X, and adding the bias B\n",
    "# Apply equation (4)\n",
    "Z[\"fp32\"] = np.dot(W0[\"fp32\"],X[\"fp32\"]) + B0[\"fp32\"]\n",
    "A[\"fp32\"] = np.maximum(Z[\"fp32\"],0)\n",
    "Y[\"fp32\"] = np.dot(W1[\"fp32\"],A[\"fp32\"].T) + B1[\"fp32\"]\n",
    "\n",
    "# Y[\"fp32\"] will be the reference that we are trying to approximate with integer arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize matrix W0 to INT8\n",
    "W0[\"threshold\"] = np.max(np.abs(W0[\"fp32\"]))\n",
    "W0[\"sf\"] = 127/W0[\"threshold\"]\n",
    "W0[\"int8\"] = np.round(W0[\"sf\"]*np.clip(W0[\"fp32\"],-1*W0[\"threshold\"],W0[\"threshold\"]))\n",
    "\n",
    "# Quantize matrix W1 to INT8\n",
    "W1[\"threshold\"] = np.max(np.abs(W1[\"fp32\"]))\n",
    "W1[\"sf\"] = 127/W1[\"threshold\"]\n",
    "W1[\"int8\"] = np.round(W1[\"sf\"]*np.clip(W1[\"fp32\"],-1*W1[\"threshold\"],W1[\"threshold\"]))\n",
    "\n",
    "# Vector X is already quantized from earlier\n",
    "\n",
    "# Since A is the input to the next layer, we need to quantize it to 8b\n",
    "# Assumption made here is that the multiplier in hardware can only take 8b operands\n",
    "# Thus we need a scale factor\n",
    "A[\"threshold\"] = np.max(np.abs(A[\"fp32\"]))\n",
    "A[\"sf\"] = 127/A[\"threshold\"]\n",
    "\n",
    "# Quantize the bias B0\n",
    "B0[\"int\"] = np.round(W0[\"sf\"]*X[\"sf\"]*B0[\"fp32\"])\n",
    "\n",
    "# Quantize the bias B1\n",
    "B1[\"int\"] = np.round(W1[\"sf\"]*A[\"sf\"]*B1[\"fp32\"])\n",
    "\n",
    "# Calculate Z[\"int\"] - matrix product + bias\n",
    "Z[\"int\"] = np.dot(W0[\"int8\"],X[\"int8\"]) + B0[\"int\"]\n",
    "\n",
    "# Calculate A[\"int\"] - ReLU\n",
    "A[\"int\"] = np.maximum(Z[\"int\"],0)\n",
    "\n",
    "# Adjust A\n",
    "# A was an integer accumulator, you could assume the width to be 32b; INT32\n",
    "# We need to dequantize it back to floating point (Divide by the scaling factors of the inputs)\n",
    "# We need to quantize it to INT8 for use in the next layer's multiplier (Multiply by scaling factor for next layer)\n",
    "# This creates an adjustment factor\n",
    "A[\"adjustment\"] = A[\"sf\"]/(W0[\"sf\"]*X[\"sf\"])\n",
    "\n",
    "# The challenge posed now is to do this manipulation in the integer domain\n",
    "# There are several options to acheive this, but here we will simply do an integer multiply,\n",
    "# followed by a right shift\n",
    "# These shift and scale parameters are found heuristically offline before the computation is performed, \n",
    "# just like the scale factors\n",
    "from util import findShiftScale\n",
    "shift,scale = findShiftScale(A[\"adjustment\"])\n",
    "A[\"int8\"] = scale*A[\"int\"] # Integer will be multiplied\n",
    "A[\"int8\"] = np.power(2,shift)*A[\"int8\"] # Shift out the unnecessary bits\n",
    "\n",
    "# Calculate Y[\"int\"]\n",
    "Y[\"int\"] = np.dot(W1[\"int8\"],A[\"int8\"].T) + B1[\"int\"]\n",
    "\n",
    "# Dequantize Y (Approximation of Y)\n",
    "Y[\"dequantized\"] = Y[\"int\"]/(W1[\"sf\"]*A[\"sf\"])\n",
    "\n",
    "# Calculate the L2 Distance between the original vector, and our recovered vector\n",
    "l2dist = np.sqrt(np.sum(np.square(Y[\"fp32\"] - Y[\"dequantized\"])))\n",
    "\n",
    "# Calculate the percent error of every element\n",
    "Y[\"perror\"] = 100*np.abs((Y[\"dequantized\"] - Y[\"fp32\"])/(Y[\"fp32\"]))\n",
    "\n",
    "print (\"The L2 Distance between the FP32 calculation, and its INT->Dequantized Version is: \",l2dist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import plot_all2\n",
    "plot_all2(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
