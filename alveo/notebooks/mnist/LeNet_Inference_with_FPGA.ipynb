{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Inference with Cloud Alveo U200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the steps for hand-writing digit recognition using ML with classic LeNet. Alveo U200 is used for neural network acceleration.\n",
    "1. Use Windows Paint tool to open a square canvas, fill the background with black, and draw a digit (0 - 9) with white brush, save as **input.png**\n",
    "2. Upload the **input.png** file to the home directory\n",
    "3. Run the code in this page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example for the **input.png** file:\n",
    "![test](./example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code in this page will finish following jobs:\n",
    "1. **Quantize the model** - The quantizer will generate scaling parameters for quantizing floats INT8. This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve more parallelization at lower power. \n",
    "2. **Compile the Model** - In this step, the network Graph (prototxt) and the Weights (caffemodel) are compiled, the compiler \n",
    "3. **Subgraph Cutting** - In this step, the original graph is cut, and a custom FPGA accelerated python layer is inserted to be used for Inference. \n",
    "4. **Classification** - In this step, the caffe model and the prototxt from the previous step are run on the FPGA to perform inference on an input image.\n",
    "\n",
    "\n",
    "The pre-trained nerual network model includes following files:\n",
    "* **lenet.prototxt**: Caffe PROTOTXT file\n",
    "* **lenet_iter_10000.caffemodel**: Caffe model parameters file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input image and finish pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_IMAGE is the input file name\n",
    "# SMALL_IMAGE is the 28x28 size image generated from the input image\n",
    "INPUT_IMAGE = './input.png'\n",
    "SMALL_IMAGE = './small.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the input image file\n",
    "from IPython.display import Image as NBImage\n",
    "NBImage(INPUT_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprossing: scale the input input image to 28x28 size, and display it\n",
    "from PIL import Image\n",
    "im = Image.open(INPUT_IMAGE)\n",
    "im.thumbnail((28, 28))\n",
    "tt =  im.convert('L')\n",
    "tt.save(SMALL_IMAGE)\n",
    "NBImage(SMALL_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from decent import CaffeFrontend as xfdnnQuantizer\n",
    "import subprocess\n",
    "from xfdnn_subgraph import CaffeCutter as xfdnnCutter\n",
    "\n",
    "# Environment Variables (\"source overlaybins/setup.sh\")\n",
    "import os\n",
    "VAI_ALVEO_ROOT = os.getenv(\"VAI_ALVEO_ROOT\",\"../\")\n",
    "MLSUITE_PLATFORM = os.getenv(\"MLSUITE_PLATFORM\",\"alveo-u200\")\n",
    "print(\"Running with VAI_ALVEO_ROOT: %s\" % VAI_ALVEO_ROOT)\n",
    "print(\"Running with MLSUITE_PLATFORM: %s\" % MLSUITE_PLATFORM)\n",
    "\n",
    "from IPython.display import Image as display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxt = \"./lenet_train_test.prototxt\"\n",
    "caffemodel = \"./lenet_iter_10000.caffemodel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Run the Quantizer\n",
    "\n",
    "Here, we will quantize the model. The inputs are model prototxt, model weights, number of test iterations and calibration iterations. The output is quantized prototxt, weights, and quantize_info.txt and will be generated in the quantize_results/ directory.\n",
    "\n",
    "The Quantizer will generate a json file holding scaling parameters for quantizing floats to INT8\n",
    "This is required, because FPGAs will take advantage of Fixed Point Precision, to achieve accelerated inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantize(prototxt,caffemodel,calib_iter=1):\n",
    "    \n",
    "    quantizer = xfdnnQuantizer(\n",
    "        model=prototxt,\n",
    "        weights=caffemodel,\n",
    "        calib_iter=calib_iter,\n",
    "    )\n",
    "    \n",
    "    quantizer.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantize(prototxt,caffemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Compiler\n",
    "\n",
    "The compiler takes in the quantizer outputs from the previous step (prototxt, weights, quantize_info) and outputs a compiler.json and quantizer.json.\n",
    "\n",
    "* A Network Graph (prototxt) and a Weights Blob (caffemodel) are compiled\n",
    "* The network is optimized\n",
    "* FPGA Instructions are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard compiler arguments - PLEASE DONT TOUCH\n",
    "def Getopts():\n",
    "    return {\n",
    "            \"bytesperpixels\":1,\n",
    "            \"dsp\":96,\n",
    "            \"memory\":9,\n",
    "            \"ddr\":256,\n",
    "            \"cpulayermustgo\":True,\n",
    "            \"mixmemorystrategy\":True,\n",
    "            \"pipelineconvmaxpool\":True,\n",
    "            \"usedeephi\":True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compile(prototxt=\"quantize_results/deploy.prototxt\",\\\n",
    "            caffemodel=\"quantize_results/deploy.caffemodel\",\\\n",
    "            quantize_info=\"quantize_results/quantize_info.txt\"):\n",
    "    \n",
    "    subprocess.call([\"vai_c_caffe\",\n",
    "                    \"--prototxt\", prototxt,\n",
    "                    \"--caffemodel\", caffemodel,\n",
    "                    \"--net_name\", name,\n",
    "                    \"--output_dir\", \"work\",\n",
    "                    \"--arch\", \"/opt/vitis_ai/compiler/arch/DPUCADX8G/ALVEO/arch.json\",\n",
    "                    \"--options\", \"{\\\"quant_cfgfile\\\":\\\"%s\\\", \\\n",
    "                    \\\"pipelineconvmaxpool\\\":False, \\\n",
    "                    }\" %(quantize_info)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Subgraph Cutter\n",
    "\n",
    "The subgraph cutter creates a custom python layer to be accelerated on the FPGA. The inputs are compiler.json, quantizer.json and model weights from the compiler step, as well as the FPGA xclbin. This outputs a cut prototxt file with FPGA references, to be used for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cut(prototxt):\n",
    "    \n",
    "    cutter = xfdnnCutter(\n",
    "        inproto=\"quantize_results/deploy.prototxt\",\n",
    "        trainproto=prototxt,\n",
    "        outproto=\"xfdnn_auto_cut_deploy.prototxt\",\n",
    "        outtrainproto=\"xfdnn_auto_cut_train_val.prototxt\",\n",
    "        cutAfter=\"data\",\n",
    "        xclbin=VAI_ALVEO_ROOT+\"/overlaybins/\"+MLSUITE_PLATFORM+\"/overlay_4.xclbin\",\n",
    "        netcfg=\"work/compiler.json\",\n",
    "        quantizecfg=\"work/quantizer.json\",\n",
    "        weights=\"work/weights.h5\",\n",
    "        profile=True\n",
    "    )\n",
    "    \n",
    "    cutter.cut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cut(prototxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Execute inference \n",
    "\n",
    "The inputs are the FPGA prototxt file, caffemodel weights, a test image, and the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classify(prototxt,caffemodel,image):\n",
    "\n",
    "    import numpy as np\n",
    "    from caffe import Classifier,io\n",
    "    classifier = Classifier(prototxt,caffemodel)\n",
    "    predictions = classifier.predict([io.load_image(image,color=False)]).flatten()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Classify(\"xfdnn_auto_cut_deploy.prototxt\",\"quantize_results/deploy.caffemodel\",SMALL_IMAGE)\n",
    "print( 'predicted class:', result.argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
