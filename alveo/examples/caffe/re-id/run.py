# Copyright 2019 Xilinx Inc.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import print_function
#coding=utf_8
import numpy as np
import cv2
import os
import sys


#from decent import CaffeFrontend as xfdnnQuantizer
import subprocess
from vai.dpuv1.rt.scripts.framework.caffe.xfdnn_subgraph import CaffeCutter as xfdnnCutter

import caffe
from google.protobuf import text_format
from caffe.proto import caffe_pb2
import time
import argparse


def Quantize(prototxt,caffemodel,test_iter=1,calib_iter=15):
    os.environ["DECENT_DEBUG"] = "1"
    subprocess.call(["vai_q_caffe", "quantize",
                 "--model", prototxt,
                 "--weights", caffemodel,
                 "--calib_iter", str(calib_iter)])

# Standard compiler arguments for XDNNv3
def Getopts():
  return {
     "bytesperpixels":1,
     "dsp":96,
     "memory":9,
     "ddr":"256",
     "cpulayermustgo":True,
     "usedeephi":True,
  }

name = "inception_v2_ssd"
# Generate hardware instructions for runtime -> compiler.json
def Compile(prototxt="quantize_results/deploy.prototxt",\
            caffemodel="quantize_results/deploy.caffemodel",\
            quantize_info="quantize_results/quantize_info.txt"):
    
    VAI_ROOT = os.environ['VAI_ALVEO_ROOT']
    arch_json = "/opt/vitis_ai/compiler/arch/DPUCADX8G/ALVEO/arch.json"
    if(not os.path.exists(arch_json)):
        arch_json = os.path.join(VAI_ROOT, "arch.json")
 
    subprocess.call(["vai_c_caffe",
                    "--prototxt", prototxt,
                    "--caffemodel", caffemodel,
                    "--net_name", name,
                    "--output_dir", "work",
                    "--arch", arch_json,
                    "--options", "{\"quant_cfgfile\":\"%s\", \
                    \"pipelineconvmaxpool\":False, \
                    }" %(quantize_info)])

# Generate a new prototxt with custom python layer in place of FPGA subgraph
def Cut(prototxt):
  cutter = xfdnnCutter(
    inproto="quantize_results/deploy.prototxt",
    trainproto=prototxt,
    outproto="xfdnn_auto_cut_deploy.prototxt",
    outtrainproto="xfdnn_auto_cut_train_val.prototxt",
    cutAfter="data",
    xclbin="/opt/xilinx/overlaybins/xdnnv3",
    netcfg="work/compiler.json",
    quantizecfg="work/quantizer.json",
    weights="work/weights.h5",
    #profile=True
  )
  cutter.cut()


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--prototxt', required=True, type=str,
                        help='Provide the xfdnn_auto_cut prototxt generated by subgraph or original deploy.prototxt')
    parser.add_argument('--caffemodel', required=True, type=str,
                        help = 'Provide the caffemodel file')
    parser.add_argument('--prepare', action="store_true", help='In prepare mode, model preperation will be perfomred = Quantize + Compile')
    parser.add_argument('--qtest_iter', type=int, default=1, help='User can provide the number of iterations to test the quantization')
    parser.add_argument('--qcalib_iter', type=int, default=1, help='User can provide the number of iterations to run the quantization')
  

    args = vars(parser.parse_args())
    
    if 	args["prepare"]:
        Quantize(args["prototxt"],args["caffemodel"], args["qtest_iter"], args["qcalib_iter"])
        Compile()
        Cut(args["prototxt"])



       
