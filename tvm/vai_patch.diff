diff --git .gitmodules .gitmodules
index a1367c9..fa6cb55 100644
--- .gitmodules
+++ .gitmodules
@@ -10,3 +10,9 @@
 [submodule "3rdparty/vta-hw"]
 	path = 3rdparty/vta-hw
 	url = https://github.com/apache/incubator-tvm-vta
+[submodule "tutorials/accelerators/device/runtime/lib/cvx"]
+	path = tutorials/accelerators/device/runtime/lib/cvx
+	url = https://github.com/jtuyls/cvx.git
+[submodule "tutorials/accelerators/device/runtime/models"]
+	path = tutorials/accelerators/device/runtime/models
+	url = https://gitenterprise.xilinx.com/jornt/models.git
diff --git docker/Dockerfile.ci_cpu.18.04 docker/Dockerfile.ci_cpu.18.04
new file mode 100644
index 0000000..7374c65
--- /dev/null
+++ docker/Dockerfile.ci_cpu.18.04
@@ -0,0 +1,68 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# CI docker CPU env
+# tag: v0.55
+FROM ubuntu:18.04
+
+RUN apt-get update --fix-missing
+
+COPY install/ubuntu_install_core.sh /install/ubuntu_install_core.sh
+RUN bash /install/ubuntu_install_core.sh
+
+RUN apt-get install -y python3-dev python3-pip
+RUN pip3 install setuptools numpy pytest cython decorator scipy tornado psutil xgboost
+
+
+COPY install/ubuntu_install_llvm.sh /install/ubuntu_install_llvm.sh
+RUN bash /install/ubuntu_install_llvm.sh
+
+# SGX deps (build early; changes infrequently)
+##COPY install/ubuntu_install_sgx.sh /install/ubuntu_install_sgx.sh
+##RUN bash /install/ubuntu_install_sgx.sh
+##ENV LD_LIBRARY_PATH /opt/sgxsdk/lib64:${LD_LIBRARY_PATH}
+
+# Rust env (build early; takes a while)
+COPY install/ubuntu_install_rust.sh /install/ubuntu_install_rust.sh
+RUN bash /install/ubuntu_install_rust.sh
+ENV RUSTUP_HOME /opt/rust
+ENV CARGO_HOME /opt/rust
+
+# AutoTVM deps
+#COPY install/ubuntu_install_redis.sh /install/ubuntu_install_redis.sh
+#RUN bash /install/ubuntu_install_redis.sh
+
+# Golang environment
+COPY install/ubuntu_install_golang.sh /install/ubuntu_install_golang.sh
+RUN bash /install/ubuntu_install_golang.sh
+
+# NNPACK deps
+COPY install/ubuntu_install_nnpack.sh /install/ubuntu_install_nnpack.sh
+RUN bash /install/ubuntu_install_nnpack.sh
+
+ENV PATH $PATH:$CARGO_HOME/bin:/usr/lib/go-1.10/bin
+
+# ANTLR deps
+COPY install/ubuntu_install_java.sh /install/ubuntu_install_java.sh
+RUN bash /install/ubuntu_install_java.sh
+
+COPY install/ubuntu_install_antlr.sh /install/ubuntu_install_antlr.sh
+RUN bash /install/ubuntu_install_antlr.sh
+
+# Chisel deps for TSIM
+COPY install/ubuntu_install_chisel.sh /install/ubuntu_install_chisel.sh
+RUN bash /install/ubuntu_install_chisel.sh
diff --git docker/install/ubuntu_install_python.sh docker/install/ubuntu_install_python.sh
index c1f9d50..58d72f3 100755
--- docker/install/ubuntu_install_python.sh
+++ docker/install/ubuntu_install_python.sh
@@ -27,7 +27,7 @@ apt-get install -y python-dev
 # python 3.6
 apt-get install -y software-properties-common
 
-add-apt-repository ppa:deadsnakes/ppa
+add-apt-repository -y ppa:deadsnakes/ppa
 apt-get update
 apt-get install -y python-pip python-dev python3.6 python3.6-dev
 
diff --git docker/install/ubuntu_install_vai_core.sh docker/install/ubuntu_install_vai_core.sh
new file mode 100644
index 0000000..a046e80
--- /dev/null
+++ docker/install/ubuntu_install_vai_core.sh
@@ -0,0 +1,36 @@
+#!/bin/bash
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+# 
+#   http://www.apache.org/licenses/LICENSE-2.0
+# 
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+set -e
+set -u
+set -o pipefail
+
+# install libraries for building c++ core on ubuntu
+apt-get update && apt-get install -y --no-install-recommends \
+    build-essential\
+    ca-certificates\
+    cmake\
+    sudo\
+    wget\
+    git\
+    vim\
+    graphviz\
+    python-dev\
+    gnupg2
+
+apt-get update && apt-get install -y gcc-aarch64-linux-gnu
diff --git docker/install/ubuntu_install_vai_python_package.sh docker/install/ubuntu_install_vai_python_package.sh
new file mode 100644
index 0000000..07c6e66
--- /dev/null
+++ docker/install/ubuntu_install_vai_python_package.sh
@@ -0,0 +1,41 @@
+#!/bin/bash
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+. $VAI_ROOT/conda/etc/profile.d/conda.sh && \
+    conda activate vitis-ai-tensorflow && \
+    pip install --no-cache-dir antlr4-python3-runtime
+
+# set -e
+# set -u
+# set -o pipefail
+# 
+# install libraries for python package on ubuntu
+# pip install --upgrade pip
+# pip3 install --no-cache-dir \
+#     onnx==1.5.0 \
+#     numpy \
+#     pydot==1.4.1 \
+#     h5py==2.8.0 \
+#     opencv-python \
+#     matplotlib \
+#     jupyter \
+#     psutil \
+#     sklearn \
+#     scipy \
+#     progressbar2 \
+#     dill
diff --git include/tvm/relay/attrs/nn.h include/tvm/relay/attrs/nn.h
index a9c3059..81f8e72 100644
--- include/tvm/relay/attrs/nn.h
+++ include/tvm/relay/attrs/nn.h
@@ -1203,6 +1203,26 @@ struct SubPixelAttrs : public tvm::AttrsNode<SubPixelAttrs> {
   }
 };  // struct SubPixelAttrs
 
+// TOOD: CHANGE NAME OF ATTRS AND JSON_PATH
+ struct ACCELAttrs : public tvm::AttrsNode<ACCELAttrs> {
+   Array <Array<IndexExpr>> output_shape;
+   std::string input_name;
+   std::string output_name;
+   std::string kernel_name;
+   
+   TVM_DECLARE_ATTRS(ACCELAttrs, "relay.attrs.ACCELAttrs") {
+       TVM_ATTR_FIELD(output_shape)
+      .describe("Shape of the output node");
+       TVM_ATTR_FIELD(kernel_name)
+	 .describe("Kernel name");
+       TVM_ATTR_FIELD(input_name)
+	 .describe("Input name");
+       TVM_ATTR_FIELD(output_name)
+	 .describe("Output_name");
+  
+   }
+};
+ 
 }  // namespace relay
 }  // namespace tvm
 #endif  // TVM_RELAY_ATTRS_NN_H_
diff --git python/tvm/contrib/vai/__init__.py python/tvm/contrib/vai/__init__.py
new file mode 100644
index 0000000..cb39280
--- /dev/null
+++ python/tvm/contrib/vai/__init__.py
@@ -0,0 +1,18 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+""" Contrib API for Xilinx Vitis-AI CNN accelerators """
diff --git python/tvm/contrib/vai/base.py python/tvm/contrib/vai/base.py
new file mode 100644
index 0000000..ef8f59d
--- /dev/null
+++ python/tvm/contrib/vai/base.py
@@ -0,0 +1,60 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+"""
+Registration of Vitis-AI Relay 'accel' operations
+"""
+
+import tvm
+from tvm import te
+from tvm import tir
+from tvm.target import generic_func
+from tvm.relay.op import op
+
+@op.register_compute("nn.accel", level=15)
+def compute_nn_accel(attrs, inputs, out_dtype):
+    """Compute definition of accel operation for Relay"""
+    name = 'accel0'
+    
+    if isinstance(out_dtype, tvm.ir.TensorType):
+        oshapes = out_dtype.shape
+    else:
+        oshapes = [tuple(e.shape) for e in list(out_dtype.fields)]
+    
+    def extern_vai_func(ins, outs):
+        tensors = ins + outs
+        return tir.call_packed('tvm.accel.accel_fused', attrs.kernel_name,
+                               attrs.input_name, attrs.output_name,
+                               *tensors)
+
+    out = te.extern(oshapes, inputs, extern_vai_func, name=name)
+
+    if isinstance(out, list):
+        return out
+    else:
+        return [out]
+
+# @op.register_schedule("nn.accel", level=15)
+@generic_func
+def schedule_nn_accel(_, outputs, target):
+    del target
+    return te.create_schedule([x.op for x in outputs])
+
+op.register_schedule('nn.accel', schedule_nn_accel)
+
+
+
diff --git python/tvm/contrib/vai/extern_accel.py python/tvm/contrib/vai/extern_accel.py
new file mode 100644
index 0000000..23bbeee
--- /dev/null
+++ python/tvm/contrib/vai/extern_accel.py
@@ -0,0 +1,130 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+'''
+Registration of Xilinx Vitis-AI fused acceleration operation for aceleration of 
+convolutional neural networks
+
+This Vitis-AI acceleration exploits DPU hardware accelerator built for 
+following evaluation boards:
+- Alveo u200: https://www.xilinx.com/products/boards-and-kits/alveo/u200.html
+- Alveo u250: https://www.xilinx.com/products/boards-and-kits/alveo/u250.html
+- Zynq Ultra96: https://www.xilinx.com/products/boards-and-kits/1-vad4rl.html
+- Zynq ZCU104: https://www.xilinx.com/products/boards-and-kits/zcu104.html
+- Zynq ZCU102: https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html
+
+More information about Xilinx DPU and Vitis-AI can be found here:
+https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html
+'''
+
+import os
+import tvm
+import warnings
+import numpy as np
+import time
+
+from .vai_runner import Runner
+
+RUNNER_CACHE = None
+DPU_RUNDIR = None
+
+def setDpuRunDir(dpu_rundir):
+    global DPU_RUNDIR
+
+    if not os.path.exists(dpu_rundir):
+        raise ValueError("Provided nonexisting run directory: {}".format(dpu_rundir))
+  
+    DPU_RUNDIR = dpu_rundir
+
+class RunnerCache(object):
+
+    """
+    Wrapper around Runner for connection handling and caching
+    """
+    
+    def __init__(self):
+        self.runner_cache = {}
+
+    def createRunner(self, fdir):
+        # type: (str) -> Runner
+        """ 
+        Return the Runner for the given file dir
+        """
+        if fdir in self.runner_cache:
+            return self.runner_cache[fdir]
+
+        # Create Runner
+        runner = Runner(fdir)
+
+        self.runner_cache[fdir] = runner
+
+        return runner
+
+    def __del__(self):
+        """ Cleanup """
+        for fdir in list(self.runner_cache.keys()):
+            del self.runner_cache[fdir]
+
+
+
+RUNNER_CACHE = RunnerCache()
+
+
+@tvm.register_func("tvm.accel.accel_fused")
+def accel_fused(kernel_name, input_name, output_name, *tensors):
+    """
+    Registration of external accel.accel_fused operation
+    Arguments
+    ---------
+    kernel_name: str
+        the name of the DPU kernel
+    input_name: str
+        the input_name of the DPU Kernel
+    output_name: str
+        the output_name of the DPU kernel
+    tensors: List[tvm.nd.NDArray]
+        the operation inputs and outputs as List
+    """
+    input_names = input_name.split('*')
+    output_names = output_name.split('*')
+
+    num_inputs = len(tensors) - len(output_names)
+
+    # Fetch the inputs and outputs
+    ins = tensors[:num_inputs]
+    outs = tensors[num_inputs:]
+
+    runner = RUNNER_CACHE.createRunner(DPU_RUNDIR)
+    
+    in_tensor_names = [str(it.name.decode('utf-8')) for it in runner.get_input_tensors()]
+    out_tensor_names = [str(ot.name.decode('utf-8')) for ot in runner.get_output_tensors()]
+    out_tensor_shapes = [tuple([t.dims[i] for i in range(t.ndims)]) for t in runner.get_output_tensors()]
+
+    in_map = {in_name: i.asnumpy() for in_name, i in zip(input_names, ins)}
+    out_map = {out_name: np.empty(out_shape, dtype=np.float32, order='C')
+               for out_name, out_shape in zip(out_tensor_names, out_tensor_shapes)}
+
+    in_data = [in_map[it_name] for it_name in in_tensor_names]
+    out_data = [out_map[ot_name] for ot_name in out_tensor_names]
+
+
+    jid = runner.execute_async(in_data, out_data)
+    runner.wait(jid)
+
+    for idx, out_name in enumerate(output_names):
+        out_data_idx = out_tensor_names.index(out_name)
+        tvm.nd.array(out_data[out_data_idx]).copyto(outs[idx])
diff --git python/tvm/contrib/vai/relay_transform.py python/tvm/contrib/vai/relay_transform.py
new file mode 100644
index 0000000..30027e5
--- /dev/null
+++ python/tvm/contrib/vai/relay_transform.py
@@ -0,0 +1,107 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+"""
+Vitis-AI Relay pass to partition Relay graph for Xilinx FPGA acceleration
+
+"""
+
+import os
+import json
+import pyxir
+import tvm
+from tvm import relay
+
+from pyxir.frontend.tvm import from_relay
+from pyxir.graph.io.xgraph_io import XGraphIO
+from pyxir.target_registry import TargetRegistry
+from pyxir.contrib.dpuv1 import dpuv1
+from pyxir.contrib.dpuv2 import dpuv2
+import pyxir.contrib.target.DPUCADX8G
+import pyxir.contrib.target.DPUCZDX8G
+from .xgraph_to_relay_transform import XgraphRelayTransform
+
+@tvm.transform.module_pass(opt_level=4)
+class PartitioningPass:
+
+    """
+    The Vitis-AI partitioning pass
+
+    Arguments
+    ---------
+    target: str
+        the target accelerator, only 'dpu' is supported at the moment
+
+    params: dict from str to array
+        the relay model parameters
+
+    inputs_func: function
+        a python function which takes an iterator number and a layout and
+        provides a numpy array of inputs to be used for quantization
+        calibration
+
+    layout: str
+        the layout of the Relay model, only 'NCHW' and 'NHWC' supported
+        at the moment
+
+    """
+
+    def __init__(self, target, params, inputs_func, postprocessing):
+
+        # check if target is supported
+        TargetRegistry().check_target(target)
+        
+      
+
+        self.target         = target
+        self.params         = params
+        self.inputs_func    = inputs_func
+        self.postprocessing = postprocessing
+        
+        self.work_dir = '/tmp/vai'
+        os.makedirs(self.work_dir, exist_ok=True)
+
+    def transform_module(self, mod, _):
+        """
+        Transformation module method which is called from parent __call__
+        """
+
+        xgraph = from_relay(mod, 
+                            params         = self.params, 
+                            postprocessing = self.postprocessing)
+        # print(xgraph.get_layer_names())
+        # last_layer = input("Last partitioning layer: ")        
+        # XGRAPH PARTITIONING
+        xgraph = pyxir.partition(xgraph, targets=[self.target]) #, last_layer=last_layer)
+        xgraph = pyxir.optimize(xgraph, self.target)
+        
+        dpu_xgraph = pyxir.schedule(xgraph, target=self.target)        
+        XGraphIO.save(dpu_xgraph, os.path.join(self.work_dir, 'dpu_xgraph'))
+
+        # QUANTIZE XGRAPH
+        xgraph = pyxir.quantize(xgraph, target=self.target, inputs_func=self.inputs_func, work_dir=self.work_dir)
+
+        # COMPILE XGRAPH
+        xgraph = pyxir.compile(xgraph, target=self.target, work_dir=self.work_dir)
+        
+        # RELAY PARTITIONING
+        xrt = XgraphRelayTransform(self.target)
+        mod = xrt.transform(xgraph, mod, self.work_dir)
+        
+        return mod
+
+ 
diff --git python/tvm/contrib/vai/tvmruntime_util.py python/tvm/contrib/vai/tvmruntime_util.py
new file mode 100644
index 0000000..0b5d308
--- /dev/null
+++ python/tvm/contrib/vai/tvmruntime_util.py
@@ -0,0 +1,56 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+"""
+Small wrapper around TVM runtime which create a runtime module based on
+the TVM runtime files in a given directory
+"""
+
+
+import os
+import tvm
+from tvm.contrib import graph_runtime
+
+class TVMRuntimeUtil:
+
+    def __init__(self, fdir):
+
+        lib_path   = os.path.join(fdir,'tvm_dpu_cpu.so')
+        json_path  = os.path.join(fdir,'tvm_dpu_cpu.json')
+        param_path = os.path.join(fdir,'tvm_dpu_cpu.params')
+        
+        assert os.path.isfile(lib_path)\
+        and os.path.isfile(lib_path)\
+        and os.path.isfile(param_path),\
+            print("Error: compiled files do not exist in path: {}".format(fdir))
+        
+        self.fdir          = fdir
+        self.loaded_lib    = tvm.runtime.module.load_module(lib_path)
+        self.loaded_json   = open(json_path).read()
+        self.loaded_params = bytearray(open(param_path, 'rb').read())
+
+        target = tvm.cpu(0)
+        self.module = graph_runtime.create(self.loaded_json, self.loaded_lib, tvm.cpu(0))
+
+    def run(self, inputs):
+
+        self.module.load_params(self.loaded_params)
+        self.module.run(**inputs)
+        outs = [self.module.get_output(idx).asnumpy()
+                for idx in range(self.module.get_num_outputs())]
+
+        return outs
diff --git python/tvm/contrib/vai/vai_runner.py python/tvm/contrib/vai/vai_runner.py
new file mode 100644
index 0000000..c43c1d1
--- /dev/null
+++ python/tvm/contrib/vai/vai_runner.py
@@ -0,0 +1,155 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+''' Python Vitis-AI runtime API '''
+
+from ctypes import *
+import numpy as np
+import json
+import os
+import re
+
+
+class Tensor(Structure):
+    _fields_ = [
+        ('name', c_char_p),
+        ('dims', POINTER(c_int32)),
+        ('ndims', c_int32),
+        ('dtype', c_int32)
+    ]
+
+
+class Runner:
+    # tensor format enum
+    TensorFormat = type('', (), {})()
+    TensorFormat.NCHW = 0
+    TensorFormat.NHWC = 1
+
+    def __init__(self, path):
+        metaFile = os.path.join(path, "meta.json")
+        if not os.path.isfile(metaFile):
+            raise AssertionError("meta.json file %s not found" % metaFile)
+
+        # select .so file based on path/meta.json
+        with open(metaFile) as f:
+            meta = json.load(f)
+            libFile = self._parse_path(meta['lib'])
+
+        if not libFile or not os.path.isfile(libFile):
+            raise AssertionError("C++ library .so file %s not found" % libFile)
+
+        self._libFile = os.path.abspath(libFile)
+        self._lib = cdll.LoadLibrary(self._libFile)
+
+        self._lib.DpuPyRunnerCreate.argtypes = [c_char_p]
+        self._lib.DpuPyRunnerCreate.restype = c_void_p
+        self._lib.DpuPyRunnerGetInputTensors.argtypes = \
+            [c_void_p, POINTER(c_void_p), POINTER(c_int)]
+        self._lib.DpuPyRunnerGetOutputTensors.argtypes = \
+            [c_void_p, POINTER(c_void_p), POINTER(c_int)]
+        self._lib.DpuPyRunnerGetTensorFormat.argtypes = [c_void_p]
+        self._lib.DpuPyRunnerGetTensorFormat.restype = c_int
+        self._lib.DpuPyRunnerExecuteAsync.argtypes = \
+            [c_void_p,
+             POINTER(np.ctypeslib.ndpointer(c_float, flags="C_CONTIGUOUS")),
+             POINTER(np.ctypeslib.ndpointer(c_float, flags="C_CONTIGUOUS")),
+             c_int,
+             POINTER(c_int)]
+        self._lib.DpuPyRunnerExecuteAsync.restype = c_int
+        self._lib.DpuPyRunnerWait.argtypes = [c_void_p, c_int]
+        self._lib.DpuPyRunnerWait.restype = c_int
+        self._lib.DpuPyRunnerDestroy.argtypes = [c_void_p]
+
+        self._runner = self._lib.DpuPyRunnerCreate(path.encode('utf-8'))
+
+    def get_input_tensors(self):
+        ptr = c_void_p()
+        n = c_int(0)
+        self._lib.DpuPyRunnerGetInputTensors(self._runner, byref(ptr),
+                                             byref(n))
+        tensors = []
+        for i in range(n.value):
+            tensors.append(Tensor.from_address(ptr.value + (i*sizeof(Tensor))))
+        return tensors
+
+    def get_output_tensors(self):
+        ptr = c_void_p()
+        n = c_int(0)
+        self._lib.DpuPyRunnerGetOutputTensors(self._runner, byref(ptr),
+                                              byref(n))
+        tensors = []
+        for i in range(n.value):
+            tensors.append(Tensor.from_address(ptr.value + (i*sizeof(Tensor))))
+        return tensors
+
+    def get_tensor_format(self):
+        return(self._lib.DpuPyRunnerGetTensorFormat(self._runner))
+
+    def execute_async(self, inputs, outputs):
+        """
+        Args:
+            inputs: list of numpy arrays
+            outputs: list of numpy arrays
+            order of numpy arrays in inputs/outputs must match
+            the order in get_input_tensors() and get_output_tensors()
+        """
+        status = c_int(0)
+        ret = self._lib.DpuPyRunnerExecuteAsync(
+            self._runner,
+            self._numpy_list_2_cptr_list(inputs),
+            self._numpy_list_2_cptr_list(outputs),
+            inputs[0].shape[0], byref(status)
+        )
+
+        if status.value != 0:
+            raise RuntimeError("Runner.execute_async could not enqueue new"
+                               " DPU job")
+
+        return ret
+
+    def _numpy_list_2_cptr_list(self, nplist):
+        ptrList = (np.ctypeslib.ndpointer(c_float, flags="C_CONTIGUOUS") *
+                   len(nplist))()
+
+        for i, tensor in enumerate(nplist):
+            ptrList[i] = tensor.ctypes.data_as(
+                np.ctypeslib.ndpointer(c_float, flags="C_CONTIGUOUS"))
+
+        return ptrList
+
+    def _parse_path(self, path):
+        """
+        Translate any {STRING} in 'path' to os.environ["STRING"]
+        E.g., {XILINX_ROOT}/path/to/file to /opt/xilinx/path/to/file
+        """
+        retpath = path
+        regex = r"\{(.*?)\}"
+        matches = re.finditer(regex, path, re.MULTILINE | re.DOTALL)
+        for matchNum, match in enumerate(matches):
+            word = match.group(1)
+            retpath = retpath.replace("{"+word+"}", os.environ[word])
+
+        return retpath
+
+    def wait(self, job_id):
+        return self._lib.DpuPyRunnerWait(self._runner, job_id)
+
+    def __del__(self):
+        if hasattr(self, '_lib') and self._lib and\
+                hasattr(self, '_runner') and self._runner:
+            self._lib.DpuPyRunnerDestroy(self._runner)
+
diff --git python/tvm/contrib/vai/xgraph_to_relay_transform.py python/tvm/contrib/vai/xgraph_to_relay_transform.py
new file mode 100644
index 0000000..0fad604
--- /dev/null
+++ python/tvm/contrib/vai/xgraph_to_relay_transform.py
@@ -0,0 +1,323 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+"""
+Utility module for XGraph to Relay transformation
+"""
+
+
+import numpy as np
+import os
+import tvm
+import pyxir
+
+from tvm import relay
+
+
+
+class XgraphRelayTransform:
+
+    def __init__(self, target):
+        self.target = target
+        self.mod    = None   
+        
+    # HIGH LEVEL OBJECT FUNCTION THAT GETS CALLED TO TRANSFORM THE XGRAPH
+    def transform(self, xgraph, mod,work_dir):
+
+        xgraph_schedule = pyxir.schedule(xgraph, self.target)
+        sch_layers = xgraph_schedule.get_layers()
+                
+        start_idx, relay_map = self.relay_hash_transform(mod)
+        map_keys = list(relay_map.keys())
+        
+        assert (all(map_keys.count(x) == 1 for x in map_keys)),\
+            print("Error: Duplicate found in the Relay hash map")
+
+        c_output = xgraph.get_compiler_output()
+        for c_key in c_output.keys():
+            in_map = c_output.get_in_map(c_key)
+            out_map = c_output.get_out_map(c_key)
+
+        # PARSE THE SCHEDULED LAYERS
+        # CREATE A LIST OF SUBGRAPHS, AND POST_PROCESSING
+        subgraph_list   = []
+        postproc_list   = []
+        for layer in sch_layers:
+            try:
+                relay_id = layer.attrs['relay_id']
+            except KeyError:
+                relay_id  = -1
+         
+            if 'DPU' in layer.type[0] :
+                in_keys  = list(layer.attrs['input_layers'].keys())
+                out_keys = list(layer.attrs['output_layers'].keys())
+                kernel_name = layer.name
+
+                ins = []
+                ins_xgraph = []
+                for input_name in layer.attrs['input_names']:
+                    ins.append(layer.attrs['orig_bottom_tensors'][input_name][0])
+                    ins_xgraph.append(layer.attrs['__bottom_tensors'][input_name][0])
+
+                outs = [on for on in layer.attrs['output_names']]
+             
+                    
+                ins_xlayers   = map(lambda i: xgraph_schedule.get(i), ins)
+                outs_xlayers  = map(lambda o: xgraph.get(o), outs)
+                
+                
+                ins_rid   = []
+                ins_shape = [xgraph_schedule.get(i).shapes for i in ins_xgraph]
+                for xlayer in list(ins_xlayers):
+                    rid = xlayer.attrs['relay_id'][0]
+                    ins_rid.append(rid)
+              
+                        
+                outs_rid   = []
+                outs_shape = layer.shapes
+                for xlayer in list(outs_xlayers):
+                    outs_rid.append(xlayer.attrs['relay_id'][-1])###
+                    #outs_shape.append(xlayer.shapes)
+                                          
+                subgraph_list.append({ 'ins'  : dict(zip(ins_rid,ins_shape)),
+                                       'outs' : dict(zip(outs_rid,outs_shape)),
+                                       'attrs': {'in_map'     : in_map,
+                                                 'out_map'    : out_map,
+                                                 'kernel_name': kernel_name}
+                })
+                                                  
+            elif 'Softmax' in layer.type[0] and relay_id == -1 :
+                postproc_list += ['Softmax']
+
+        # PROCESS EACH SUBGRAPH AND MODIFY THE RELAY_MAP
+        for sg in subgraph_list:
+            ins_rid  = list(sg['ins'].keys())
+            input_shapes        = [tuple([1] + sg['ins'][rid][1:])
+                                    for rid in ins_rid]
+            relay_input_shapes = [tuple(self.get_shape(relay_map[rid]['expr']))
+                                  for rid in ins_rid]
+
+            outs_rid = list(sg['outs'].keys())
+            output_shapes        = [tuple([1] + sg['outs'][rid][1:])
+                                  for rid in outs_rid]
+            relay_output_shapes = [tuple(self.get_shape(relay_map[rid]['expr']))
+                                   for rid in outs_rid]
+            accel_input_names   = list(in_map.values())[0]
+            accel_output_names  = list(out_map.values()) #[out_map[on] for on in output_names]
+            accel_output_names  = '*'.join(accel_output_names)
+           
+
+            # ADD TRANSPOSE TO THE INPUT NODE
+            for rid,xs,rs in  zip(ins_rid,input_shapes,relay_input_shapes) :
+                
+                rs = tuple(map(lambda idx:int(idx), rs))
+
+                if xs != rs:
+                    if xs == (rs[0], rs[2], rs[3], rs[1]):
+                        axes = (0, 2, 3, 1)
+                    else:
+                        axes = (0, 3, 1, 2)
+
+                    expr          = relay_map[rid]['expr']
+                    new_expr      = tvm.relay.transpose(expr, axes=axes)
+                else:
+                    expr          = relay_map[rid]['expr']
+                    new_expr = expr
+                    
+                relay_map[rid]['expr']    = new_expr
+                relay_map[rid]['updated'] = True
+       
+            # CREATE THE SUBGRAPH NODE
+            ins_expr           = [relay_map[rid]['expr'] for rid in ins_rid]
+            accel_expr = relay.nn.accel(ins_expr,
+                                  output_shape = output_shapes,
+                                  input_name   = accel_input_names,
+                                  output_name  = accel_output_names,
+                                  kernel_name  = sg['attrs']['kernel_name'])
+                            
+
+                        
+            # UPDATE RELAY OUTPUT NODES
+            if len(outs_rid) > 1:
+                for idx,rid in enumerate(outs_rid):
+                    new_expr = tvm.relay.expr.TupleGetItem(accel_expr,idx)
+                    relay_map[rid]['expr']    = new_expr
+                    relay_map[rid]['updated'] = True
+            else:
+                relay_map[outs_rid[0]]['expr']    = accel_expr
+                relay_map[outs_rid[0]]['updated'] = True
+
+    
+            for rid,xs,rs in  zip(outs_rid,output_shapes,relay_output_shapes) :
+                
+                rs = tuple(map(lambda idx:int(idx),rs))
+
+                if xs != rs:
+                    if rs == (xs[0], xs[2], xs[3], xs[1]):
+                        axes = (0, 2, 3, 1)
+                    else:
+                        axes = (0, 3, 1, 2)
+
+                    expr          = relay_map[rid]['expr']
+                    new_expr      = tvm.relay.transpose(expr, axes=axes)
+                    
+                    relay_map[rid]['expr']    = new_expr
+                    relay_map[rid]['updated'] = True
+
+        # UPDATE THE RELAY GRAPH BASE ON THE RELAY_MAP
+        graph = self.update(relay_map[start_idx]['expr'], relay_map)
+
+        # ADD POST-PROCESSING LAYER
+        for op in postproc_list:
+            if op == 'Softmax':
+                graph = relay.nn.softmax(graph)
+
+      
+        mod = tvm.IRModule.from_expr(graph)
+            
+        return mod
+
+    def extract_hash(self, name, key=None):
+        """
+        Extract Relay expression hash from xgraph layer names
+        """
+        val = name.split('-')
+        if key == 'in_map':
+            val = name.split('_')
+            return int(val[2])
+        try:
+            return int(val[1])
+        except (IndexError,ValueError):
+            if len(val) == 1:
+                return val[0]
+            return int(val[1])
+        
+    def relay_hash_transform(self, mod):
+        hash_map = {}
+        expr = mod.functions[mod.get_global_var('main')]
+        expr = expr.body
+        relay_hash_map = self.recurse(expr,hash_map)
+        return (hash(expr),relay_hash_map)
+
+    def get_shape(self,expr):
+        if isinstance(expr, tvm.relay.expr.Var):
+            return expr.type_annotation.shape
+        elif isinstance(expr, tvm.relay.expr.Call):
+            return expr.checked_type.shape
+        else:
+            return None 
+        
+    def recurse(self, expr, hash_map):
+        """
+        Recursively to create a hash map of nodes
+        """
+
+        expr_hash = hash(expr)
+        if expr_hash in hash_map:
+            return hash_map
+        else:
+            if isinstance(expr, tvm.relay.function.Function):
+                children = []
+                hash_map[expr_hash] = {'expr':expr,'children': children, 'updated': False}
+                self.recurse(expr.body, hash_map)
+                return hash_map
+            
+            elif isinstance(expr, tvm.relay.expr.Call):
+                children = [hash(child) for child in expr.args]
+                hash_map[expr_hash] = {'expr':expr,'children': children, 'updated': False}
+                for node in expr.args:
+                    self.recurse(node, hash_map)
+                return hash_map
+                
+            elif isinstance(expr, tvm.relay.expr.TupleGetItem):
+                children = []
+                hash_map[expr_hash] = {'expr':expr,'children': children, 'updated': False}
+                self.recurse(expr.tuple_value, hash_map)
+                return hash_map
+            
+            elif isinstance(expr, tvm.relay.expr.Var):
+                children = []
+                hash_map[expr_hash] = {'expr':expr,'children': children, 'updated': False}
+                return hash_map
+            
+            elif isinstance(expr, tvm.relay.expr.Tuple):
+                children = [hash(child) for child in expr.fields]
+                hash_map[expr_hash] = {'expr':expr,'children': children, 'updated': False}
+                for node in expr.fields:
+                    self.recurse(node, hash_map)
+                return hash_map
+            elif isinstance(expr, tvm.relay.expr.Constant):
+                children = []
+                hash_map[expr_hash] = {'expr':expr,'children': children, 'updated': False}
+                return hash_map
+            
+            else:
+                raise NotImplementedError("Condition to create hash map for node type {}"\
+                                          " has not been implemented"\
+                                                  .format(type(expr)))
+            
+    def update(self, expr, hash_map):
+        """
+        Recursively to create a hash map of nodes
+        """
+
+        expr_hash = hash(expr)
+        if expr_hash not in hash_map:
+            # Happens when accel is the last layer in the expr
+            # Return this expression
+            return expr
+        elif hash_map[expr_hash]['updated'] == True:
+            return  hash_map[expr_hash]['expr']
+        else:
+            hash_map[expr_hash]['updated'] = True
+            
+            if isinstance(expr, tvm.relay.function.Function):
+                new_body = self.update(expr.body, hash_map)
+                new_expr = relay.function.Function(expr.params,new_body,expr.ret_type,expr.type_params)
+            
+            elif isinstance(expr, tvm.relay.expr.Call):
+                new_args = [self.update(child, hash_map) for child in expr.args]
+                new_expr = relay.Call(expr.op, new_args, expr.attrs, expr.type_args)
+                
+            elif isinstance(expr, tvm.relay.expr.TupleGetItem):
+                tuple_value = self.update(expr.tuple_value, hash_map)
+                new_expr    = relay.TupleGetItem(tuple_value,expr.index)
+            
+            elif isinstance(expr, tvm.relay.expr.Var):
+                return expr
+            
+            elif isinstance(expr, tvm.relay.expr.Tuple):
+                new_fields = [self.update(child, hash_map) for child in expr.fields]
+                new_expr = relay.Tuple(new_fields)
+                
+            elif isinstance(expr,tvm.relay.expr.If):
+                true_branch  = self.update(expr.true_branch, hash_map)
+                false_branch = self.update(expr.false_branch, hash_map)
+                new_expr     = relay.If(expr.cond,true_branch,false_branch)
+
+            elif isinstance(expr,tvm.relay.GlobalVar):
+                return expr
+
+            elif isinstance(expr, tvm.relay.expr.Constant):
+                return expr
+                                       
+            else:
+                raise NotImplementedError("Condition to recurse relay graph for node type {}"\
+                                          " has not been implemented"\
+                                                  .format(type(expr)))
+            hash_map[expr_hash]['expr'] = new_expr
+            return new_expr
diff --git python/tvm/relay/frontend/tensorflow.py python/tvm/relay/frontend/tensorflow.py
index ab9e9e6..46eee4a 100644
--- python/tvm/relay/frontend/tensorflow.py
+++ python/tvm/relay/frontend/tensorflow.py
@@ -265,7 +265,8 @@ def _conv(opname):
                 attr['strides'][3], attr['strides'][1], attr['strides'][2]
             attr['data_format'] = 'NCHW'
 
-            if opname == 'conv_transpose' and len(attr['_output_shapes']) > 0:
+            if opname == 'conv_transpose' and len(attr['_output_shapes']) > 0\
+                    and attr['_output_shapes'][0] is not None:
                 tmp_shape = attr['_output_shapes'][0]
                 tmp_shape = [tmp_shape[ii] for ii in (0, 3, 1, 2)]
                 attr['_output_shapes'][0] = tmp_shape
@@ -350,7 +351,8 @@ def _conv(opname):
             kernel_h, kernel_w = attr['kernel_shape']
 
             pdata_shape = input_shape
-            if opname == 'conv_transpose' and len(attr['_output_shapes']) > 0:
+            if opname == 'conv_transpose' and len(attr['_output_shapes']) > 0\
+                    and attr['_output_shapes'][0] is not None:
                 pdata_shape = attr['_output_shapes'][0]
 
             if attr['data_format'] == 'NHWC':
@@ -374,8 +376,10 @@ def _conv(opname):
             raise tvm.error.OpAttributeInvalid(msg.format(attr['padding']))
 
         if 'kernel_layout' not in attr:
-            if opname in ['conv', 'conv_transpose']:
+            if opname == 'conv':
                 attr['kernel_layout'] = 'HWIO' if attr['data_format'] == 'NHWC' else 'OIHW'
+            elif opname == 'conv_transpose':
+                attr['kernel_layout'] = 'HWOI' if attr['data_format'] == 'NHWC' else 'IOHW'
             else:
                 attr['kernel_layout'] = 'HWOI' if attr['data_format'] == 'NHWC' else 'OIHW'
 
diff --git python/tvm/relay/op/nn/nn.py python/tvm/relay/op/nn/nn.py
index 96708c9..e902bdc 100644
--- python/tvm/relay/op/nn/nn.py
+++ python/tvm/relay/op/nn/nn.py
@@ -991,6 +991,88 @@ def avg_pool2d_grad(out_grad,
     return _make.avg_pool2d_grad(out_grad, data, pool_size, strides, padding,
                                  layout, ceil_mode, count_include_pad)
 
+def max_pool2d_grad(out_grad,
+                    data,
+                    pool_size=(1, 1),
+                    strides=(1, 1),
+                    padding=(0, 0),
+                    layout="NCHW",
+                    ceil_mode=False):
+    r"""Gradient of 2D maximum pooling operator.
+
+    This operator takes out_grad and data as input and calculates gradient of max_pool2d.
+
+    Parameters
+    ----------
+    out_grad : tvm.relay.Expr
+        The output gradient
+
+    data : tvm.relay.Expr
+        The input data to the operator.
+
+    strides : tuple of int, optional
+        The strides of pooling.
+
+    padding : tuple of int, optional
+        The padding for pooling.
+
+    layout : str, optional
+        Layout of the input.
+
+    ceil_mode : bool, optional
+        To enable or disable ceil while pooling.
+
+    Returns
+    -------
+    result : tvm.relay.Expr
+        The computed result.
+    """
+    return _make.max_pool2d_grad(out_grad, data, pool_size, strides, padding,
+                                 layout, ceil_mode)
+
+def avg_pool2d_grad(out_grad,
+                    data,
+                    pool_size=(1, 1),
+                    strides=(1, 1),
+                    padding=(0, 0),
+                    layout="NCHW",
+                    ceil_mode=False,
+                    count_include_pad=False):
+    r"""Gradient of 2D average pooling operator.
+
+    This operator takes out_grad and data as input and calculates gradient of avg_pool2d.
+
+    Parameters
+    ----------
+    out_grad : tvm.relay.Expr
+        The output gradient
+
+    data : tvm.relay.Expr
+        The input data to the operator.
+
+    strides : tuple of int, optional
+        The strides of pooling.
+
+    padding : tuple of int, optional
+        The padding for pooling.
+
+    layout : str, optional
+        Layout of the input.
+
+    ceil_mode : bool, optional
+        To enable or disable ceil while pooling.
+
+    count_include_pad : bool, optional
+        To include padding to compute the average.
+
+    Returns
+    -------
+    result : tvm.relay.Expr
+        The computed result.
+    """
+    return _make.avg_pool2d_grad(out_grad, data, pool_size, strides, padding,
+                                 layout, ceil_mode, count_include_pad)
+
 def global_max_pool2d(data,
                       layout="NCHW"):
     r"""2D global maximum pooling operator.
@@ -1720,7 +1802,7 @@ def layer_norm(data,
     Parameters
     ----------
     data : tvm.relay.Expr
-        Input to which layer_norm will be applied.
+        Input to which batch_norm will be applied.
 
     gamma : tvm.relay.Expr
         The gamma scale factor.
@@ -1904,6 +1986,7 @@ def sparse_transpose(x):
     return expr.TupleWrapper(
         _make.sparse_transpose(x.data, x.indices, x.indptr), 3)
 
+
 def contrib_conv2d_winograd_without_weight_transform(data,
                                                      weight,
                                                      tile_size,
@@ -2632,7 +2715,6 @@ def adaptive_max_pool3d(data,
     ----------
     data : tvm.relay.Expr
         The input data to the operator.
-
     output_size : tuple of int. optional
         Output height and width.
 
@@ -2761,3 +2843,37 @@ def global_avg_pool3d(data,
     """
     output_size = [1, 1, 1]
     return _make.adaptive_avg_pool3d(data, output_size, layout)
+
+
+def accel(data,
+          output_shape,
+          input_name,
+          output_name,
+          kernel_name):
+
+    """ Accelerator operator.
+    Provides hook to insert generic TVM accelerator calls
+    output_shape : tvm.relay.Expr
+        The expected output shape.
+
+    input_name : str
+        Tensor input name.
+
+    output_name : str
+        Tensor output name.
+
+    kernel_name : str
+        Identifier of the subgraph to be executed.
+
+    Returns
+    -------
+    result : tvm.relay.Expr
+        The computed result.
+    """
+    
+    data = list(data)
+    return _make.accel(expr.Tuple(data),
+                       output_shape,
+                       input_name,  
+                       output_name, 
+                       kernel_name)
diff --git python/tvm/relay/testing/tf.py python/tvm/relay/testing/tf.py
index dc7937c..93632b2 100644
--- python/tvm/relay/testing/tf.py
+++ python/tvm/relay/testing/tf.py
@@ -32,7 +32,7 @@ from tvm.contrib.download import download_testdata
 
 try:
     tf_compat_v1 = tf.compat.v1
-except ImportError:
+except (ImportError, AttributeError):
     tf_compat_v1 = tf
 
 ######################################################################
diff --git src/relay/op/nn/nn.cc src/relay/op/nn/nn.cc
index d65fc27..9189a55 100644
--- src/relay/op/nn/nn.cc
+++ src/relay/op/nn/nn.cc
@@ -1114,5 +1114,76 @@ RELAY_REGISTER_OP("nn.space_to_depth")
     .set_support_level(5)
     .add_type_rel("SpaceToDepth", SpaceToDepthRel);
 
+// Generic Accelerator 
+TVM_REGISTER_NODE_TYPE(ACCELAttrs);
+  
+// relay.nn.accel
+bool ACCELRel(const Array<Type>& types,
+                    int num_inputs,
+                    const Attrs& attrs,
+                    const TypeReporter& reporter) {
+
+  const auto* data = types[0].as<TupleTypeNode>();
+  const auto& first = Downcast<TensorType>(data->fields[0]);
+
+  if (data == nullptr) {
+    CHECK(types[0].as<IncompleteTypeNode>())
+      << "cast: expect input type to be TupleType but get "
+      << types[0];
+    return false;
+  }
+  
+  const auto* param = attrs.as<ACCELAttrs>();
+  CHECK(param != nullptr);
+
+  std::vector<Type> fields;
+
+  for (const auto& os : param->output_shape){
+    auto ret_type = TensorType(os, first->dtype);
+    fields.push_back(ret_type);
+  }
+
+  if (fields.size() == 1)
+    reporter->Assign(types[1], fields.front());
+  else
+    reporter->Assign(types[1], TupleType(Array<Type>(fields)));
+
+  return true;
+}
+
+
+Expr MakeACCEL(Expr data,
+	       Array<Array<IndexExpr>> output_shape,
+	       std::string      input_name,
+	       std::string      output_name,
+	       std::string      kernel_name
+	       ) {
+  auto attrs = make_object<ACCELAttrs>();
+    
+  attrs->output_shape = std::move(output_shape);
+  attrs->input_name	= std::move(input_name	);
+  attrs->output_name	= std::move(output_name );
+  attrs->kernel_name	= std::move(kernel_name );
+    
+  static const Op& op = Op::Get("nn.accel");
+
+
+  return Call(op, {data}, Attrs(attrs), {});
+
+}
+
+
+TVM_REGISTER_GLOBAL("relay.op.nn._make.accel").set_body_typed(MakeACCEL);
+
+RELAY_REGISTER_OP("nn.accel")
+.describe(R"code(acceleration OP that runs fused operation using the accelration runtime)code" TVM_ADD_FILELINE)
+.set_attrs_type<ACCELAttrs>()
+.set_num_inputs(1)
+.add_argument("data","Tensor", "List of Input Tensors")
+.set_support_level(15)
+.add_type_rel("ACCEL",ACCELRel)
+.set_attr<TOpPattern>("TOpPattern",kOutEWiseFusable);
+
 }  // namespace relay
 }  // namespace tvm
+
diff --git tutorials/accelerators/compile/mxnet_resnet_18.py tutorials/accelerators/compile/mxnet_resnet_18.py
new file mode 100644
index 0000000..47d18c4
--- /dev/null
+++ tutorials/accelerators/compile/mxnet_resnet_18.py
@@ -0,0 +1,218 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+"""
+Compile TVM model for Xilinx Vitis-AI acceleration
+==================================================
+
+This example shows how to build a TVM convolutional neural network 
+model with Relay for Vitis-AI acceleration
+
+Setup: 
+    - Add imagenet validation subset for calibration in imagenet/val-small
+
+"""
+
+import os
+import numpy as np
+
+import logging
+logging.basicConfig()
+logger = logging.getLogger('pyxir')
+logger.setLevel(logging.INFO)
+
+import pyxir
+
+import tvm
+from tvm import contrib
+import tvm.relay as relay
+from tvm.contrib.vai import base
+from tvm.contrib.vai.relay_transform import PartitioningPass
+from tvm.contrib.vai import extern_accel
+from tvm.contrib.vai.tvmruntime_util import TVMRuntimeUtil
+
+import time
+import cv2
+
+FILE_DIR   = os.path.dirname(os.path.abspath(__file__))
+HOME_DIR = os.getenv('HOME')
+######################################################################
+# Download Resnet18 model from Gluon Model Zoo
+# ---------------------------------------------
+# In this section, we download a pretrained imagenet model and classify an image.
+###############################################################################
+from tvm.contrib.download import download_testdata
+from mxnet.gluon.model_zoo.vision import get_model
+from PIL import Image
+#from matplotlib import pyplot as plt
+block = get_model('resnet18_v1', pretrained=True)
+img_url = 'https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true'
+img_name = 'cat.png'
+synset_url = ''.join(['https://gist.githubusercontent.com/zhreshold/',
+                      '4d0b62f3d01426887599d4f7ede23ee5/raw/',
+                      '596b27d23537e5a1b5751d2b0481ef172f58b539/',
+                      'imagenet1000_clsid_to_human.txt'])
+synset_name = 'imagenet1000_clsid_to_human.txt'
+img_path = download_testdata(img_url, 'cat.png', module='data')
+synset_path = download_testdata(synset_url, synset_name, module='data')
+with open(synset_path) as f:
+    synset = eval(f.read())
+
+def transform_image(image):
+    image = np.array(image) - np.array([123., 117., 104.])
+    image /= np.array([58.395, 57.12, 57.375])
+    image = image.transpose((2, 0, 1))
+    image = image[np.newaxis, :]
+    return image
+
+# DOWNLOAD IMAGE FOR TEST
+image = Image.open(img_path).resize((224, 224))
+image = transform_image(image)
+
+###############################################################################
+# MODEL SETTINGS
+#
+# Parameter settings for compiling a model using tvm-vai flow
+# quant_dir      : path to images for quantization
+# shape_dict     : dictionary of input names as keys and input shapes as values
+#                  dict{input_name:input_shape}
+# postprocessing : 'Softmax' if necessary
+# target         : hardware accelerator to run the compiled model
+#                      options: 'DPUCADX8G', 'DPUCZDX8G-zcu104', 'DPUCZDX8G-zcu102'
+#                      options(deprecated): 'dpuv1', 'dpuv2-zcu104', 'dpuv2-zcu102'
+
+###############################################################################
+
+quant_dir      = os.path.join(HOME_DIR,'CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min')
+shape_dict     = {'data': image.shape}
+postprocessing = ['Softmax']
+target         = 'DPUCADX8G'
+
+###############################################################################
+# INPUTS FUNC
+#
+# Define and inputs function which takes in an iterator value and returns a
+# dictionary mapping from input name to array containing dataset inputs. Note 
+# that the input function should always return image data in NCHW layout as 
+# all models are converted to NCHW layout internally for Vitis-AI compilation.
+# 
+# This is necessary for quantizating the model for acceleration using Vitis-AI.
+###############################################################################
+
+def inputs_func(iter):
+    import os
+
+    img_files = [os.path.join(quant_dir, f) for f in os.listdir(quant_dir) if f.endswith(('JPEG', 'jpg', 'png'))][:10]
+    size=shape_dict[list(shape_dict.keys())[0]][2:]
+
+    imgs = []
+    for path in img_files:
+        img = cv2.imread(path)
+        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
+        imgs.append(img.astype(np.float32))
+        
+    out = []
+    for img in imgs:
+
+        img = cv2.resize(img, tuple(size), interpolation=1)
+        img = transform_image(img)
+        img = img.reshape(img.shape[1:])
+        out.append(img)
+
+        
+    res = np.array(out).astype(np.float32)
+    print (res.shape)
+    input_name = list(shape_dict.keys())[0]
+    return {input_name: res}
+
+
+###############################################################################
+# PARTITION & BUILD
+# 
+# Module pass to partition Relay for Vitis-AI acceleration. Targets can be
+#  'DPUCADX8G', 'DPUCZDX8G-zcu104', 'DPUCZDX8G-zcu102'
+# Afterwards build graph, lib and params using standard TVM flow.
+##############################################################################
+
+if  target.startswith('DPUCZ') or target.startswith('dpuv2'):
+    tvm_target = tvm.target.arm_cpu('ultra96')
+    lib_kwargs = {
+        'fcompile': contrib.cc.create_shared,
+        'cc': "/usr/aarch64-linux-gnu/bin/ld"
+    }
+else:
+    tvm_target = 'llvm'
+    lib_kwargs = {}
+
+mod, params = relay.frontend.from_mxnet(block, shape_dict)
+ 
+# CUSTOM VAI MODULE PASS
+ 
+mod = PartitioningPass(target=target, params=params,
+                       inputs_func=inputs_func, postprocessing= postprocessing)(mod)
+ 
+print("Mod", mod)
+graph, lib, params = relay.build(
+    mod, tvm_target, params=params)
+
+print(" Compilation successfully finished")
+###############################################################################
+# SAVE OUTPUT
+# 
+# Save the output files for running on the board
+##############################################################################
+TVM_OUTPUT_DIR = os.path.join(FILE_DIR, "mxnet_resnet_18")
+DPU_OUTPUT_DIR = os.path.join(FILE_DIR, "mxnet_resnet_18/libdpu")
+os.makedirs(DPU_OUTPUT_DIR, exist_ok = True)
+
+lib.export_library(os.path.join(TVM_OUTPUT_DIR,"tvm_dpu_cpu.so"), **lib_kwargs)
+ 
+with open(os.path.join(TVM_OUTPUT_DIR,"tvm_dpu_cpu.json"),"w") as f:
+    f.write(graph)
+    
+with open(os.path.join(TVM_OUTPUT_DIR,"tvm_dpu_cpu.params"), "wb") as f:
+    f.write(relay.save_param_dict(params))
+
+import glob
+from shutil import copy2
+
+if target == 'DPUCADX8G' or target == 'dpuv1':
+    DPU_LIBDIR = '/tmp/vai/'
+    try:
+        copy2(os.path.join(DPU_LIBDIR,"meta.json"), DPU_OUTPUT_DIR)
+        copy2(os.path.join(DPU_LIBDIR,"compiler.json"), DPU_OUTPUT_DIR)
+        copy2(os.path.join(DPU_LIBDIR,"weights.h5"), DPU_OUTPUT_DIR)
+        copy2(os.path.join(DPU_LIBDIR,"quantizer.json"), DPU_OUTPUT_DIR)
+    except IOError as e:
+        print("Compiled files were not found in directory: ", DPU_LIBDIR)
+
+else:
+    DPU_LIBDIR = FILE_DIR
+    try:
+        copy2(os.path.join(DPU_LIBDIR,"meta.json"), DPU_OUTPUT_DIR)
+        copy2(os.path.join(DPU_LIBDIR,glob.glob("libdpu*.so")[0]), DPU_OUTPUT_DIR)
+    except IOError as e:
+        print("Compiled files were not found in directory: ", DPU_LIBDIR)
+
+
+
+###############################################################################
+# MOVE FILES TO BOARD
+# 
+# Copy the mxnet_resnet_18 output directory to the board
+# Run the compiled model on the board using  /tvm/tutorials/accelerators/run/mxnet_resnet_18.py
+##############################################################################
+ 
diff --git tutorials/accelerators/compile/relay_darknet_yolov2.py tutorials/accelerators/compile/relay_darknet_yolov2.py
new file mode 100755
index 0000000..b59bcfa
--- /dev/null
+++ tutorials/accelerators/compile/relay_darknet_yolov2.py
@@ -0,0 +1,227 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+"""
+Compile TVM model for Xilinx Vitis-AI acceleration
+==================================================
+
+This example shows how to build a TVM convolutional neural network 
+model with Relay for Vitis-AI acceleration
+
+Setup: 
+    - Add imagenet validation subset for calibration in imagenet/val-small
+
+"""
+
+import os
+import numpy as np
+import sys
+
+import logging
+logging.basicConfig()
+logger = logging.getLogger('pyxir')
+logger.setLevel(logging.INFO)
+#logger.setLevel(logging.DEBUG)
+
+import pyxir
+
+import tvm
+from tvm import contrib
+import tvm.relay as relay
+from tvm.contrib.vai import base
+from tvm.contrib.vai.relay_transform import PartitioningPass
+from tvm.contrib.vai import extern_accel
+from tvm.contrib.vai.tvmruntime_util import TVMRuntimeUtil
+from tvm.contrib.download import download_testdata
+from tvm.relay.testing.darknet import __darknetffi__
+import tvm.relay.testing.yolo_detection
+import tvm.relay.testing.darknet
+
+
+FILE_DIR   = os.path.dirname(os.path.abspath(__file__))
+HOME_DIR   = os.getenv('HOME')
+######################################################################
+# Choose the model
+# -----------------------
+# Models are: 'yolov2', 'yolov3' or 'yolov3-tiny'
+######################################################################
+
+# Model name
+MODEL_NAME = 'yolov2'
+
+######################################################################
+# Download required files
+# -----------------------
+# Download cfg and weights file if first time.
+######################################################################
+
+CFG_NAME = MODEL_NAME + '.cfg'
+WEIGHTS_NAME = MODEL_NAME + '.weights'
+REPO_URL = 'https://github.com/dmlc/web-data/blob/master/darknet/'
+CFG_URL = 'https://raw.githubusercontent.com/pjreddie/darknet/master/' + 'cfg/' + CFG_NAME + '?raw=true'
+WEIGHTS_URL = 'https://pjreddie.com/media/files/' + WEIGHTS_NAME
+
+cfg_path = download_testdata(CFG_URL, CFG_NAME, module="darknet")
+weights_path = download_testdata(WEIGHTS_URL, WEIGHTS_NAME, module="darknet")
+
+
+# Download and Load darknet library
+if sys.platform in ['linux', 'linux2']:
+    DARKNET_LIB = 'libdarknet2.0.so'
+    DARKNET_URL = REPO_URL + 'lib/' + DARKNET_LIB + '?raw=true'
+elif sys.platform == 'darwin':
+    DARKNET_LIB = 'libdarknet_mac2.0.so'
+    DARKNET_URL = REPO_URL + 'lib_osx/' + DARKNET_LIB + '?raw=true'
+else:
+    err = "Darknet lib is not supported on {} platform".format(sys.platform)
+    raise NotImplementedError(err)
+
+lib_path = download_testdata(DARKNET_URL, DARKNET_LIB, module="darknet")
+
+DARKNET_LIB = __darknetffi__.dlopen(lib_path)
+net = DARKNET_LIB.load_network(cfg_path.encode('utf-8'), weights_path.encode('utf-8'), 0)
+dtype = 'float32'
+batch_size = 1
+
+data = np.empty([batch_size, net.c, net.h, net.w], dtype)
+print("Converting darknet to relay functions...")
+
+###############################################################################
+# MODEL SETTINGS
+#
+# Parameter settings for compiling a model using tvm-vai flow
+# quant_dir      : path to images for quantization
+# shape_dict     : dictionary of input names as keys and input shapes as values
+#                  dict{input_name:input_shape}
+# postprocessing : 
+# target         : hardware accelerator to run the compiled model
+#                      options: 'DPUCADX8G', 'DPUCZDX8G-zcu104', 'DPUCZDX8G-zcu102'
+#                      options: 'dpuv1', 'dpuv2-zcu104', 'dpuv2-zcu102'
+
+###############################################################################
+
+quant_dir      = os.path.join(HOME_DIR,'CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min')
+shape_dict     = {'data': data.shape}
+postprocessing = 'Softmax'
+target         = 'DPUCADX8G'
+
+###############################################################################
+# INPUTS FUNC
+#
+# Define and inputs function which takes in an iterator value and returns a
+# dictionary mapping from input name to array containing dataset inputs. Note 
+# that the input function should always return image data in NCHW layout as 
+# all models are converted to NCHW layout internally for Vitis-AI compilation.
+# 
+# This is necessary for quantizating the model for acceleration using Vitis-AI.
+###############################################################################
+
+def inputs_func(iter):
+    import os
+
+    img_files = [os.path.join(quant_dir, f) for f in os.listdir(quant_dir) if f.endswith(('JPEG', 'jpg', 'png'))][:10]
+       
+    [neth, netw] = shape_dict['data'][2:]
+    out = []
+    for img_path in img_files:
+            out.append(tvm.relay.testing.darknet.load_image(img_path, netw, neth))
+
+    res = np.array(out)
+    print (res.shape)
+    input_name = list(shape_dict.keys())[0]
+    return {input_name: res}
+
+
+###############################################################################
+# PARTITION & BUILD
+# 
+# Module pass to partition Relay for Vitis-AI acceleration. Targets can be
+#  'DPUCADX8G', 'DPUCZDX8G-zcu104', 'DPUCZDX8G-zcu102'
+# Afterwards build graph, lib and params using standard TVM flow.
+##############################################################################
+
+from pyxir.frontend.tvm import load_model_from_file
+
+
+if  target.startswith('DPUCZ') or target.startswith('dpuv2'):
+    tvm_target = tvm.target.arm_cpu('ultra96')
+    lib_kwargs = {
+        'fcompile': contrib.cc.create_shared,
+        'cc': "/usr/aarch64-linux-gnu/bin/ld"
+    }
+else:
+    tvm_target = 'llvm'
+    lib_kwargs = {}
+    
+
+mod, params = relay.frontend.from_darknet(net, dtype=dtype, shape=data.shape)
+
+# CUSTOM VAI MODULE PASS
+mod = PartitioningPass(target=target, params=params,
+                       inputs_func=inputs_func, postprocessing= postprocessing)(mod)
+
+print("Mod", mod)
+graph, lib, params = relay.build(
+    mod, tvm_target, params=params)
+
+print(" Compilation successfully finished")
+###############################################################################
+# SAVE OUTPUT
+# 
+# Save the output files for running on the board
+##############################################################################
+
+TVM_OUTPUT_DIR = os.path.join(FILE_DIR, "relay_darknet_yolov2")
+DPU_OUTPUT_DIR = os.path.join(FILE_DIR, "relay_darknet_yolov2/libdpu")
+os.makedirs(DPU_OUTPUT_DIR, exist_ok = True)
+
+lib.export_library(os.path.join(TVM_OUTPUT_DIR,"tvm_dpu_cpu.so"), **lib_kwargs)
+ 
+with open(os.path.join(TVM_OUTPUT_DIR,"tvm_dpu_cpu.json"),"w") as f:
+    f.write(graph)
+    
+with open(os.path.join(TVM_OUTPUT_DIR,"tvm_dpu_cpu.params"), "wb") as f:
+    f.write(relay.save_param_dict(params))
+
+import glob
+from shutil import copy2
+
+if target == 'DPUCADX8G' or target == 'dpuv1':
+    DPU_LIBDIR = '/tmp/vai/'
+    try:
+        copy2(os.path.join(DPU_LIBDIR,"meta.json"), DPU_OUTPUT_DIR)
+        copy2(os.path.join(DPU_LIBDIR,"compiler.json"), DPU_OUTPUT_DIR)
+        copy2(os.path.join(DPU_LIBDIR,"weights.h5"), DPU_OUTPUT_DIR)
+        copy2(os.path.join(DPU_LIBDIR,"quantizer.json"), DPU_OUTPUT_DIR)
+    except IOError as e:
+        print("Compiled files were not found in {} directory".format(DPU_LIBDIR))
+else:
+    DPU_LIBDIR = FILE_DIR
+    try:
+        copy2(os.path.join(DPU_LIBDIR,"meta.json"), DPU_OUTPUT_DIR)
+        copy2(os.path.join(DPU_LIBDIR,glob.glob("libdpu*.so")[0]), DPU_OUTPUT_DIR)
+    except IOError as e:
+        print("Compiled files were not found in {} directory".format(DPU_LIBDIR))    
+
+
+        
+###############################################################################
+# MOVE FILES TO BOARD
+# 
+# Copy the relay_darknet_yolov2 output directory to the board
+# Run the compiled model on the board using  /tvm/tutorials/accelerators/run/relay_darknet_yolov2.py
+##############################################################################
+ 
diff --git tutorials/accelerators/run/mxnet_resnet_18.py tutorials/accelerators/run/mxnet_resnet_18.py
new file mode 100644
index 0000000..cb29284
--- /dev/null
+++ tutorials/accelerators/run/mxnet_resnet_18.py
@@ -0,0 +1,130 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+"""
+Run TVM model for Xilinx Vitis-AI acceleration
+==================================================
+
+This example shows how to run MxNet Resent_18 model
+ built with TVM for Vitis-AI acceleration
+
+"""
+
+import os
+import argparse
+import numpy as np
+import time
+
+
+from PIL import Image
+from tvm.contrib.download import download_testdata
+from tvm.contrib.vai import extern_accel
+from tvm.contrib.vai.tvmruntime_util import TVMRuntimeUtil
+
+FILE_DIR = os.path.dirname(os.path.abspath(__file__))
+
+img_url = 'https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true'
+img_name = 'cat.png'
+synset_url = ''.join(['https://gist.githubusercontent.com/zhreshold/',
+                      '4d0b62f3d01426887599d4f7ede23ee5/raw/',
+                      '596b27d23537e5a1b5751d2b0481ef172f58b539/',
+                      'imagenet1000_clsid_to_human.txt'])
+synset_name = 'imagenet1000_clsid_to_human.txt'
+img_path = download_testdata(img_url, 'cat.png', module='data')
+synset_path = download_testdata(synset_url, synset_name, module='data')
+with open(synset_path) as f:
+    synset = eval(f.read())
+
+def transform_image(image):
+    image = np.array(image) - np.array([123., 117., 104.])
+    image /= np.array([58.395, 57.12, 57.375])
+    image = image.transpose((2, 0, 1))
+    image = image[np.newaxis, :]
+    return image
+
+
+
+def run(fdir, dpu_rundir,shape_dict, iterations):
+
+    # SETUP
+    extern_accel.setDpuRunDir(dpu_rundir)
+
+    
+    # DOWNLOAD IMAGE FOR TEST
+    img_shape=shape_dict[list(shape_dict.keys())[0]][2:]
+    print(img_shape)
+    image = Image.open(img_path).resize(img_shape)
+    
+    # IMAGE PRE-PROCESSING
+    image = transform_image(image)
+    
+    # RUN #
+    inputs = {}
+    inputs[list(shape_dict.keys())[0]] = image
+
+    
+    # VAI FLOW
+    tru = TVMRuntimeUtil(fdir)
+    for i in range(iterations):
+        start = time.time()
+        res = tru.run(inputs)
+        stop = time.time()
+        
+        print("VAI iteration: {}/{}, run time: {}".format(i+1, iterations, stop - start))
+        
+        # PREDICTIONS #
+        for idx, prediction in enumerate(res[0]):
+            print('-----------------------')
+            top_k = prediction.argsort()[-1:-(5+1):-1]
+            print('TVM-VAI prediction top-5:')
+            for pred in top_k:
+                print (pred,synset[pred])
+        
+    del extern_accel.RUNNER_CACHE
+
+###############################################################################
+# RUN MXNET_RESNET_18
+# 
+# Before running the mxnet_resnet_18 model, you have to compile the model
+# using the script provided at /tvm/tutorials/accelerators/compile/mxnet_resnet_18.py
+# The compile script generates an output file name "mxnet_resnet_18"
+# Once you setup your device, you could run the model as follows:
+#
+# Parameter settings for the run script:
+# -f           : Path to directory containing TVM compilation files
+# -d           : Path to directory containing DPU model lib 
+# --iterations : The number of iterations to run the model
+#
+# example:
+# ./mxnet_resnet_18.py -f /PATH_TO_DIR/mxnet_resnet_18 -d /PATH_TO_DIR/mxnet_resnet_18/libdpu --iterations 1
+
+##############################################################################
+ 
+    
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("-f", help="Path to directory containing TVM compilation files", default=FILE_DIR)
+    parser.add_argument("-d", help="Path to directory containing DPU model lib and meta.json", required=True)
+    parser.add_argument("--iterations", help="The number of iterations to run.", default=2, type=int)
+    args = parser.parse_args()
+    fdir = args.f if os.path.isabs(args.f) else os.path.join(os.getcwd(), args.f)
+    dpu_rundir = args.d if  os.path.isabs(args.d) else os.path.join(os.getcwd(), args.d)
+    iterations = args.iterations
+    shape_dict = {'data': [1, 3, 224, 224]}
+
+    
+    run(fdir, dpu_rundir, shape_dict, iterations)
diff --git tutorials/accelerators/run/relay_darknet_yolov2.py tutorials/accelerators/run/relay_darknet_yolov2.py
new file mode 100755
index 0000000..07b108c
--- /dev/null
+++ tutorials/accelerators/run/relay_darknet_yolov2.py
@@ -0,0 +1,164 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+"""
+Compile TVM model for Xilinx Vitis-AI acceleration
+==================================================
+
+This example shows how to build a TVM convolutional neural network 
+model with Relay for Vitis-AI acceleration
+
+Setup: 
+    - Add imagenet validation subset for calibration in imagenet/val-small
+
+"""
+import os
+import sys
+import argparse
+import numpy as np
+import signal
+import shutil
+import sys
+import time
+import cv2
+
+from tvm.contrib.vai import extern_accel
+from tvm.contrib.vai.tvmruntime_util import TVMRuntimeUtil
+
+from tvm.contrib.download import download_testdata
+import os, importlib, sys
+import tvm
+# manually import yolo_detection and darkent to avoid libtvm.so dependency
+tvm_dir = os.path.dirname(tvm.__file__)
+detection_path= os.path.join(tvm_dir,"relay/testing/yolo_detection.py")
+darknet_path= os.path.join(tvm_dir,"relay/testing/darknet.py")
+detection_spec = importlib.util.spec_from_file_location('tvm.relay.testing.yolo_detection',detection_path)
+darknet_spec = importlib.util.spec_from_file_location('tvm.relay.testing.darknet',darknet_path)
+yolo_detection = importlib.util.module_from_spec(detection_spec)
+darknet = importlib.util.module_from_spec(darknet_spec)
+sys.modules['tvm.relay.testing.yolo_detection'] = yolo_detection
+sys.modules['tvm.relay.testing.darknet'] = darknet
+detection_spec.loader.exec_module(yolo_detection)
+darknet_spec.loader.exec_module(darknet)
+
+
+FILE_DIR   = os.path.dirname(os.path.abspath(__file__))
+REPO_URL='https://github.com/dmlc/web-data/blob/master/darknet/'
+def run(fdir, dpu_rundir, shape_dict, iterations):
+    assert len(shape_dict) == 1
+
+    coco_name = 'coco.names'
+    coco_url = REPO_URL + 'data/' + coco_name + '?raw=true'
+    font_name = 'arial.ttf'
+    font_url = REPO_URL + 'data/' + font_name + '?raw=true'
+    coco_path = download_testdata(coco_url, coco_name, module='data')
+    font_path = download_testdata(font_url, font_name, module='data')
+    
+    extern_accel.setDpuRunDir(dpu_rundir)
+       
+    netw, neth = 608, 608
+    thresh = 0.5
+    nms_thresh = 0.45
+    num_classes = 80
+
+    # Load a test image
+    test_image = 'dog.jpg'
+    print("Loading the test image...")
+    img_url = REPO_URL + 'data/' + test_image + '?raw=true'
+    img_path = download_testdata(img_url, test_image, "data")
+
+    # RUN #
+    inputs = {}
+    image = darknet.load_image(img_path, 608, 608)
+    inputs[list(shape_dict.keys())[0]] = np.expand_dims(image,axis=0)
+    # VAI FLOW
+    tru = TVMRuntimeUtil(fdir)
+    for i in range(iterations):
+        start = time.time()
+        res = tru.run(inputs)
+        stop = time.time()
+        
+        print("VAI iteration: {}/{}, run time: {}".format(i+1, iterations, stop - start))
+        
+        tvm_out = []                
+        layer_out = {}
+        layer_out['type'] = 'Region'
+        # Get the region layer attributes (n, out_c, out_h, out_w, classes, coords, background)
+        layer_attr = res[2]
+        layer_out['biases'] = res[1]
+        out_shape = (layer_attr[0], layer_attr[1]//layer_attr[0],
+                 layer_attr[2], layer_attr[3])
+        layer_out['output'] = res[0].reshape(out_shape)
+        layer_out['classes'] = layer_attr[4]
+        layer_out['coords'] = layer_attr[5]
+        layer_out['background'] = layer_attr[6]
+        tvm_out.append(layer_out)
+        # do the detection and bring up the bounding boxes
+        img = darknet.load_image_color(img_path)
+        _, im_h, im_w = img.shape
+        dets = yolo_detection.fill_network_boxes((netw, neth), (im_w, im_h), thresh,
+                                                     1, tvm_out)
+            
+        yolo_detection.do_nms_sort(dets, num_classes, nms_thresh)
+            
+        with open(coco_path) as f:
+            content = f.readlines()
+            
+        names = [x.strip() for x in content]
+            
+        yolo_detection.draw_detections(font_path, img, dets, thresh, names, num_classes)
+           
+        cv2.imwrite("./output.png", (img.transpose(1, 2, 0)*256).astype(int))
+    del extern_accel.RUNNER_CACHE
+
+# RUN
+###############################################################################
+# RUN RELAY_DARKNET_YOLOV2
+# 
+# Before running the relay_darknet_yolov2 model, you have to compile the model
+# using the script provided at /tvm/tutorials/accelerators/compile/mxnet_resent_18.py
+# The compile script generates an output file name "mxnet_resent_18"
+# Once you setup your device, you could run the model as follows:
+#
+# Parameter settings for the run script:
+# -f           : Path to directory containing TVM compilation files
+# -d           : Path to directory containing DPU model lib 
+# --iterations : The number of iterations to run the model
+#
+# example:
+# ./relay_darknet_yolov2.py -f /PATH_TO_DIR/relay_darknet_yolov2 -d /PATH_TO_DIR/relay_darknet_yolov2/libdpu --iterations 1
+
+##############################################################################
+
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser()
+    parser.add_argument("-f", help="Path to directory containing TVM compilation files", default=FILE_DIR)
+    parser.add_argument("-d", help="Path to directory containing DPU model lib and meta.json", required=True)
+    parser.add_argument("--iterations", help="The number of iterations to run.", default=2, type=int)
+    args = parser.parse_args()
+
+    fdir = args.f if os.path.isabs(args.f) else os.path.join(os.getcwd(), args.f)
+    dpu_rundir = args.d if  os.path.isabs(args.d) else os.path.join(os.getcwd(), args.d)
+    iterations = args.iterations
+
+
+    input_shapes = {'data': [1, 3, 608, 608]}
+    input_names  = list(input_shapes.keys())
+    
+    
+    run(fdir, dpu_rundir,  input_shapes, iterations)
+
+
