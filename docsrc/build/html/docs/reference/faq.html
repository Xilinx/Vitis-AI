<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
<!-- End Google Tag Manager -->
  <title>Frequently Asked Questions &mdash; Vitis™ AI 3.5 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Branching / Tagging Strategy" href="../install/branching_tagging_strategy.html" />
    <link rel="prev" title="IP and Tool Version Compatibility" href="version_compatibility.html" /> 
</head>

<body class="wy-body-for-nav">

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
<!-- End Google Tag Manager --> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../../index.html" class="icon icon-home"> Vitis™ AI
            <img src="../../_static/xilinx-header-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                3.5
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Setup and Install</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="system_requirements.html">System Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/install.html">Host Install Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/vek280.html">Versal AI Edge VEK280</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/v70.html">Alveo V70</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Workflow and Components</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../workflow.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-system-integration.html">DPU IP Details and System Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-model-zoo.html">Vitis AI Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-model-development.html">Developing a Model for Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-model-deployment.html">Deploying a Model with Vitis AI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Runtime API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../doxygen/api/classlist.html">C++ API Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../doxygen/api/pythonlist.html">Python APIs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional Information</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="release_documentation.html">Vitis AI User Guides &amp; IP Product Guides</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/Vitis-AI-Tutorials">Vitis AI Developer Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../workflow-third-party.html">Third-party Inference Stack Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="version_compatibility.html">IP and Tools Compatibility</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-is-vitis-ai">What is Vitis AI?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#are-all-the-components-of-vitis-ai-free">Are all the components of Vitis AI free?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#is-vitis-ai-a-separate-download">Is Vitis AI a separate download?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-are-the-vitis-ai-vitis-and-vivado-version-compatibility-requirements">What are the Vitis AI, Vitis, and Vivado version compatibility requirements?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#which-deep-learning-frameworks-does-vitis-ai-support">Which deep learning frameworks does Vitis AI support?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#which-ai-models-does-vitis-ai-support">Which AI Models does Vitis AI Support?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-xilinx-target-device-families-and-platforms-does-vitis-ai-support">What Xilinx Target Device Families and Platforms does Vitis AI Support?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-does-the-vitis-ai-library-provide">What does the Vitis AI Library provide?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-layers-are-supported-for-hardware-acceleration">What layers are supported for hardware acceleration?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-batch-sizes-are-supported">What batch sizes are supported?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-does-fpga-compare-to-cpu-and-gpu-acceleration">How does FPGA compare to CPU and GPU acceleration?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#where-do-i-begin-with-a-new-trained-model">Where do I begin with a new trained model?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-quantization-why-needed-does-it-impact-accuracy">What is quantization why needed does it impact accuracy?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#can-i-leverage-vitis-ai-on-a-pure-fpga-target-with-or-without-a-microblaze-processor">Can I leverage Vitis™ AI on a pure FPGA target with or without a Microblaze™ processor?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#is-it-possible-to-use-the-dpu-without-petalinux">Is it possible to use the DPU without PetaLinux?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#can-the-dpucz-be-used-for-alternate-purposes-beyond-deployment-of-neural-networks-for-example-signal-processing-operations">Can the DPUCZ be used for alternate purposes beyond deployment of neural networks? For example, signal processing operations?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-the-difference-between-the-vitis-ai-integrated-development-environment-and-the-finn-workflow">What is the difference between the Vitis AI integrated development environment and the FINN workflow?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#i-have-a-zcu106-board-can-i-leverage-the-vitis-ai-ide-with-the-zcu106-how-do-i-get-started">I have a ZCU106 board. Can I leverage the Vitis AI IDE with the ZCU106? How do I get started?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-the-specific-ai-accelerator-that-amd-xilinx-provides-for-zynq-ultrascale-is-it-a-systolic-array">What is the specific AI accelerator that AMD Xilinx provides for Zynq™ Ultrascale+?  Is it a systolic array?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../install/branching_tagging_strategy.html">Branching and Tagging Strategy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Resources and Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="additional_resources.html">Technical Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="additional_resources.html#id1">Additional Resources</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Related AMD Solutions</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/DPU-PYNQ">DPU-PYNQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/finn/">FINN &amp; Brevitas</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/inference-server/">Inference Server</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/amd/UIF">Unified Inference Frontend</a></li>
<li class="toctree-l1"><a class="reference external" href="https://onnxruntime.ai/docs/execution-providers/community-maintained/Vitis-AI-ExecutionProvider.html">Vitis AI ONNX Runtime Execution Provider</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/VVAS/">Vitis Video Analytics SDK</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Vitis™ AI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Frequently Asked Questions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/reference/faq.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="frequently-asked-questions">
<h1>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="#what-is-vitis-ai">What is Vitis AI?</a></p></li>
<li><p><a class="reference external" href="#what-are-the-different-components-of-vitis-ai">What are the different components of Vitis
AI?</a></p></li>
<li><p><a class="reference external" href="#are-all-the-components-of-vitis-ai-free">Are all the components of Vitis AI
free?</a></p></li>
<li><p><a class="reference external" href="#is-vitis-ai-a-separate-download">Is Vitis AI a separate
download?</a></p></li>
<li><p><a class="reference external" href="#what-are-the-vitis-ai-vitis-and-vivado-version-compatibility-requirements">What are the Vitis AI, Vitis, and Vivado version compatibility
requirements?</a></p></li>
<li><p><a class="reference external" href="#which-deep-learning-frameworks-does-vitis-ai-support">Which deep learning frameworks does Vitis AI
support?</a></p></li>
<li><p><a class="reference external" href="#which-ai-models-will-vitis-ai-support">Which AI Models does Vitis AI
Support?</a></p></li>
<li><p><a class="reference external" href="#which-vitis-platforms-does-vitis-ai-support">Which Vitis Platforms will Vitis AI
Support?</a></p></li>
<li><p><a class="reference external" href="#what-does-the-vitis-ai-library-provide">What does the Vitis AI Library
provide?</a></p></li>
<li><p><a class="reference external" href="#what-layers-are-supported-for-hardware-acceleration">What layers are supported for hardware
acceleration?</a></p></li>
<li><p><a class="reference external" href="#what-batch-sizes-are-supported">What batch sizes are supported?</a></p></li>
<li><p><a class="reference external" href="#how-does-fpga-compare-to-cpu-and-gpu-acceleration">How does FPGA compare to CPU and GPU
acceleration?</a></p></li>
<li><p><a class="reference external" href="#where-do-i-begin-with-a-new-trained-model">Where do I begin with a new trained
model?</a></p></li>
<li><p><a class="reference external" href="#what-is-quantization-why-needed-does-it-impact-accuracy">What is quantization why needed does it impact
accuracy?</a></p></li>
<li><p><a class="reference external" href="#can-i-leverage-vitis-ai-on-a-pure-fpga-target-with-or-without-a-microblaze-processor">Can I leverage Vitis™ AI on a pure FPGA target with or without a Microblaze™ processor?</a></p></li>
<li><p><a class="reference external" href="#is-it-possible-to-use-the-dpu-without-petalinux">Is it possible to use the DPU without PetaLinux?</a></p></li>
<li><p><a class="reference external" href="#is-it-possible-to-use-the-dpu-without-petalinux">Is it possible to deploy the DPUCZ without using Linux?</a></p></li>
<li><p><a class="reference external" href="#can-the-dpucz-be-used-for-alternate-purposes-beyond-deployment-of-neural-networks-for-example-signal-processing-operations">Can the DPUCZ be used for alternate purposes beyond deployment of neural networks? For example, signal processing operations?</a></p></li>
<li><p><a class="reference external" href="#what-is-the-difference-between-the-vitis-ai-integrated-development-environment-and-the-finn-workflow">What is the difference between the Vitis AI integrated development environment and the FINN workflow?</a></p></li>
<li><p><a class="reference external" href="#i-have-a-zcu106-board-can-i-leverage-the-vitis-ai-ide-with-the-zcu106-how-do-i-get-started">I have a ZCU106 board. Can I leverage the Vitis AI IDE with the ZCU106? How do I get started?</a></p></li>
<li><p><a class="reference external" href="#what-is-the-specific-ai-accelerator-that-amd-xilinx-provides-for-zynq-ultrascale-is-it-a-systolic-array">What is the specific AI accelerator that AMD Xilinx provides for Zynq™ Ultrascale+? Is it a systolic array?</a></p></li>
</ul>
<section id="what-is-vitis-ai">
<h2>What is Vitis AI?<a class="headerlink" href="#what-is-vitis-ai" title="Permalink to this heading">¶</a></h2>
<p>Vitis AI is our unified AI inference solution for all Xilinx platforms. It consists of optimized IP, tools, libraries, models, and example
designs. With Vitis AI, ML and AI developers can have a familiar and consistent user experience that is scalable from edge-to-cloud across a variety of Xilinx targets.</p>
</section>
<section id="are-all-the-components-of-vitis-ai-free">
<h2>Are all the components of Vitis AI free?<a class="headerlink" href="#are-all-the-components-of-vitis-ai-free" title="Permalink to this heading">¶</a></h2>
<p>Yes.  As of the 3.5 release all components are free!  For releases &lt;3.5, the Vitis AI Optimizer does require a separate license which can be obtained free-of-charge upon request. Additional details surrounding this
license can be found in the Optimizer <a class="reference internal" href="../workflow-model-development.html#model-optimization"><span class="std std-ref">introduction</span></a>.</p>
</section>
<section id="is-vitis-ai-a-separate-download">
<h2>Is Vitis AI a separate download?<a class="headerlink" href="#is-vitis-ai-a-separate-download" title="Permalink to this heading">¶</a></h2>
<p>Yes!  Users can started by cloning the Vitis AI Github <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">repository</a></p>
</section>
<section id="what-are-the-vitis-ai-vitis-and-vivado-version-compatibility-requirements">
<h2>What are the Vitis AI, Vitis, and Vivado version compatibility requirements?<a class="headerlink" href="#what-are-the-vitis-ai-vitis-and-vivado-version-compatibility-requirements" title="Permalink to this heading">¶</a></h2>
<p>Vitis AI, Vitis and Vivado are released on a bi-annual cadence. Currently there is a slight lag in the release timing of Vitis AI in order to address the complexities involved in verification and compatibility with Vivado® and Vitis™. Each Vitis AI release is verified with the (then) current release of Vivado and Vitis. It is not generally advised to mix tool versions.  See <a class="reference internal" href="version_compatibility.html"><span class="doc">the version compatibility documentation</span></a> for more details.</p>
</section>
<section id="which-deep-learning-frameworks-does-vitis-ai-support">
<h2>Which deep learning frameworks does Vitis AI support?<a class="headerlink" href="#which-deep-learning-frameworks-does-vitis-ai-support" title="Permalink to this heading">¶</a></h2>
<p>Vitis AI 3.0 supports TensorFlow 1.x, 2.x and PyTorch. Prior to release 2.5, Caffe and DarkNet were supported and for those frameworks, users can leverage a previous release of Vitis AI for quantization and compilation, while leveraging the latest Vitis-AI Library and Runtime components for deployment.</p>
</section>
<section id="which-ai-models-does-vitis-ai-support">
<h2>Which AI Models does Vitis AI Support?<a class="headerlink" href="#which-ai-models-does-vitis-ai-support" title="Permalink to this heading">¶</a></h2>
<p>With the release of the Vitis AI IDE, more than 120 models are released in the Vitis AI Model Zoo. In addition, the Vitis AI IDE is designed to enable developers to deploy custom models, subject to layer, parameter, and activation support. Due to the non-permissive licensing associated with newer YOLO variants, we will not be releasing pre-trained versions of these models. Users will need to train, quantize,and compile those models using the Vitis AI tool, and we will provide the resources to enable this. In general, Vitis AI can support the majority of CNNs, including custom user networks. As part of our development process and continuous effort to provide more diverse operator and layer support, we train and deploy new models with each release. All of these models, together with performance benchmarks for out-of-the-box supported Xilinx targets are published in the <a class="reference internal" href="../workflow-model-zoo.html#workflow-model-zoo"><span class="std std-ref">Vitis AI Model Zoo</span></a>.</p>
</section>
<section id="what-xilinx-target-device-families-and-platforms-does-vitis-ai-support">
<h2>What Xilinx Target Device Families and Platforms does Vitis AI Support?<a class="headerlink" href="#what-xilinx-target-device-families-and-platforms-does-vitis-ai-support" title="Permalink to this heading">¶</a></h2>
<p>Vitis AI DPUs are available for both Zynq Ultrascale+ MPSoC as well as Versal Edge and Core chip-down designs. The Kria K26 SOM is supported as a production-ready Edge platform, and Alveo accelerator cards are supported for cloud applications.</p>
</section>
<section id="what-does-the-vitis-ai-library-provide">
<h2>What does the Vitis AI Library provide?<a class="headerlink" href="#what-does-the-vitis-ai-library-provide" title="Permalink to this heading">¶</a></h2>
<p>The Vitis AI Library provides a lightweight set of C++ APIs suited to a variety of AI tasks, such as image classification, object detection and semantic segmentation, simplifying deployment of user applications.</p>
</section>
<section id="what-layers-are-supported-for-hardware-acceleration">
<h2>What layers are supported for hardware acceleration?<a class="headerlink" href="#what-layers-are-supported-for-hardware-acceleration" title="Permalink to this heading">¶</a></h2>
<p>The answer to this question varies somewhat depending on the specific target platform and DPU IP that is leveraged on that platform. In general, most common CNN layers and activation types are supported for DPU hardware acceleration. The toolchain supports graph partitioning, which enables developers to augment DPU operator support through the use of an inference framework, or via custom acceleration code.</p>
<p>More specific details can be found
<a class="reference internal" href="../workflow-model-development.html#operator-support"><span class="std std-ref">here</span></a></p>
</section>
<section id="what-batch-sizes-are-supported">
<h2>What batch sizes are supported?<a class="headerlink" href="#what-batch-sizes-are-supported" title="Permalink to this heading">¶</a></h2>
<p>Since FPGA hardware accelerators can be designed for precisely for the task at hand, the notion of batching as it is widely understood in CPU and GPU deployments is not directly applicable to Xilinx targets. In general, Xilinx DPUs process one image per accelerator core at a time. The implication of this is that the performance of a single-core DPU (Zynq/Kria targets) is specified with a batch size of 1, and that inference efficiency is not improved through the use of batching.</p>
<p>For Versal and Alveo targets, higher performance DPUs have been developed that have more than one core, and in this context, the runtime and DPU are designed to compute inference on multiple images simultaneously. Thus, in the case of Versal and Alveo targets, batching is leveraged in order to ensure that each of the parallel acceleration cores is performing useful processing. However, this also differs from the CPU/GPU notion of batching in that the batch size requirement is much lower (2-8). For specific details, please refer to the respective :doc: <cite>DPU product guides &lt;release_documentation&gt;</cite></p>
<p>As there is a greatly reduced requirement to queue multiple input samples, the end-to-end latency and memory footprint for inference is reduced, which can be an important advantage in some applications.</p>
</section>
<section id="how-does-fpga-compare-to-cpu-and-gpu-acceleration">
<h2>How does FPGA compare to CPU and GPU acceleration?<a class="headerlink" href="#how-does-fpga-compare-to-cpu-and-gpu-acceleration" title="Permalink to this heading">¶</a></h2>
<p>FPGA accelerated networks can run upto 90x faster as compared to CPU. FPGA accelerated networks are on par with GPU accelerated networks for throughput critical applications, yet provide support for more custom applications. FPGA accelerated networks are far superior to GPU accelerated networks for latency critical applications such as autonomous driving.
<a class="reference external" href="https://www.xilinx.com/support/documentation/white_papers/wp504-accel-dnns.pdf">See this white paper for an example benchmark</a></p>
</section>
<section id="where-do-i-begin-with-a-new-trained-model">
<h2>Where do I begin with a new trained model?<a class="headerlink" href="#where-do-i-begin-with-a-new-trained-model" title="Permalink to this heading">¶</a></h2>
<p>We would recommend the workflow documentations pages in this repository as the ideal starting point for new users.</p>
</section>
<section id="what-is-quantization-why-needed-does-it-impact-accuracy">
<h2>What is quantization why needed does it impact accuracy?<a class="headerlink" href="#what-is-quantization-why-needed-does-it-impact-accuracy" title="Permalink to this heading">¶</a></h2>
<p>Quantization is a very old concept. The idea is to map a range of real   numbers to a set of discrete numbers. This is how digital audio has always worked. The magnitude of a pressure wave (aka sound) could be near infinite. Imagine the sound of two planets colliding. However, that range does not need to be represented when you are listening to The Beatles greatest hits. Digital Audio typically quantizes an electrical signal from a microphone into a 16b or 24b discrete numbers. In computers, real numbers are best approximated using the floating point representation, which is in and of its self a logarithmic quantization of real values. However, floating point arithmetic is classically known to be more complex, and more demanding of hardware resources. To implement neural networks more efficiently in FPGAs we re-quantize images, weights, and activations into 16b or 8b integer representations. This process requires determining the range of floating point values that need to be represented and determining a set of scaling factors. Fixed point arithmetic allows us to reach maximum OPs/second, and there is a pool of research papers that show how fixed point quantization minimally impacts the accuracy of convolutional neural nets. Some papers citing a degradation in accuracy of 2%. Xilinx has actually seen some networks perform better with fixed-point quantization.</p>
</section>
<section id="can-i-leverage-vitis-ai-on-a-pure-fpga-target-with-or-without-a-microblaze-processor">
<h2>Can I leverage Vitis™ AI on a pure FPGA target with or without a Microblaze™ processor?<a class="headerlink" href="#can-i-leverage-vitis-ai-on-a-pure-fpga-target-with-or-without-a-microblaze-processor" title="Permalink to this heading">¶</a></h2>
<p>The Vitis AI integrated development environment (IDE) supports SoC targets (Zynq™ UltraScale+™ MPSoC, Versal™ adaptive SoCs) and Alveo™ platforms (AMD64 host). It does not claim to support FPGA-class devices including Spartan™, Artix™, Kintex™, or Virtex™ FPGAs.  While it is possible to enable and run Vitis AI IDE firmware components on the MicroBlaze processor, this is not a documented and supported flow for mainstream development. Officially, the AMD Xilinx “Space deep-learning processor unit (DPU)” project leverages the MicroBlaze processor targeting Kintex UltraScale-class devices. For deployment in standard non-space applications, we do have an experimental flow that we can potentially share, but it has limitations, and the expectation is that the developer will need to invest additional time in optimization. If you have a strong need for this, please reach out to us directly, and we can discuss your use case further.</p>
</section>
<section id="is-it-possible-to-use-the-dpu-without-petalinux">
<h2>Is it possible to use the DPU without PetaLinux?<a class="headerlink" href="#is-it-possible-to-use-the-dpu-without-petalinux" title="Permalink to this heading">¶</a></h2>
<p>There are at least two potential interpretations of this question:</p>
<p><cite>Is it possible to deploy the DPUCZ using Yocto flows, or even Ubuntu, rather than PetaLinux?</cite></p>
<blockquote>
<div><p>Yes, what is important to consider is that each release of the Vitis AI tool and the DPUCZ IP is provided with drivers and a runtime that targets a specific Linux kernel release. Misalignment between the target kernel version can pose challenges and may require extensive code changes.</p>
</div></blockquote>
<p><cite>Is it possible to deploy the DPUCZ without using Linux?</cite></p>
<blockquote>
<div><p>We do have proof-of-concept beta support for both Green Hills Integrity and Blackberry QNX. For QNX and Integrity support, users should contact their local FAE or Sales representative to request additional discussions with the factory. As of this release, no support exists for bare-metal or FreeRTOS; however, Zynq UltraScale+ family members do support asymmetric multiprocessing, with the potential that developers can integrate the DPU via Linux, while continuing to develop the bulkof their application in their chosen OS. Please refer to <a class="reference external" href="https://docs.xilinx.com/r/en-US/ug1137-zynq-ultrascale-mpsoc-swdev/Asymmetric-Multiprocessing-AMP">UG1137</a> for additional information on AMP modes of operation.</p>
</div></blockquote>
</section>
<section id="can-the-dpucz-be-used-for-alternate-purposes-beyond-deployment-of-neural-networks-for-example-signal-processing-operations">
<h2>Can the DPUCZ be used for alternate purposes beyond deployment of neural networks? For example, signal processing operations?<a class="headerlink" href="#can-the-dpucz-be-used-for-alternate-purposes-beyond-deployment-of-neural-networks-for-example-signal-processing-operations" title="Permalink to this heading">¶</a></h2>
<p>While Xilinx DPUs are well optimized for certain operations that overlap with signal processing (i.e., convolution, elementwise, etc.), deployment of conventional signal processing functions is neither the purpose nor intent of the Vitis AI IDE and the DPUCZ. Today, the Vitis AI tool DPU instruction compiler is not provided as open source, and the instruction set for the DPUCZ is not publicly documented.For signal processing applications, we provide highly optimized IP cores as well as open-source libraries supporting a wide variety of FFT architectures, including both streaming and time-division multiplex applications. Furthermore, we have support for advanced FFT architectures such as SSR. It is much more efficient to deploy such functions by leveraging IP that has been optimized for these tasks.  Remember, the DPUCZ is optimized for INT8 quantized operations. In many signal processing applications, the pipeline employs a higher dynamic range, such as 10, 12, 14, bit  s.   Furthermore, many signal processing applications employ a streaming data pipeline that does not align well with the operation of the DPUCZ.Similarly, common signal processing operations are provided as optimized IP in the Vivado®IP catalog. For the highest levels of performance in RTL designs, we would generally refer users to these IP. Most of the IP that you will require is free and available in the Vivado Design Suite (free “included” license).  For users who prefer an open-source, HLS-based implementation or pure software (Vitis™ IDE) flow, the Vitis Accelerated Libraries are an excellent solution.Finally, if you are an avid Simulink® tool user, you may also wish to consider our Vitis Model Composer / System Generator workflow.</p>
</section>
<section id="what-is-the-difference-between-the-vitis-ai-integrated-development-environment-and-the-finn-workflow">
<h2>What is the difference between the Vitis AI integrated development environment and the FINN workflow?<a class="headerlink" href="#what-is-the-difference-between-the-vitis-ai-integrated-development-environment-and-the-finn-workflow" title="Permalink to this heading">¶</a></h2>
<p>The two methods are complementary. The FINN workflow is differentiated from the Vitis AI IDE workflow in that it does not employ a general-purpose AI inference accelerator. Rather, the FINN toolchain builds a network-specific dataflow architecture, leveraging streaming interfaces between layers. Effectively, the entire CNN is unrolled and implemented layer by layer in fabric. The result is that a FINN implementation is optimized specifically for a specific neural network architecture. A new bitstream is required if the user chooses to modify the structure or parameters (excluding weights) of the neural network graph. The benefits of the FINN approach can include:</p>
<ul class="simple">
<li><p>Highly optimized latency</p></li>
<li><p>Highly optimized programmable logic utilization for small networks</p></li>
<li><p>High throughput for small networks</p></li>
<li><p>High power efficiency</p></li>
<li><p>Flexibility to  employ mixed precision (quantization on a layer-by-layer basis)</p></li>
</ul>
<p>This is all in contrast to the Vitis AI DPUs, which are fixed architecture, general-purpose AI accelerators that can deploy a wide variety of neural networks without requiring a new bitstream when the neural network graph structure is changed. The benefits of the Vitis AI IDE workflow can include:</p>
<ul class="simple">
<li><p>Flexibility to deploy both shallow and deep neural networks with comparable programmable logic resource utilization</p></li>
<li><p>No requirement to update the bitstream when switching to a different neural network architecture</p></li>
<li><p>Ability to deploy multiple networks and process a variable number of streams with a single accelerator</p></li>
<li><p>Fixed INT8 precision</p></li>
<li><p>Low-latency, general-purpose applications</p></li>
<li><p>Multi-network, multi-stream, deployments with a single DPU instance</p></li>
</ul>
</section>
<section id="i-have-a-zcu106-board-can-i-leverage-the-vitis-ai-ide-with-the-zcu106-how-do-i-get-started">
<h2>I have a ZCU106 board. Can I leverage the Vitis AI IDE with the ZCU106? How do I get started?<a class="headerlink" href="#i-have-a-zcu106-board-can-i-leverage-the-vitis-ai-ide-with-the-zcu106-how-do-i-get-started" title="Permalink to this heading">¶</a></h2>
<p>The ZCU106 board can be leveraged, but you are correct in concluding that the Vitis AI IDE repository does not provide an image that supports this board for evaluation purposes. You might wish to test drive DPU Pynq, which does have (currently unverified) support for the ZCU106.</p>
<p>In the past, developers have ported the DPUCZ reference design (formerly known as the TRD) to the ZCU106 board. We do not formally document this process today, but if you wish to pursue this, you may want to start with these references:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/Xilinx/DPU-PYNQ/tree/master/boards/zcu106">DPU-PYNQ</a></p></li>
<li><p><a class="reference external" href="https://github.com/jimheaton/Vitis-AI-DPU_TRD-for-ZCU106">Xilinx Specialist, Jim Heaton’s reference</a></p></li>
<li><p><a class="reference external" href="https://logictronix.com/machine-learning-with-fpga/vitisai-dpu-dnndk/dpu-3-0-trd-for-zcu106/">Vitis AI Design Partner, Logictronix’s reference</a></p></li>
<li><p><a class="reference external" href="https://support.xilinx.com/s/question/0D52E00006hpM10SAE/vitisai-dputrd-for-zcu106?language=en_US">Xilinx forums reference</a></p></li>
</ul>
</section>
<section id="what-is-the-specific-ai-accelerator-that-amd-xilinx-provides-for-zynq-ultrascale-is-it-a-systolic-array">
<h2>What is the specific AI accelerator that AMD Xilinx provides for Zynq™ Ultrascale+?  Is it a systolic array?<a class="headerlink" href="#what-is-the-specific-ai-accelerator-that-amd-xilinx-provides-for-zynq-ultrascale-is-it-a-systolic-array" title="Permalink to this heading">¶</a></h2>
<p>The DPUCZ IP that is provided with the Vitis AI IDE is the specialized accelerator. It is a custom processor that has a specialized instruction set. Graph operators such as CONV, POOL, ELTWISE are compiled as instructions that are executed by the DPU. The DPUCZ bears similarities to a systolic array but has specialized micro-coded engines that are optimized for specific tasks. Some of these engines are optimized for conventional convolution, while some are optimized for tasks such as depth-wise convolution, eltwise and others. We tend to refer to the DPUCZ as a Matrix of (Heterogeneous) Processing Engines.</p>
</section>
</section>


           </div>
          </div>
          
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="version_compatibility.html" class="btn btn-neutral float-left" title="IP and Tool Version Compatibility" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../install/branching_tagging_strategy.html" class="btn btn-neutral float-right" title="Branching / Tagging Strategy" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2023, Advanced Micro Devices, Inc.
      <span class="lastupdated">Last updated on July 2, 2023.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>